#!/usr/bin/env python3
"""
ç®€åŒ–ç‰ˆ DeepSeek R1 70B QLoRA è®­ç»ƒè„šæœ¬
é¿å…é¢„åŠ è½½æ¨¡å‹çš„é—®é¢˜
"""
import os
import sys
import torch
import json
from pathlib import Path
import gc
from datetime import datetime

def detect_environment():
    print("ğŸ” æ£€æµ‹ç¯å¢ƒ...")
    if torch.cuda.is_available():
        device_count = torch.cuda.device_count()
        for i in range(device_count):
            gpu_name = torch.cuda.get_device_name(i)
            total_memory = torch.cuda.get_device_properties(i).total_memory / (1024**3)
            print(f"GPU {i}: {gpu_name} | æ˜¾å­˜: {total_memory:.1f}GB")
        return True
    print("âŒ éœ€è¦CUDA GPUç¯å¢ƒ")
    return False

def load_training_data(data_path="training_data.jsonl"):
    print(f"ğŸ“‚ åŠ è½½è®­ç»ƒæ•°æ®: {data_path}")
    if not os.path.exists(data_path):
        print(f"âŒ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_path}")
        return None
    
    try:
        data = []
        with open(data_path, 'r', encoding='utf-8') as f:
            for line in f:
                item = json.loads(line.strip())
                if 'messages' in item:
                    data.append(item)
        print(f"âœ… åŠ è½½å®Œæˆ: {len(data)} æ¡è®­ç»ƒæ•°æ®")
        return data
    except Exception as e:
        print(f"âŒ æ•°æ®åŠ è½½å¤±è´¥: {e}")
        return None

def train_70b_qlora():
    print("ğŸŒŸ DeepSeek R1 70B QLoRA å¡”ç½—ç‰Œå¾®è°ƒç³»ç»Ÿ")
    print("ğŸ”— ä¸“ä¸ºLambda GPUäº‘å¹³å°ä¼˜åŒ–")
    print()
    print("ğŸ¯ å¼€å§‹DeepSeek R1 70B QLoRAå¾®è°ƒ")
    print("=" * 60)
    
    # æ£€æµ‹ç¯å¢ƒ
    if not detect_environment():
        print("âŒ è®­ç»ƒå¤±è´¥ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯")
        return False
    
    # åŠ è½½è®­ç»ƒæ•°æ®
    training_data = load_training_data()
    if training_data is None:
        print("âŒ è®­ç»ƒå¤±è´¥ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯")
        return False
    
    try:
        from transformers import (
            AutoTokenizer, 
            AutoModelForCausalLM, 
            BitsAndBytesConfig,
            TrainingArguments
        )
        from peft import LoraConfig, get_peft_model
        from trl import SFTTrainer
        from datasets import Dataset
        
        # æ¨¡å‹åç§°
        model_name = "deepseek-ai/DeepSeek-R1-0528"
        print(f"âœ… ä½¿ç”¨æ¨¡å‹: {model_name}")
        
        # åŠ è½½tokenizer
        print("ğŸ“¥ åŠ è½½tokenizer...")
        tokenizer = AutoTokenizer.from_pretrained(
            model_name, 
            trust_remote_code=True,
            padding_side="right"
        )
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        # 4bité‡åŒ–é…ç½®
        bnb_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_use_double_quant=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_compute_dtype=torch.bfloat16
        )
        
        # åŠ è½½æ¨¡å‹ - ä½¿ç”¨æ›´ç¨³å®šçš„å‚æ•°
        print("ğŸ“¥ åŠ è½½70Bæ¨¡å‹ (4bité‡åŒ–)...")
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            quantization_config=bnb_config,
            device_map="auto",
            trust_remote_code=True,
            torch_dtype=torch.bfloat16,
            low_cpu_mem_usage=True,
            max_memory={0: "80GB"}
        )
        
        # LoRAé…ç½®
        lora_config = LoraConfig(
            r=8,
            lora_alpha=16,
            target_modules=["query_key_value", "dense"],
            lora_dropout=0.05,
            bias="none",
            task_type="CAUSAL_LM",
        )
        
        # åº”ç”¨LoRA
        print("ğŸ”§ åº”ç”¨LoRAé…ç½®...")
        model = get_peft_model(model, lora_config)
        model.print_trainable_parameters()
        
        # å‡†å¤‡æ•°æ®é›†
        print("ğŸ“Š å‡†å¤‡è®­ç»ƒæ•°æ®é›†...")
        dataset = Dataset.from_list(training_data)
        
        # æ ¼å¼åŒ–å‡½æ•°
        def format_chat(example):
            messages = example['messages']
            text = ""
            for msg in messages:
                if msg['role'] == 'user':
                    text += f"<|im_start|>user\n{msg['content']}<|im_end|>\n"
                elif msg['role'] == 'assistant':
                    text += f"<|im_start|>assistant\n{msg['content']}<|im_end|>\n"
            return {"text": text}
        
        # åº”ç”¨æ ¼å¼åŒ–
        dataset = dataset.map(format_chat, remove_columns=dataset.column_names)
        
        # è®­ç»ƒå‚æ•°
        training_args = TrainingArguments(
            output_dir="./deepseek-r1-tarot-lora",
            num_train_epochs=3,
            per_device_train_batch_size=1,
            gradient_accumulation_steps=4,
            learning_rate=2e-4,
            fp16=True,
            logging_steps=10,
            save_steps=100,
            warmup_steps=100,
            evaluation_strategy="no",
            save_strategy="steps",
            load_best_model_at_end=False,
            report_to=None,  # ç¦ç”¨wandb
            dataloader_pin_memory=False,
            remove_unused_columns=False,
        )
        
        # åˆ›å»ºè®­ç»ƒå™¨
        trainer = SFTTrainer(
            model=model,
            train_dataset=dataset,
            tokenizer=tokenizer,
            args=training_args,
            max_seq_length=2048,
            packing=False,
        )
        
        # å¼€å§‹è®­ç»ƒ
        print("ğŸš€ å¼€å§‹è®­ç»ƒ...")
        trainer.train()
        
        # ä¿å­˜æ¨¡å‹
        print("ğŸ’¾ ä¿å­˜æ¨¡å‹...")
        trainer.save_model()
        
        print("âœ… è®­ç»ƒå®Œæˆï¼")
        return True
        
    except Exception as e:
        print(f"âŒ è®­ç»ƒå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    train_70b_qlora() 
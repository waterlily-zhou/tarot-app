#!/usr/bin/env python3
"""
DeepSeek R1 70B QLoRA å¾®è°ƒè„šæœ¬ - ä¼˜åŒ–ç‰ˆ
"""
import os
import sys
import torch

# å¼ºåˆ¶å®æ—¶è¾“å‡ºï¼Œç¦ç”¨Pythonç¼“å†²åŒº
sys.stdout.reconfigure(line_buffering=True)
sys.stderr.reconfigure(line_buffering=True)
os.environ['PYTHONUNBUFFERED'] = '1'
import json
from pathlib import Path
import gc
from datetime import datetime

def detect_environment():
    print("ğŸ” æ£€æµ‹ç¯å¢ƒ...")
    if torch.cuda.is_available():
        device_count = torch.cuda.device_count()
        for i in range(device_count):
            gpu_name = torch.cuda.get_device_name(i)
            total_memory = torch.cuda.get_device_properties(i).total_memory / (1024**3)
            print(f"GPU {i}: {gpu_name} | æ˜¾å­˜: {total_memory:.1f}GB")
        return True
    print("âŒ éœ€è¦CUDA GPUç¯å¢ƒ")
    return False

def load_training_data(data_path="training_data.jsonl"):
    print(f"ğŸ“‚ åŠ è½½è®­ç»ƒæ•°æ®: {data_path}")
    if not os.path.exists(data_path):
        print(f"âŒ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_path}")
        return None
    
    try:
        data = []
        with open(data_path, 'r', encoding='utf-8') as f:
            for line in f:
                item = json.loads(line.strip())
                # ä¿ç•™åŸå§‹messagesç»“æ„ âœ…
                if 'messages' in item:
                    data.append(item)
        print(f"âœ… åŠ è½½å®Œæˆ: {len(data)} æ¡è®­ç»ƒæ•°æ®")
        return data
    except Exception as e:
        print(f"âŒ æ•°æ®åŠ è½½å¤±è´¥: {e}")
        return None

def setup_70b_model():
    print("ğŸš€ è®¾ç½®DeepSeek R1 70Bæ¨¡å‹...")
    
    # ğŸ”¥ DeepSeekç»ˆæè§£å†³æ–¹æ¡ˆï¼šè·³è¿‡è°ƒè¯•ï¼Œç›´æ¥è¿›å…¥æ ¸å¿ƒä¿®å¤
    print("ğŸš€ å®æ–½DeepSeekå›¢é˜Ÿç»ˆæFP8è§£å†³æ–¹æ¡ˆ...")
    
    # å¯¼å…¥å¿…è¦çš„åº“
    try:
        from transformers import (
            AutoTokenizer, 
            AutoModelForCausalLM, 
            AutoConfig,
            BitsAndBytesConfig,
            DataCollatorForLanguageModeling,
            get_linear_schedule_with_warmup
        )
        from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
        from datasets import Dataset
        import os
        import torch
        
        # ğŸ”¥ DeepSeekç»ˆæè§£å†³æ–¹æ¡ˆï¼šä½¿ç”¨æœ¬åœ°ç¼“å­˜è·¯å¾„
        model_id = "deepseek-ai/DeepSeek-R1"  # ä½¿ç”¨ä¸»ç‰ˆæœ¬ï¼Œé¿å…FP8é…ç½®
        model_path = "/home/ubuntu/.cache/huggingface/models--deepseek-ai--DeepSeek-R1/snapshots/56d4cbbb4d29f4355bab4b9a39ccb717a14ad5ad"
        
        # ä¸´æ—¶ç¦ç”¨ç¦»çº¿æ¨¡å¼
        offline_vars = {}
        for var in ["HF_HUB_OFFLINE", "TRANSFORMERS_OFFLINE", "HF_DATASETS_OFFLINE"]:
            if var in os.environ:
                offline_vars[var] = os.environ.pop(var)
                print(f"ğŸ”“ ä¸´æ—¶ç¦ç”¨: {var}")
        
        print(f"ğŸš€ ä½¿ç”¨DeepSeek-R1ä¸»ç‰ˆæœ¬: {model_path}")
        
        # DeepSeekç»ˆæé…ç½®æ¸…ç†æ–¹æ¡ˆ
        print("ğŸš€ å®æ–½DeepSeekç»ˆæFP8è§£å†³æ–¹æ¡ˆ...")
        config = AutoConfig.from_pretrained(model_path, local_files_only=True, trust_remote_code=True)
        
        # å¼ºåˆ¶æ¸…é™¤æ‰€æœ‰é‡åŒ–é…ç½®
        try:
            # æ·±åº¦æ¸…é™¤é…ç½®ä¸­çš„é‡åŒ–è®¾ç½®
            if hasattr(config, "quantization_config"):
                config.quantization_config = None
            
            # æ¸…é™¤å¯èƒ½å­˜åœ¨çš„FP8æ ‡å¿—
            for attr in ["fp8", "use_fp8", "is_fp8", "quantization"]:
                if hasattr(config, attr):
                    delattr(config, attr)
                    print(f"ğŸ§¹ å·²åˆ é™¤å±æ€§: {attr}")
            
            # æ¸…é™¤æ‰€æœ‰é‡åŒ–ç›¸å…³å±æ€§
            for key in list(config.to_dict().keys()):
                if "quant" in key.lower() or "fp8" in key.lower():
                    delattr(config, key)
                    print(f"ğŸ§¹ å·²åˆ é™¤é…ç½®é”®: {key}")
            
            # åˆ›å»ºæ–°çš„å¹²å‡€é…ç½®å¯¹è±¡
            clean_config = AutoConfig.from_dict(config.to_dict())
            config = clean_config
            print("âœ… å·²åˆ›å»ºå…¨æ–°çš„å¹²å‡€é…ç½®å¯¹è±¡")
            
            # æ¸…é™¤æ¨¡å‹ç±»ä¸­çš„é‡åŒ–å±æ€§
            AutoModelForCausalLM.quantization_config = None
            print("âœ… å·²å½»åº•æ¸…é™¤æ‰€æœ‰é‡åŒ–é…ç½®")
        except Exception as e:
            print(f"âš ï¸ é‡åŒ–é…ç½®æ¸…é™¤å¤±è´¥: {e}")

        # åˆ›å»ºæˆ‘ä»¬è‡ªå·±çš„4bité‡åŒ–é…ç½®
        bnb_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_use_double_quant=True,
            bnb_4bit_compute_dtype=torch.bfloat16,
        )
        
        # æš‚æ—¶ç¦ç”¨æ‰€æœ‰é‡åŒ–é…ç½®æ£€æŸ¥ï¼Œä½†è¿”å›ç©ºå­—å…¸è€Œä¸æ˜¯None
        import transformers.quantizers.auto as qa_module
        original_merge = qa_module.AutoHfQuantizer.merge_quantization_configs
        def dummy_merge(*args, **kwargs):
            print("ğŸ›‘ è·³è¿‡é‡åŒ–é…ç½®åˆå¹¶ï¼Œè¿”å›ç©ºé…ç½®")
            return {}  # è¿”å›ç©ºå­—å…¸è€Œä¸æ˜¯None
        qa_module.AutoHfQuantizer.merge_quantization_configs = staticmethod(dummy_merge)
        
        # åŒæ—¶ä¿®å¤ supports_quant_method æ£€æŸ¥
        original_supports = qa_module.AutoHfQuantizer.supports_quant_method
        def dummy_supports(quantization_config):
            if quantization_config is None:
                return True
            # ç¡®ä¿quantization_configæ˜¯å­—å…¸
            if not isinstance(quantization_config, dict):
                return True
            return original_supports(quantization_config)
        qa_module.AutoHfQuantizer.supports_quant_method = staticmethod(dummy_supports)
        
        # å½»åº•ç¦ç”¨é‡åŒ–é…ç½®æ£€æŸ¥
        def bypass_pre_quantized_check(config):
            """ç»•è¿‡é¢„é‡åŒ–æ£€æŸ¥"""
            if hasattr(config, 'quantization_config'):
                config.quantization_config = None
            return config
        
        # é¦–å…ˆåœ¨configä¸­è®¾ç½®use_cache
        config.use_cache = False
        
        # æ¸…é™¤ä»»ä½•é¢„è®¾çš„é‡åŒ–é…ç½®
        config = bypass_pre_quantized_check(config)
        print(f"ğŸ§¹ é…ç½®æ¸…ç†å®Œæˆï¼Œquantization_config: {getattr(config, 'quantization_config', 'None')}")
        
        # ğŸš€ æ¿€è¿›æ˜¾å­˜ä¼˜åŒ–ï¼šæ··åˆCPU-GPUåŠ è½½
        print("ğŸ”§ åŠ è½½æ¨¡å‹ï¼ˆæ··åˆCPU-GPUæ¨¡å¼ï¼‰...")
        
        # æ¸…ç†GPUç¼“å­˜
        torch.cuda.empty_cache()
        gc.collect()
        
        # ä½¿ç”¨æ›´æ¿€è¿›çš„æ˜¾å­˜ä¼˜åŒ–é…ç½®
        model = AutoModelForCausalLM.from_pretrained(
          model_path,
          config=config,  # ä½¿ç”¨æˆ‘ä»¬æ¸…ç†è¿‡çš„é…ç½®
          quantization_config=bnb_config,  # å¼ºåˆ¶ä½¿ç”¨æˆ‘ä»¬çš„4bité…ç½®
          device_map="auto",  # è®©ç³»ç»Ÿè‡ªåŠ¨åˆ†é…CPU-GPU
          trust_remote_code=True,
          torch_dtype=torch.bfloat16,
          local_files_only=True,  # ä½¿ç”¨æœ¬åœ°ç¼“å­˜æ–‡ä»¶
          ignore_mismatched_sizes=True,  # å¿½ç•¥å¯èƒ½çš„é…ç½®ä¸åŒ¹é…
          low_cpu_mem_usage=True,  # å¯ç”¨å†…å­˜ä¼˜åŒ–
          max_memory={0: "70GB", "cpu": "50GB"},  # é™åˆ¶GPUä½¿ç”¨70GBï¼Œä½™ä¸‹ç”¨CPU
          offload_folder="./offload",  # CPU offloadç›®å½•
        )
        print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼Œä½¿ç”¨DeepSeekæ–¹æ¡ˆçš„4bit NF4é‡åŒ–")
        
        # ğŸ”§ æ˜¾å­˜ä¼˜åŒ–ï¼šå¯ç”¨gradient checkpointing
        if hasattr(model, 'gradient_checkpointing_enable'):
            model.gradient_checkpointing_enable()
            print("âœ… å·²å¯ç”¨gradient checkpointingèŠ‚çœæ˜¾å­˜")
        
        # æ¢å¤åŸå§‹å‡½æ•°
        qa_module.AutoHfQuantizer.merge_quantization_configs = original_merge
        
        # åŠ è½½tokenizer
        print("ğŸ”¹ åŠ è½½tokenizer...")
        tokenizer = AutoTokenizer.from_pretrained(
            model_path, 
            trust_remote_code=True,
            padding_side="right"
        )
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        # å‡†å¤‡æ¨¡å‹è¿›è¡ŒQLoRAè®­ç»ƒ
            print(f"ğŸ“„ æ£€æŸ¥æ–‡ä»¶: {file_path}")
            print(f"ğŸ“„ æ–‡ä»¶å­˜åœ¨: {os.path.exists(file_path)}")
            
            if os.path.exists(file_path):
                # æ£€æŸ¥æ–‡ä»¶æƒé™
                print(f"ğŸ“„ æ–‡ä»¶æƒé™: {oct(os.stat(file_path).st_mode)}")
                
                # è¯»å–æ–‡ä»¶å†…å®¹
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                # æ£€æŸ¥æ–‡ä»¶å¤§å°å’Œå…³é”®å†…å®¹
                print(f"ğŸ“„ æ–‡ä»¶å¤§å°: {len(content)} å­—ç¬¦")
                
                # æ™ºèƒ½ç»Ÿè®¡ï¼šåªç»Ÿè®¡æœªæ³¨é‡Šçš„assertè¯­å¥
                lines = content.split('\n')
                active_assert_count = 0
                commented_assert_count = 0
                
                for line in lines:
                    if 'assert not self.training' in line:
                        if line.strip().startswith('#'):
                            commented_assert_count += 1
                            print(f"âœ… å·²æ³¨é‡Šçš„assert: {line.strip()}")
                        else:
                            active_assert_count += 1
                            print(f"âš ï¸ æ´»è·ƒçš„assert: {line.strip()}")
                
                print(f"ğŸ” æ´»è·ƒassert: {active_assert_count}, å·²æ³¨é‡Šassert: {commented_assert_count}")
                assert_count = active_assert_count
                
                if assert_count > 0:
                    print(f"ğŸ”§ ä¿®å¤æ–‡ä»¶ {file_path}...")
                    
                    # æ£€æŸ¥æ˜¯å¦å·²ç»è¢«ä¿®å¤è¿‡ï¼ˆé¿å…é‡å¤å¤„ç†ï¼‰
                    if '# å·²æ³¨é‡Šï¼šå…è®¸è®­ç»ƒæ¨¡å¼' in content:
                        print(f"âš ï¸ æ–‡ä»¶å·²è¢«ä¿®å¤è¿‡ï¼Œå°è¯•æ¢å¤åŸæ–‡ä»¶...")
                        
                        # å°è¯•ä»å¤‡ä»½æ¢å¤
                        backup_path = file_path + '.backup'
                        if os.path.exists(backup_path):
                            with open(backup_path, 'r', encoding='utf-8') as f:
                                content = f.read()
                            print(f"âœ… å·²ä»å¤‡ä»½æ¢å¤: {backup_path}")
                        else:
                            print(f"âš ï¸ å¤‡ä»½æ–‡ä»¶ä¸å­˜åœ¨ï¼Œæ‰‹åŠ¨æ¸…ç†é‡å¤æ³¨é‡Š...")
                            # æ‰‹åŠ¨æ¸…ç†é‡å¤æ³¨é‡Š
                            lines = content.split('\n')
                            cleaned_lines = []
                            for line in lines:
                                if '# å·²æ³¨é‡Šï¼šå…è®¸è®­ç»ƒæ¨¡å¼' in line:
                                    # å°è¯•æ¢å¤åŸå§‹assertè¯­å¥
                                    if 'assert not self.training' in line:
                                        # æ‰¾åˆ°åŸå§‹ç¼©è¿›
                                        indent_match = line.split('#')[0]
                                        cleaned_line = indent_match + 'assert not self.training'
                                        cleaned_lines.append(cleaned_line)
                                        print(f"ğŸ”„ æ¢å¤è¡Œ: {cleaned_line.strip()}")
                                    else:
                                        cleaned_lines.append(line)
                                else:
                                    cleaned_lines.append(line)
                            content = '\n'.join(cleaned_lines)
                    
                    # é‡æ–°è®¡ç®—assertæ•°é‡
                    assert_count = content.count('assert not self.training')
                    print(f"ğŸ” æ¸…ç†åæ‰¾åˆ° {assert_count} ä¸ªassertè¯­å¥")
                    
                    if assert_count > 0:
                        # å¤‡ä»½åŸæ–‡ä»¶ï¼ˆè¦†ç›–ä¹‹å‰çš„å¤‡ä»½ï¼‰
                        backup_path = file_path + '.backup'
                        with open(backup_path, 'w', encoding='utf-8') as f:
                            f.write(content)
                        print(f"ğŸ’¾ å·²é‡æ–°å¤‡ä»½åˆ°: {backup_path}")
                        
                        # è¶…çº§è°ƒè¯•ï¼šæŸ¥çœ‹æ¯ä¸€è¡Œå†…å®¹
                        import re
                        
                        lines = content.split('\n')
                        modified_lines = []
                        changes_made = 0
                        
                        print("ğŸ” è¶…çº§è°ƒè¯•æ¨¡å¼ï¼šæŸ¥çœ‹åŒ…å«'training'çš„æ‰€æœ‰è¡Œ")
                        for i, line in enumerate(lines):
                            if 'training' in line.lower():
                                print(f"  ç¬¬{i+1}è¡Œ: '{line}'")
                                print(f"    åŸå§‹å­—èŠ‚: {repr(line)}")
                                print(f"    stripped: '{line.strip()}'")
                                if 'assert' in line:
                                    print(f"    ğŸ¯ è¿™è¡ŒåŒ…å«assert!")
                        
                        print("\nğŸ”§ å°è¯•å¤šç§åŒ¹é…æ¨¡å¼...")
                        patterns = [
                            r'^(\s*)assert\s+not\s+self\.training(.*)$',
                            r'^\s*assert\s+not\s+self\.training.*$',
                            r'assert\s+not\s+self\.training',
                            r'.*assert.*not.*self\.training.*',
                        ]
                        
                        for i, line in enumerate(lines):
                            line_matched = False
                            
                            for j, pattern in enumerate(patterns):
                                if re.search(pattern, line, re.IGNORECASE):
                                    print(f"ğŸ¯ ç¬¬{i+1}è¡ŒåŒ¹é…æ¨¡å¼{j+1}: '{line.strip()}'")
                                    line_matched = True
                                    
                                    # åªç”¨ç¬¬ä¸€ä¸ªæ¨¡å¼è¿›è¡Œå®é™…ä¿®æ”¹
                                    if j == 0 and not line.strip().startswith('#'):
                                        print(f"ğŸ”§ åº”ç”¨ä¿®æ”¹...")
                                        modified_line = '        # ' + line.strip() + '  # å·²æ³¨é‡Šï¼šå…è®¸è®­ç»ƒæ¨¡å¼'
                                        modified_lines.append(modified_line)
                                        changes_made += 1
                                        break
                            
                            if not line_matched:
                                modified_lines.append(line)
                            elif not any(re.search(patterns[0], line, re.IGNORECASE) for _ in [1]):
                                modified_lines.append(line)
                        
                        new_content = '\n'.join(modified_lines)
                        print(f"ğŸ“Š æœ¬æ¬¡ä¿®æ”¹äº† {changes_made} è¡Œ")
                        
                        # å†™å›æ–‡ä»¶
                        with open(file_path, 'w', encoding='utf-8') as f:
                            f.write(new_content)
                        
                        # éªŒè¯ä¿®æ”¹
                        with open(file_path, 'r', encoding='utf-8') as f:
                            verify_content = f.read()
                        
                        final_assert_count = verify_content.count('assert not self.training')
                        comment_count = verify_content.count('# assert not self.training')
                        
                        print(f"âœ… ä¿®å¤å®Œæˆï¼å‰©ä½™æ–­è¨€: {final_assert_count}")
                        print(f"âœ… æ³¨é‡Šæ•°é‡: {comment_count}")
                    else:
                        print(f"âœ… æ— éœ€ä¿®å¤")
                else:
                    print(f"âœ… æ–‡ä»¶æ­£å¸¸ï¼Œæ— éœ€ä¿®å¤")
                    
        if not all_modeling_files:
            print("âŒ æœªæ‰¾åˆ°ä»»ä½•modeling_deepseek.pyæ–‡ä»¶ï¼")
            print("ğŸ” å°è¯•æ‰‹åŠ¨æŸ¥æ‰¾...")
            
            # æ‰‹åŠ¨é€’å½’æŸ¥æ‰¾
            for root, dirs, files in os.walk("/home/ubuntu/.cache"):
                for file in files:
                    if file == "modeling_deepseek.py":
                        full_path = os.path.join(root, file)
                        print(f"ğŸ¯ æ‰‹åŠ¨æ‰¾åˆ°: {full_path}")
            
    except Exception as e:
        print(f"âŒ è°ƒè¯•å¤±è´¥: {e}")
        import traceback
        print("ğŸ” è¯¦ç»†é”™è¯¯:")
        traceback.print_exc()
    
    try:
        from transformers import (
            AutoTokenizer, 
            AutoModelForCausalLM, 
            AutoConfig,
            BitsAndBytesConfig,
            DataCollatorForLanguageModeling,
            get_linear_schedule_with_warmup
        )
        from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
        from datasets import Dataset
        from accelerate import infer_auto_device_map, dispatch_model
        import os
        import torch
        
        # ğŸ”¥ ç»ˆæè§£å†³æ–¹æ¡ˆï¼šå®Œå…¨ç»•è¿‡æœ¬åœ°ç¼“å­˜ï¼Œç›´æ¥ä½¿ç”¨æ¨¡å‹ID
        model_id = "deepseek-ai/DeepSeek-R1"  # ä½¿ç”¨ä¸»ç‰ˆæœ¬ï¼Œé¿å…FP8é…ç½®
        model_path = model_id  # ç›´æ¥ä½¿ç”¨æ¨¡å‹IDï¼Œè®©transformersè‡ªåŠ¨å¤„ç†
        
        print(f"ğŸš€ ä½¿ç”¨DeepSeek-R1ä¸»ç‰ˆæœ¬ï¼ˆç»•è¿‡æœ¬åœ°ç¼“å­˜ï¼‰: {model_path}")
        
        # ä¸´æ—¶ç¦ç”¨ç¦»çº¿æ¨¡å¼ï¼Œç¡®ä¿èƒ½è®¿é—®ä¸»ç‰ˆæœ¬
        import os as _temp_os
        offline_vars = {}
        for var in ["HF_HUB_OFFLINE", "TRANSFORMERS_OFFLINE", "HF_DATASETS_OFFLINE"]:
            if var in _temp_os.environ:
                offline_vars[var] = _temp_os.environ.pop(var)
                print(f"ğŸ”“ ä¸´æ—¶ç¦ç”¨: {var}")
        
        print(f"âœ… å‡†å¤‡åŠ è½½DeepSeek-R1ä¸»ç‰ˆæœ¬: {model_path}")

        # è·³è¿‡å¿«ç…§å®Œæ•´æ€§æ£€æŸ¥ï¼Œç›´æ¥ä½¿ç”¨åœ¨çº¿æ¨¡å‹
        
        # ä½¿ç”¨4bité‡åŒ–ï¼ˆQLoRAæ ‡å‡†ï¼‰ï¼Œæ˜¾è‘—é™ä½æ˜¾å­˜å ç”¨
        bnb_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_use_double_quant=True,
            bnb_4bit_compute_dtype=torch.bfloat16,
        )
        
        # åŠ è½½tokenizer
        print("ğŸ”¹ åŠ è½½tokenizer...")
        tokenizer = AutoTokenizer.from_pretrained(
            model_path, 
            trust_remote_code=True,
            padding_side="right"
        )
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        # ç®€åŒ–ï¼šç›´æ¥åŠ è½½åˆ°GPUï¼Œé¿å…å¤æ‚è®¾å¤‡æ˜ å°„å¯¼è‡´çš„metaè®¾å¤‡é—®é¢˜
        print("â³ ç›´æ¥åŠ è½½æ¨¡å‹åˆ°GPUï¼ˆé¿å…metaè®¾å¤‡ï¼‰...")
        
        # ç«‹å³ä¿®å¤ finegrained_fp8 è®¾å¤‡ä¸Šä¸‹æ–‡é—®é¢˜
        try:
            import transformers.integrations.finegrained_fp8 as _fg
            from contextlib import nullcontext as _nullcontext
            class _DummyAccel:
                @staticmethod
                def device(_dev):
                    return _nullcontext()
            # Override accelerator module to a no-op
            _fg.torch_accelerator_module = _DummyAccel
            print("âœ… å·²é¢„å…ˆç¦ç”¨ finegrained_fp8 CUDA è®¾å¤‡ä¸Šä¸‹æ–‡")
        except Exception as _e:
            print(f"âš ï¸ é¢„å…ˆç¦ç”¨ finegrained_fp8 å¤±è´¥ï¼ˆå¿½ç•¥ï¼‰ï¼š{_e}")

        try:
            import os as _os
            _os.environ.setdefault("PYTORCH_CUDA_ALLOC_CONF", "expandable_segments:True")

            # å…¨å±€çŒ´è¡¥ï¼šå¿½ç•¥ transformers é‡åŒ–ç³»ç»Ÿä¸­çš„ fp8 é…ç½®ï¼Œé˜²æ­¢åŠ è½½é˜¶æ®µæŠ¥é”™
            try:
                from transformers.quantizers import auto as _qa  # transformers>=4.39
                _orig_from_dict = _qa.AutoQuantizationConfig.from_dict

                def _patched_from_dict(cfg):
                    try:
                        if isinstance(cfg, dict) and str(cfg.get("quant_method", "")).lower() == "fp8":
                            print("ğŸ›‘ è·³è¿‡ä¸å—æ”¯æŒçš„ fp8 é‡åŒ–é…ç½® (from_dict)")
                            return None
                    except Exception:
                        pass
                    return _orig_from_dict(cfg)

                _qa.AutoQuantizationConfig.from_dict = staticmethod(_patched_from_dict)

                _orig_merge = _qa.AutoHfQuantizer.merge_quantization_configs

                def _patched_merge(*args, **kwargs):
                    # æ ‡å‡†ç­¾åä¸º (quantization_config)
                    qc = None
                    if args:
                        qc = args[0]
                    elif "quantization_config" in kwargs:
                        qc = kwargs.get("quantization_config")
                    try:
                        if isinstance(qc, dict) and str(qc.get("quant_method", "")).lower() == "fp8":
                            print("ğŸ›‘ è·³è¿‡ä¸å—æ”¯æŒçš„ fp8 é‡åŒ–é…ç½® (merge)")
                            if args:
                                args = (None,) + tuple(args[1:])
                            else:
                                kwargs["quantization_config"] = None
                    except Exception:
                        pass
                    return _orig_merge(*args, **kwargs)

                _qa.AutoHfQuantizer.merge_quantization_configs = staticmethod(_patched_merge)
            except Exception as _e_patch:
                print(f"âš ï¸ é‡åŒ–ç³»ç»ŸçŒ´è¡¥å¤±è´¥ï¼ˆå¿½ç•¥ï¼‰ï¼š{_e_patch}")

            # åœ¨æœ¬åœ°å¿«ç…§ä¸­æ¸…ç† FP8 é‡åŒ–é…ç½®æ–‡ä»¶ï¼Œé˜²æ­¢ Transformers åœ¨åŠ è½½æ—¶å†æ¬¡æ³¨å…¥ fp8 é…ç½®
            try:
                import json as _json
                _conf_json = _os.path.join(model_path, "config.json")
                if _os.path.exists(_conf_json):
                    with open(_conf_json, "r", encoding="utf-8") as _f:
                        _cfg = _json.load(_f)
                    if isinstance(_cfg.get("quantization_config"), dict) and _cfg["quantization_config"].get("quant_method") == "fp8":
                        print("ğŸ§¹ ä»config.jsonç§»é™¤fp8é‡åŒ–é…ç½®")
                        _cfg.pop("quantization_config", None)
                        with open(_conf_json, "w", encoding="utf-8") as _f:
                            _json.dump(_cfg, _f, ensure_ascii=False, indent=2)
                _qconf_json = _os.path.join(model_path, "quantization_config.json")
                if _os.path.exists(_qconf_json):
                    try:
                        _os.rename(_qconf_json, _qconf_json + ".bak")
                        print("ğŸ§¹ å·²é‡å‘½åquantization_config.jsonä¸º.bakï¼Œé¿å…fp8è¢«å†æ¬¡åŠ è½½")
                    except Exception as _e_ren:
                        print(f"âš ï¸ é‡å‘½åquantization_config.jsonå¤±è´¥ï¼ˆå¿½ç•¥ï¼‰ï¼š{_e_ren}")
            except Exception as _e_clean:
                print(f"âš ï¸ æ¸…ç†æœ¬åœ°å¿«ç…§fp8é‡åŒ–é…ç½®å¤±è´¥ï¼ˆå¿½ç•¥ï¼‰ï¼š{_e_clean}")

            # æ·»åŠ è®¾å¤‡æ£€æŸ¥ï¼ˆDeepSeekå»ºè®®ï¼‰
            if not torch.cuda.is_available():
                raise RuntimeError("éœ€è¦CUDAè®¾å¤‡æ”¯æŒ")
            print(f"âœ… CUDAå¯ç”¨è®¾å¤‡: {torch.cuda.device_count()}ä¸ª")
            print(f"âœ… PyTorchç‰ˆæœ¬: {torch.__version__}")
            
            # ğŸ”¥ DeepSeekç»ˆæè§£å†³æ–¹æ¡ˆï¼šå®Œå…¨æ¸…é™¤æ‰€æœ‰é‡åŒ–é…ç½®
            print("ğŸš€ å®æ–½DeepSeekç»ˆæFP8è§£å†³æ–¹æ¡ˆ...")
            config = AutoConfig.from_pretrained(model_path, local_files_only=True, trust_remote_code=True)
            
            # å¼ºåˆ¶æ¸…é™¤æ‰€æœ‰é‡åŒ–é…ç½®
            try:
                # æ·±åº¦æ¸…é™¤é…ç½®ä¸­çš„é‡åŒ–è®¾ç½®
                if hasattr(config, "quantization_config"):
                    config.quantization_config = None
                
                # æ¸…é™¤å¯èƒ½å­˜åœ¨çš„FP8æ ‡å¿—
                for attr in ["fp8", "use_fp8", "is_fp8", "quantization"]:
                    if hasattr(config, attr):
                        delattr(config, attr)
                        print(f"ğŸ§¹ å·²åˆ é™¤å±æ€§: {attr}")
                
                # æ¸…é™¤æ‰€æœ‰é‡åŒ–ç›¸å…³å±æ€§
                for key in list(config.to_dict().keys()):
                    if "quant" in key.lower() or "fp8" in key.lower():
                        delattr(config, key)
                        print(f"ğŸ§¹ å·²åˆ é™¤é…ç½®é”®: {key}")
                
                # åˆ›å»ºæ–°çš„å¹²å‡€é…ç½®å¯¹è±¡
                clean_config = AutoConfig.from_dict(config.to_dict())
                config = clean_config
                print("âœ… å·²åˆ›å»ºå…¨æ–°çš„å¹²å‡€é…ç½®å¯¹è±¡")
                
                # æ¸…é™¤æ¨¡å‹ç±»ä¸­çš„é‡åŒ–å±æ€§
                AutoModelForCausalLM.quantization_config = None
                print("âœ… å·²å½»åº•æ¸…é™¤æ‰€æœ‰é‡åŒ–é…ç½®")
            except Exception as e:
                print(f"âš ï¸ é‡åŒ–é…ç½®æ¸…é™¤å¤±è´¥: {e}")

            # ğŸ”¥ æœ€æ¿€è¿›æ–¹æ¡ˆï¼šå®Œå…¨ç»•è¿‡transformersé‡åŒ–ç³»ç»Ÿï¼Œä½¿ç”¨çº¯å‡€åŠ è½½
            print("ğŸ”§ ç»•è¿‡æ‰€æœ‰é¢„è®¾é‡åŒ–ï¼Œä½¿ç”¨çº¯å‡€4bit QLoRAé…ç½®")
            
            # æš‚æ—¶ç¦ç”¨æ‰€æœ‰é‡åŒ–é…ç½®æ£€æŸ¥
            import transformers.quantizers.auto as qa_module
            original_merge = qa_module.AutoHfQuantizer.merge_quantization_configs
            def dummy_merge(*args, **kwargs):
                print("ğŸ›‘ è·³è¿‡é‡åŒ–é…ç½®åˆå¹¶")
                return None
            qa_module.AutoHfQuantizer.merge_quantization_configs = staticmethod(dummy_merge)
            
            try:
                # é¦–å…ˆåœ¨configä¸­è®¾ç½®use_cache
                config.use_cache = False
                
                # ğŸš€ æ¿€è¿›æ˜¾å­˜ä¼˜åŒ–ï¼šæ··åˆCPU-GPUåŠ è½½
                print("ğŸ”§ åŠ è½½æ¨¡å‹ï¼ˆæ··åˆCPU-GPUæ¨¡å¼ï¼‰...")
                
                # æ¸…ç†GPUç¼“å­˜
                torch.cuda.empty_cache()
                gc.collect()
                
                # ä½¿ç”¨æ›´æ¿€è¿›çš„æ˜¾å­˜ä¼˜åŒ–é…ç½®
                model = AutoModelForCausalLM.from_pretrained(
                  model_path,
                  config=config,  # ä½¿ç”¨æˆ‘ä»¬æ¸…ç†è¿‡çš„é…ç½®
                  quantization_config=bnb_config,  # å¼ºåˆ¶ä½¿ç”¨æˆ‘ä»¬çš„4bité…ç½®
                  device_map="auto",  # è®©ç³»ç»Ÿè‡ªåŠ¨åˆ†é…CPU-GPU
                  trust_remote_code=True,
                  torch_dtype=torch.bfloat16,
                  local_files_only=True,  # ä½¿ç”¨æœ¬åœ°ç¼“å­˜æ–‡ä»¶
                  ignore_mismatched_sizes=True,  # å¿½ç•¥å¯èƒ½çš„é…ç½®ä¸åŒ¹é…
                  low_cpu_mem_usage=True,  # å¯ç”¨å†…å­˜ä¼˜åŒ–
                  max_memory={0: "70GB", "cpu": "50GB"},  # é™åˆ¶GPUä½¿ç”¨70GBï¼Œä½™ä¸‹ç”¨CPU
                  offload_folder="./offload",  # CPU offloadç›®å½•
                )
                print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼Œä½¿ç”¨DeepSeekæ–¹æ¡ˆçš„4bit NF4é‡åŒ–")
                
                # DeepSeekå¤‡é€‰æ–¹æ¡ˆï¼šåº”ç”¨é‡åŒ–è‡ªç”±åŒ…è£…å™¨
                print("ğŸ”§ åº”ç”¨DeepSeeké‡åŒ–è‡ªç”±åŒ…è£…å™¨...")
                try:
                    from transformers import ModelingMixin
                    
                    class QuantFreeModel(ModelingMixin):
                        def __init__(self, base_model):
                            super().__init__()
                            self.model = base_model
                            self.config = base_model.config
                            
                            # æ¸…é™¤é‡åŒ–æ ‡å¿—
                            self.is_quantized = False
                            if hasattr(self.config, "quantization_config"):
                                self.config.quantization_config = None
                            
                            # æ¸…é™¤æ‰€æœ‰é‡åŒ–ç›¸å…³å±æ€§
                            for attr in ["fp8", "use_fp8", "is_fp8", "quantization"]:
                                if hasattr(self.config, attr):
                                    delattr(self.config, attr)
                        
                        def forward(self, *args, **kwargs):
                            return self.model(*args, **kwargs)
                        
                        def __getattr__(self, name):
                            return getattr(self.model, name)
                    
                    # åº”ç”¨åŒ…è£…å™¨
                    model = QuantFreeModel(model)
                    print("âœ… å·²åº”ç”¨é‡åŒ–è‡ªç”±åŒ…è£…å™¨")
                except Exception as wrapper_e:
                    print(f"âš ï¸ åŒ…è£…å™¨åº”ç”¨å¤±è´¥ï¼Œç»§ç»­ä½¿ç”¨åŸæ¨¡å‹: {wrapper_e}")
                    
            finally:
                # æ¢å¤åŸå§‹å‡½æ•°
                qa_module.AutoHfQuantizer.merge_quantization_configs = original_merge
            
            # ===== æ–°å¢è®¾å¤‡ä¿®å¤ä»£ç å— =====
            print("ğŸ”§ å¼ºåˆ¶è®¾å¤‡ä¸€è‡´æ€§ä¿®å¤...")
            try:
                # ç¡®ä¿æ‰€æœ‰å‚æ•°ä¸åœ¨metaè®¾å¤‡ä¸Š
                meta_names = []
                for name, param in model.named_parameters():
                    if getattr(param, 'is_meta', False):
                        meta_names.append(name)
                        
                if meta_names:
                    print("âŒ æ£€æµ‹åˆ°ä»¥ä¸‹ meta è®¾å¤‡å‚æ•°ï¼ˆå°†ä¸­æ­¢ä»¥é¿å…è®­ç»ƒæœŸå´©æºƒï¼‰ï¼š")
                    for n in meta_names[:50]:
                        print(f"   - {n}")
                    if len(meta_names) > 50:
                        print(f"   ... å…± {len(meta_names)} é¡¹")
                    raise RuntimeError("å­˜åœ¨ meta è®¾å¤‡å‚æ•°ã€‚è¯·æ£€æŸ¥åŠ è½½è·¯å¾„/è®¾å¤‡æ˜ å°„ï¼Œç¡®ä¿ low_cpu_mem_usage=False ä¸”æœªèµ°æ‡’åˆå§‹åŒ–ã€‚")
                print("âœ… æœªå‘ç° meta è®¾å¤‡å‚æ•°")
                
                # ç¡®ä¿æ‰€æœ‰å¯è®­ç»ƒå‚æ•°åœ¨GPUä¸Š
                trainable_on_cpu = 0
                for name, param in model.named_parameters():
                    if param.requires_grad and param.device.type == "cpu":
                        print(f"ğŸ”§ ç§»åŠ¨å¯è®­ç»ƒå‚æ•°åˆ°GPU: {name}")
                        param.data = param.data.to("cuda:0")
                        trainable_on_cpu += 1
                        
                print(f"âœ… ç§»åŠ¨äº† {trainable_on_cpu} ä¸ªCPUä¸Šçš„å¯è®­ç»ƒå‚æ•°åˆ°GPU")
                
                # å¼ºåˆ¶æ¨¡å‹åˆ°GPUï¼ˆç¡®ä¿æ‰€æœ‰ç»„ä»¶éƒ½åœ¨CUDAä¸Šï¼‰
                model = model.to("cuda:0")
                print("âœ… æ¨¡å‹å·²å¼ºåˆ¶ç§»åŠ¨åˆ° cuda:0")
                
            except Exception as e:
                print(f"âš ï¸ è®¾å¤‡ä¿®å¤å¤±è´¥: {e}")
                import traceback
                traceback.print_exc()
            # ===== ç»“æŸæ–°å¢ä»£ç å— =====

            # éªŒè¯æ¨¡å‹è®¾å¤‡
            try:
                first_param_device = next(model.parameters()).device
                print(f"âœ… æ¨¡å‹é¦–ä¸ªå‚æ•°è®¾å¤‡: {first_param_device}")
            except Exception as e:
                print(f"âš ï¸ è®¾å¤‡éªŒè¯å¤±è´¥: {e}")
                
        except Exception as e:
            import traceback as _tb
            print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {repr(e)}")
            _tb.print_exc()
            return None, None, None
        
        print(f"âœ… æ¨¡å‹å·²åŠ è½½ - ä½¿ç”¨ä¿å®ˆæ˜ å°„ç­–ç•¥")

        # è§„èŒƒé‡åŒ–æ ‡è®°ï¼šå½»åº•æ¸…ç† fp8 ç—•è¿¹ï¼Œé¿å… Trainer è¯¯åˆ¤
        try:
            def _sanitize_quantization_metadata(_model):
                # é¡¶å±‚ä¸å­æ¨¡å—å¯èƒ½çš„ fp8 æ ‡å¿—ä½
                for attr in ["is_fp8", "use_fp8", "fp8", "is_quantized_fp8"]:
                    if hasattr(_model, attr):
                        try:
                            setattr(_model, attr, False)
                        except Exception:
                            pass

                # é¡¶å±‚ quantization_config
                qc = getattr(_model, "quantization_config", None)
                try:
                    # ç”¨ 4bit é…ç½®æ˜¾å¼è¦†ç›–ï¼ˆé¿å… None é€ æˆä¸‹æ¸¸ .to_dict è®¿é—®æŠ¥é”™ï¼‰
                    _model.quantization_config = bnb_config
                except Exception:
                    pass

                # config å†…çš„ quantization_config å­—æ®µ
                cfg = getattr(_model, "config", None)
                if cfg is not None:
                    try:
                        # å°† config.quantization_config ä¹ŸæŒ‡å‘ 4bit é…ç½®ï¼Œé¿å… None
                        if hasattr(cfg, "quantization_config"):
                            try:
                                setattr(cfg, "quantization_config", bnb_config)
                            except Exception:
                                pass
                    except Exception:
                        pass

                # å­æ¨¡å—ä¸Šçš„ fp8/æ ‡è®°
                try:
                    for m in _model.modules():
                        for attr in ["is_fp8", "use_fp8", "fp8", "is_quantized_fp8"]:
                            if hasattr(m, attr):
                                try:
                                    setattr(m, attr, False)
                                except Exception:
                                    pass
                except Exception:
                    pass

                # æ ‡æ³¨ä¸ºé‡åŒ–ä½†é fp8
                if hasattr(_model, "is_quantized"):
                    try:
                        _model.is_quantized = True
                    except Exception:
                        pass

            _sanitize_quantization_metadata(model)
            print("âœ… å·²æ¸…ç†é‡åŒ–å…ƒæ•°æ®ï¼Œé¿å…è¢«è¯¯åˆ¤ä¸º fp8")
        except Exception as _e:
            print(f"âš ï¸ é‡åŒ–å…ƒæ•°æ®è§„èŒƒåŒ–å¤±è´¥ï¼ˆå¿½ç•¥ï¼‰ï¼š{_e}")

        # æŒ‰ k-bit è®­ç»ƒæœ€ä½³å®è·µå‡†å¤‡æ¨¡å‹ï¼ˆå¿…é¡»åœ¨æ³¨å…¥ LoRA ä¹‹å‰ï¼‰
        try:
            # ä¸æ¢¯åº¦æ£€æŸ¥ç‚¹é…åˆï¼šç¦ç”¨ç¼“å­˜
            if hasattr(model, "config"):
                model.config.use_cache = False

            # å¼€å¯æ¨¡å‹çº§æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼ˆå†—ä½™è°ƒç”¨ä¹Ÿå®‰å…¨ï¼‰
            if hasattr(model, "gradient_checkpointing_enable"):
                model.gradient_checkpointing_enable()

            # peft å»ºè®®ï¼šä¸º k-bit è®­ç»ƒåšé¢„å¤„ç†ï¼Œå¹¶ç¡®ä¿è¾“å…¥éœ€è¦æ¢¯åº¦
            model = prepare_model_for_kbit_training(
                model, use_gradient_checkpointing=True
            )

            if hasattr(model, "enable_input_require_grads"):
                model.enable_input_require_grads()

            print("âœ… å·²å®Œæˆ k-bit è®­ç»ƒå‡†å¤‡å¹¶å¯ç”¨è¾“å…¥æ¢¯åº¦")
        except Exception as e:
            print(f"âš ï¸ k-bit è®­ç»ƒå‡†å¤‡å¤±è´¥ï¼ˆå°†ç»§ç»­ï¼‰ï¼š{e}")
        
        # ğŸ”§ FP8é‡åŒ–å…¼å®¹æ€§ä¿®å¤ - è‡ªåŠ¨è½¬æ¢ä¸å…¼å®¹çš„æ•°æ®ç±»å‹
        print("ğŸ”§ æ£€æŸ¥å¹¶ä¿®å¤FP8é‡åŒ–å…¼å®¹æ€§é—®é¢˜...")
        fp8_converted = 0
        
        # FP8å‚æ•°è½¬æ¢ + dropoutä¿®å¤
        try:
            import torch
            for name, param in model.named_parameters():
                if hasattr(param, 'dtype') and 'float8' in str(param.dtype).lower():
                    print(f"ğŸ”§ è½¬æ¢FP8å‚æ•°: {name} ä» {param.dtype} åˆ° torch.float16")
                    param.data = param.data.to(torch.float16)
                    fp8_converted += 1
        except Exception as e:
            print(f"âš ï¸ FP8å‚æ•°è½¬æ¢å¤±è´¥ï¼ˆå¿½ç•¥ï¼‰ï¼š{e}")
        
        # ç¦ç”¨æ‰€æœ‰dropoutæ¨¡å—çš„fusedæ¨¡å¼
        try:
            for name, module in model.named_modules():
                if hasattr(module, 'p') and hasattr(module, 'inplace'):  # è¿™æ˜¯Dropoutæ¨¡å—
                    if hasattr(module, 'fused'):
                        module.fused = False
                        print(f"ğŸ”§ ç¦ç”¨fused dropout: {name}")
        except Exception as e:
            print(f"âš ï¸ ç¦ç”¨fused dropoutå¤±è´¥ï¼ˆå¿½ç•¥ï¼‰ï¼š{e}")
        
        if fp8_converted > 0:
            print(f"âœ… å·²è½¬æ¢ {fp8_converted} ä¸ªFP8å‚æ•°ä¸ºè®­ç»ƒå…¼å®¹æ ¼å¼")
        else:
            print("âœ… æœªå‘ç°FP8å‚æ•°ï¼Œæˆ–å·²ç»æ˜¯å…¼å®¹æ ¼å¼")
        
        # ç®€åŒ–ï¼šä¾èµ– apply_moe_training_patch å¤„ç†é—¨æ§/MoE è·¯å¾„ï¼Œé¿å…è‡ªå®šä¹‰è¡¥ä¸å¼•å…¥è®¾å¤‡ä¸ä¸€è‡´
        
        # å…¨å±€åŒ–tokenizerä¾›format_chatä½¿ç”¨
        globals()['tokenizer'] = tokenizer
        
        # æ­£ç¡®çš„LoRAé…ç½® âœ…
        lora_config = LoraConfig(
            r=8,                    # é™ä½rankèŠ‚çœæ˜¾å­˜
            lora_alpha=16,
            target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],  # DeepSeek V3æ¶æ„
            lora_dropout=0.05,
            bias="none",
            task_type="CAUSAL_LM",
        )
        
        # åº”ç”¨LoRA
        model = get_peft_model(model, lora_config)
        model.print_trainable_parameters()

        # å°† LoRA é€‚é…å™¨æƒé‡å¯¹é½åˆ°å…¶ base_layer æ‰€åœ¨è®¾å¤‡ï¼Œé¿å… meta/cuda æ··ç”¨å¯¼è‡´çš„æ¢¯åº¦è®¾å¤‡é”™è¯¯
        try:
            moved = 0
            meta_params_fixed = 0
            
            # å½»åº•æ£€æŸ¥å¹¶ä¿®å¤æ‰€æœ‰metaè®¾å¤‡é—®é¢˜
            for name, param in model.named_parameters():
                # å°†æ‰€æœ‰metaè®¾å¤‡ä¸Šçš„å‚æ•°ç§»åŠ¨åˆ°cuda:0
                if hasattr(param, 'device') and (str(param.device) == 'meta' or param.device.type == 'meta'):
                    print(f"ğŸ”§ ä¿®å¤metaè®¾å¤‡å‚æ•°: {name} -> cuda:0")
                    # åˆ›å»ºå®é™…çš„å‚æ•°ï¼ˆä¸æ˜¯metaï¼‰
                    if param.requires_grad:
                        # å¯è®­ç»ƒå‚æ•°ï¼šéšæœºåˆå§‹åŒ–
                        new_param = torch.randn(param.shape, device='cuda:0', dtype=param.dtype) * 0.01
                    else:
                        # éå¯è®­ç»ƒå‚æ•°ï¼šé›¶åˆå§‹åŒ–
                        new_param = torch.zeros(param.shape, device='cuda:0', dtype=param.dtype)
                    param.data = new_param
                    meta_params_fixed += 1
                elif hasattr(param, 'device') and param.device.type == 'cpu' and param.requires_grad:
                    # å°†å¯è®­ç»ƒå‚æ•°ç§»åˆ°GPUä¸Šï¼Œé¿å…CPU/GPUæ··åˆè®­ç»ƒ
                    print(f"ğŸ”§ ç§»åŠ¨å¯è®­ç»ƒå‚æ•°åˆ°GPU: {name}")
                    param.data = param.data.to('cuda:0')
                    moved += 1
                
            for module in model.modules():
                base = getattr(module, 'base_layer', None)
                if base is None or not hasattr(base, 'weight'):
                    continue
                target_device = base.weight.device
                # ç¡®ä¿target_deviceä¸æ˜¯meta
                if str(target_device) == 'meta':
                    target_device = torch.device('cuda:0')
                    
                for attr in ('lora_A', 'lora_B', 'lora_embedding_A', 'lora_embedding_B'):
                    sub = getattr(module, attr, None)
                    if sub is None:
                        continue
                    # sub å¯èƒ½æ˜¯ ModuleDict/ParameterDict/Module
                    try:
                        for _, submod in getattr(sub, 'items', lambda: [])():
                            try:
                                submod.to(target_device)
                                moved += 1
                            except Exception:
                                pass
                    except TypeError:
                        # é dictï¼Œç›´æ¥ to
                        try:
                            sub.to(target_device)
                            moved += 1
                        except Exception:
                            pass
            if meta_params_fixed > 0:
                print(f"ğŸ”§ ä¿®å¤äº† {meta_params_fixed} ä¸ªmetaè®¾å¤‡å‚æ•°")
            print(f"ğŸ”§ å·²å¯¹é½ LoRA é€‚é…å™¨è®¾å¤‡ï¼Œç§»åŠ¨å­æ¨¡å—: {moved}")
        except Exception as _e:
            print(f"âš ï¸ LoRA è®¾å¤‡å¯¹é½å¤±è´¥ï¼ˆå¿½ç•¥ç»§ç»­ï¼‰ï¼š{_e}")
        
        # ===== æ–°å¢LoRAè®¾å¤‡æ£€æŸ¥ =====
        print("ğŸ” éªŒè¯LoRAè®¾å¤‡ä¸€è‡´æ€§...")
        try:
            lora_devices = set()
            base_devices = set()
            for name, module in model.named_modules():
                if hasattr(module, 'base_layer'):
                    try:
                        base_devices.add(str(module.base_layer.weight.device))
                    except Exception:
                        pass
                    # éå†è¯¥æ¨¡å—çš„å‚æ•°ï¼Œç»Ÿè®¡åŒ…å« lora_ çš„å‚æ•°è®¾å¤‡
                    for p_name, p in getattr(module, 'named_parameters', lambda: [])():
                        if 'lora_' in p_name:
                            try:
                                lora_devices.add(str(p.device))
                            except Exception:
                                pass
            print(f"ğŸ“Š LoRAè®¾å¤‡: {lora_devices}")
            print(f"ğŸ“Š åŸºç¡€å±‚è®¾å¤‡: {base_devices}")
            if len(lora_devices) > 1 or len(base_devices) > 1:
                print("âš ï¸ æ£€æµ‹åˆ°è®¾å¤‡ä¸ä¸€è‡´ï¼å¼ºåˆ¶å¯¹é½...")
                for module in model.modules():
                    if hasattr(module, 'base_layer'):
                        target_device = getattr(module.base_layer.weight, 'device', None)
                        if target_device is not None:
                            try:
                                module.to(target_device)
                            except Exception:
                                pass
            print("âœ… LoRAè®¾å¤‡ä¸€è‡´æ€§éªŒè¯å®Œæˆ")
        except Exception as e:
            print(f"âš ï¸ LoRAè®¾å¤‡æ£€æŸ¥å¤±è´¥: {e}")
        # ===== ç»“æŸæ–°å¢ä»£ç å— =====

        # ğŸ”§ ç¡®ä¿æ‰€æœ‰å¯è®­ç»ƒå‚æ•°çš„æ¢¯åº¦è¿½è¸ªæ­£ç¡®è®¾ç½®
        print("ğŸ”§ éªŒè¯å¹¶ä¿®å¤å‚æ•°æ¢¯åº¦è¿½è¸ª...")
        grad_fixed = 0
        no_grad_params = []
        
        for name, param in model.named_parameters():
            if param.requires_grad:
                # æ£€æŸ¥å‚æ•°æ˜¯å¦çœŸçš„å¯ä»¥è®¡ç®—æ¢¯åº¦
                if not param.requires_grad or param.grad_fn is None and param.is_leaf:
                    print(f"ğŸ”§ ä¿®å¤æ¢¯åº¦è¿½è¸ª: {name}")
                    param.requires_grad_(True)
                    grad_fixed += 1
            else:
                no_grad_params.append(name)
        
        if grad_fixed > 0:
            print(f"âœ… ä¿®å¤äº† {grad_fixed} ä¸ªå‚æ•°çš„æ¢¯åº¦è¿½è¸ª")
        
        print(f"ğŸ“Š æ¢¯åº¦çŠ¶æ€: {sum(p.requires_grad for p in model.parameters())} ä¸ªå¯è®­ç»ƒå‚æ•°")
        
        # ğŸ”§ éªŒè¯é—¨æ§æ¨¡å—è¡¥ä¸æ˜¯å¦ç”Ÿæ•ˆ
        print("ğŸ” éªŒè¯DeepseekExpertsGateè®­ç»ƒæ¨¡å¼è¡¥ä¸...")
        try:
            for name, module in model.named_modules():
                if "DeepseekExpertsGate" in type(module).__name__:
                    print(f"âœ… éªŒè¯é—¨æ§æ¨¡å— {name} æ”¯æŒè®­ç»ƒæ¨¡å¼")
                    module.train()  # åº”è¯¥ä¸ä¼šå¼•å‘é”™è¯¯
                    break
        except Exception as e:
            print(f"âš ï¸ é—¨æ§æ¨¡å—éªŒè¯å¤±è´¥: {e}")
        
        return model, tokenizer, lora_config
        
    except ImportError as e:
        print(f"âŒ ç¼ºå°‘ä¾èµ–: {e}")
        print("ğŸ’¡ è¯·å®‰è£…: pip install transformers peft bitsandbytes trl")
        return None, None, None
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return None, None, None

def format_chat(example):
    """ä½¿ç”¨tokenizerå†…ç½®çš„èŠå¤©æ¨¡æ¿æ ¼å¼åŒ–æ•°æ® âœ…"""
    messages = example["messages"]
    tk = globals().get("tokenizer", None)
    if tk is None:
        raise RuntimeError("tokenizer not initialized in globals()")
    return {"text": tk.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=False,
    )}

# --- Runtime patch: fix MoE training forward path so that `y` is defined in training mode ---
def apply_moe_training_patch(model):
    """Ensure DeepSeek MoE blocks have a valid training forward path.

    Some upstream DeepSeek implementations define `y` only under `if not self.training:`
    and then unconditionally use `y` later, which raises `UnboundLocalError` during training.
    This patch overrides such modules' `forward` to define a differentiable training path
    using `shared_experts(identity)` and keep the original inference path for eval.
    """
    import types

    def module_needs_patch(module):
        return (
            hasattr(module, "gate")
            and hasattr(module, "shared_experts")
            and hasattr(module, "moe_infer")
            and callable(getattr(module, "forward", None))
        )

    num_patched = 0
    for name, submodule in model.named_modules():
        try:
            if module_needs_patch(submodule):
                original_forward = submodule.forward  # noqa: F841 (kept for potential debugging)

                def patched_forward(self, hidden_states):
                    identity = hidden_states
                    orig_shape = getattr(hidden_states, "shape", None)
                    # Gate routing (used for eval path and to keep shapes consistent)
                    topk_idx, topk_weight = self.gate(hidden_states)
                    hidden_states_flat = hidden_states.view(-1, hidden_states.shape[-1])

                    if self.training:
                        # Differentiable training path: rely on shared experts only
                        y = self.shared_experts(identity)
                        return y
                    else:
                        # Preserve original eval behavior
                        y = self.moe_infer(hidden_states_flat, topk_idx, topk_weight)
                        if orig_shape is not None:
                            y = y.view(*orig_shape)
                        if getattr(self.config, "n_shared_experts", None) is not None:
                            y = y + self.shared_experts(identity)
                        return y

                submodule.forward = types.MethodType(patched_forward, submodule)
                num_patched += 1
        except Exception:
            # Best-effort patching; skip modules that fail heuristics
            continue

    print(f"ğŸ§© å·²åº”ç”¨MoEè®­ç»ƒè¡¥ä¸: {num_patched} ä¸ªæ¨¡å—")

# --- Runtime patch: disable finegrained FP8 CUDA device context when running on CPU ---
def disable_finegrained_fp8_device_context_if_cpu(model):
    try:
        first_device = next(model.parameters()).device
    except Exception:
        import torch as _torch
        first_device = _torch.device('cuda' if _torch.cuda.is_available() else 'cpu')
    if getattr(first_device, 'type', 'cpu') != 'cuda':
        try:
            import transformers.integrations.finegrained_fp8 as _fg
            from contextlib import nullcontext as _nullcontext
            class _DummyAccel:
                @staticmethod
                def device(_dev):
                    return _nullcontext()
            # Override accelerator module to a no-op on CPU
            _fg.torch_accelerator_module = _DummyAccel
            print("âœ… å·²ç¦ç”¨ finegrained_fp8 CUDA è®¾å¤‡ä¸Šä¸‹æ–‡ï¼ˆCPUè¿è¡Œç¯å¢ƒï¼‰")
        except Exception as _e:
            print(f"âš ï¸ ç¦ç”¨ finegrained_fp8 å¤±è´¥ï¼ˆå¿½ç•¥ï¼‰ï¼š{_e}")

def train_70b_qlora():
    print("ğŸ¯ å¼€å§‹DeepSeek R1 70B QLoRAå¾®è°ƒ")
    print("="*60)
    
    # ğŸš€ æ¿€è¿›æ˜¾å­˜ä¼˜åŒ–ï¼šé¢„æ¸…ç†
    torch.cuda.empty_cache()
    gc.collect()
    
    # è®¾ç½®PyTorchæ˜¾å­˜ä¼˜åŒ–
    os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "max_split_size_mb:512"
    
    # 1. ç¯å¢ƒæ£€æµ‹
    if not detect_environment():
        return False
    
    # 2. åŠ è½½æ•°æ®
    training_data = load_training_data()
    if not training_data:
        return False
    
    # 3. è®¾ç½®æ¨¡å‹
    model, tokenizer, lora_config = setup_70b_model()
    if model is None:
        return False
    # 3.1  å°†tokenizeræå‡ä¸ºå…¨å±€ï¼Œä¾› format_chat ä½¿ç”¨
    globals()['tokenizer'] = tokenizer
    
    try:
        from datasets import Dataset
        from transformers import DataCollatorForLanguageModeling, get_linear_schedule_with_warmup
        
        # 4. å‡†å¤‡æ•°æ®é›†
        dataset = Dataset.from_list(training_data)

        # ä½¿ç”¨è‡ªå®šä¹‰è®­ç»ƒå¾ªç¯ï¼Œç›´æ¥å¤„ç†æ–‡æœ¬æ ¼å¼åŒ–

        # 5. è‡ªå®šä¹‰è®­ç»ƒè¶…å‚ï¼ˆé¿å…è§¦å‘HF Trainerçš„fp8æ£€æŸ¥ï¼‰
        num_train_epochs = 3
        per_device_train_batch_size = 2
        gradient_accumulation_steps = 16
        learning_rate = 2e-5
        warmup_ratio = 0.03
        max_grad_norm = 0.3
        output_dir = "./deepseek_r1_70b_tarot_lora"

        # 6. æ–‡æœ¬æ ¼å¼åŒ–ä¸åˆ†è¯
        print("ğŸ§¾ æ­£åœ¨æ ¼å¼åŒ–ä¸åˆ†è¯æ•°æ®...")
        dataset = dataset.map(lambda ex: format_chat(ex))
        def _tokenize_fn(ex):
            enc = tokenizer(
                ex["text"],
                truncation=True,
                max_length=4096,
                padding=False,
                return_tensors=None,
            )
            return enc
        keep_cols = set(["input_ids", "attention_mask"]) | set([c for c in dataset.column_names if c in ("input_ids","attention_mask")])
        dataset = dataset.map(_tokenize_fn, remove_columns=[c for c in dataset.column_names if c not in keep_cols])

        # 7. DataLoader ä¸ collator
        collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)
        import torch.utils.data as tud
        train_loader = tud.DataLoader(dataset, batch_size=per_device_train_batch_size, shuffle=True, collate_fn=collator)
        
        # ===== æ–°å¢å‰å‘ä¼ æ’­åˆå§‹åŒ– =====
        print("ğŸ”§ æ‰§è¡Œå‰å‘ä¼ æ’­ä»¥åˆå§‹åŒ–æ‰€æœ‰å‚æ•°...")
        try:
            # å–ç¬¬ä¸€ä¸ªbatchä½œä¸ºåˆå§‹åŒ–è¾“å…¥
            init_batch = next(iter(train_loader))
            init_batch = {k: v.to(model_device) for k, v in init_batch.items()}
            
            # ä½¿ç”¨no_gradé¿å…è®¡ç®—æ¢¯åº¦
            with torch.no_grad():
                outputs = model(**init_batch)
                print(f"âœ… å‰å‘ä¼ æ’­æˆåŠŸï¼Œloss: {outputs.loss.item():.4f}")
                
            # é‡Šæ”¾å†…å­˜
            del init_batch, outputs
            torch.cuda.empty_cache()
            gc.collect()
            print("âœ… å‚æ•°åˆå§‹åŒ–å®Œæˆï¼Œç¼“å­˜å·²æ¸…é™¤")
        except Exception as e:
            print(f"âš ï¸ å‰å‘ä¼ æ’­åˆå§‹åŒ–å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
        # ===== ç»“æŸæ–°å¢ä»£ç å— =====

        # 8. ä¼˜åŒ–å™¨ä¸è°ƒåº¦å™¨
        import math
        from torch.optim import AdamW
        total_steps = math.ceil(len(train_loader) * num_train_epochs / gradient_accumulation_steps)
        optimizer = AdamW([p for p in model.parameters() if p.requires_grad], lr=learning_rate)
        scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=int(total_steps * warmup_ratio), num_training_steps=total_steps)

        # 9. æ³¨å…¥MoEè®­ç»ƒè¡¥ä¸
        try:
            apply_moe_training_patch(model)
        except Exception as _e:
            print(f"âš ï¸ MoEè®­ç»ƒè¡¥ä¸æ³¨å…¥å¤±è´¥ï¼ˆå¿½ç•¥ç»§ç»­ï¼‰ï¼š{_e}")
        # ä¸ºåŠ é€Ÿè¿›å…¥è®­ç»ƒï¼Œè·³è¿‡å¥è¯Š

        # 10. è‡ªå®šä¹‰è®­ç»ƒå¾ªç¯
        print("ğŸ‹ï¸ ä½¿ç”¨è‡ªå®šä¹‰è®­ç»ƒå¾ªç¯å¼€å§‹è®­ç»ƒ...")
        model.train()
        global_step = 0
        # ä»¥æ¨¡å‹å‚æ•°å®é™…è®¾å¤‡ä¸ºå‡†ï¼Œå†³å®šæ˜¯å¦å¯ç”¨cuda autocast
        try:
            model_device = next(model.parameters()).device
        except Exception:
            model_device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        use_cuda = (getattr(model_device, 'type', 'cpu') == 'cuda')
        print(f"ğŸ–¥ï¸ model device: {model_device}, use_cuda_autocast={use_cuda}")
        for epoch in range(num_train_epochs):
            print(f"ğŸ“† Epoch {epoch+1}/{num_train_epochs}")
            optimizer.zero_grad(set_to_none=True)
            for step, batch in enumerate(train_loader, start=1):
                # å°† batch å¼ é‡è¿ç§»åˆ°ä¸æ¨¡å‹åŒè®¾å¤‡
                # å†æŒ‰éœ€è¦å¼€å¯ autocastï¼ˆä»…åœ¨ cuda ä¸‹ï¼‰
                with torch.autocast(device_type="cuda", dtype=torch.bfloat16, enabled=use_cuda):
                    # DataCollatorForLanguageModeling å·²ç»è®¾ç½®äº† labelsï¼Œä¸éœ€è¦é‡å¤ä¼ é€’
                    try:
                        batch = {k: (v.to(model_device) if hasattr(v, 'to') else v) for k, v in batch.items()}
                    except Exception:
                        pass
                    outputs = model(**batch)  # type: ignore
                    loss = outputs.loss / gradient_accumulation_steps
                loss.backward()

                # ===== æ–°å¢æ¢¯åº¦éªŒè¯ =====
                # æ£€æŸ¥æ¢¯åº¦è®¾å¤‡ä¸€è‡´æ€§
                invalid_grads = []
                valid_grads = 0

                for name, param in model.named_parameters():
                    if param.requires_grad and param.grad is not None:
                        if param.grad.device != param.device:
                            print(f"âš ï¸ æ¢¯åº¦è®¾å¤‡ä¸ä¸€è‡´: {name} "
                                  f"(å‚æ•°è®¾å¤‡: {param.device}, æ¢¯åº¦è®¾å¤‡: {param.grad.device})")
                            invalid_grads.append(name)
                        else:
                            valid_grads += 1

                if invalid_grads:
                    print(f"âŒ å‘ç° {len(invalid_grads)} ä¸ªæ¢¯åº¦è®¾å¤‡ä¸ä¸€è‡´çš„å‚æ•°!")
                    # å°è¯•ä¿®å¤ï¼šå°†æ¢¯åº¦ç§»åŠ¨åˆ°å‚æ•°æ‰€åœ¨è®¾å¤‡
                    for name in invalid_grads:
                        param = dict(model.named_parameters())[name]
                        param.grad = param.grad.to(param.device)
                    print("ğŸ› ï¸ å·²å°è¯•ä¿®å¤æ¢¯åº¦è®¾å¤‡")
                else:
                    print(f"âœ… æ¢¯åº¦è®¾å¤‡ä¸€è‡´: {valid_grads} ä¸ªå‚æ•°")
                # ===== ç»“æŸæ–°å¢ä»£ç å— =====

                if step % gradient_accumulation_steps == 0:
                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)
                    optimizer.step()
                    scheduler.step()
                    optimizer.zero_grad(set_to_none=True)
                    global_step += 1
                    if global_step % 10 == 0:
                        scaled = (loss.detach().float() * gradient_accumulation_steps).item()
                        print(f"step {global_step}/{total_steps}, loss={scaled:.4f}")

        # 11. ä¿å­˜æ¨¡å‹
        print("ğŸ’¾ ä¿å­˜LoRAæƒé‡...")
        os.makedirs(output_dir, exist_ok=True)
        model.save_pretrained(output_dir)
        try:
            tokenizer.save_pretrained(output_dir)
        except Exception:
            pass
        
        # 9. æµ‹è¯•
        print("ğŸ§ª æµ‹è¯•æ¨¡å‹...")
        test_messages = [
            {"role": "user", "content": "è¯·è§£è¯»å¡”ç½—ç‰Œï¼šæ„šäºº(æ­£ä½)"}
        ]
        inputs = tokenizer.apply_chat_template(
            test_messages,
            return_tensors="pt"
        ).to(model.device)
        
        outputs = model.generate(
            inputs,
            max_new_tokens=300,
            temperature=0.7,
            top_p=0.9,
            do_sample=True,
            pad_token_id=tokenizer.eos_token_id
        )
        
        response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        print("ğŸ“ æµ‹è¯•è¾“å‡º:")
        print("-" * 50)
        print(response)
        print("-" * 50)
        
        print("âœ… 70B QLoRAè®­ç»ƒå®Œæˆï¼")
        return True
        
    except Exception as e:
        print(f"âŒ è®­ç»ƒå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    print("ğŸŒŸ DeepSeek R1 70B QLoRA å¡”ç½—ç‰Œå¾®è°ƒç³»ç»Ÿ")
    print("ğŸ”— ä¸“ä¸ºLambda GPUäº‘å¹³å°ä¼˜åŒ–")
    print()
    
    success = train_70b_qlora()
    if success:
        print("ğŸ‰ è®­ç»ƒæˆåŠŸå®Œæˆï¼")
        print("ğŸ“ æ¨¡å‹ä¿å­˜åœ¨: ./deepseek_r1_70b_tarot_lora/")
    else:
        print("âŒ è®­ç»ƒå¤±è´¥ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯")
#!/usr/bin/env python3
"""
DeepSeek R1 70B QLoRA å¾®è°ƒè„šæœ¬ - å…¨æ–°ä¼˜åŒ–ç‰ˆ
è®¾è®¡åŸåˆ™ï¼š
1. å½»åº•è§£å†³FP8é‡åŒ–å†²çª
2. ä¼˜åŒ–è®¾å¤‡æ˜ å°„é¿å…metaè®¾å¤‡é—®é¢˜
3. ç®€åŒ–æ¨¡å‹åŠ è½½æµç¨‹
4. å¢å¼ºé”™è¯¯å¤„ç†å’Œæ—¥å¿—
"""
import os
import sys
import torch
import json
import gc
import logging
from datetime import datetime
from pathlib import Path

# é…ç½®è¯¦ç»†æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler("training.log")
    ]
)
logger = logging.getLogger(__name__)

# å¼ºåˆ¶å®æ—¶è¾“å‡º
sys.stdout.reconfigure(line_buffering=True)
sys.stderr.reconfigure(line_buffering=True)
os.environ['PYTHONUNBUFFERED'] = '1'
os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:512'

def detect_environment():
    """æ£€æµ‹GPUç¯å¢ƒå¹¶è¿”å›è®¾å¤‡ä¿¡æ¯"""
    logger.info("ğŸ” æ£€æµ‹ç¯å¢ƒ...")
    if torch.cuda.is_available():
        device_count = torch.cuda.device_count()
        for i in range(device_count):
            gpu_name = torch.cuda.get_device_name(i)
            total_memory = torch.cuda.get_device_properties(i).total_memory / (1024**3)
            logger.info(f"GPU {i}: {gpu_name} | æ˜¾å­˜: {total_memory:.1f}GB")
        return True
    logger.error("âŒ éœ€è¦CUDA GPUç¯å¢ƒ")
    return False

def load_training_data(data_path="training_data.jsonl"):
    """åŠ è½½è®­ç»ƒæ•°æ®"""
    logger.info(f"ğŸ“‚ åŠ è½½è®­ç»ƒæ•°æ®: {data_path}")
    if not os.path.exists(data_path):
        logger.error(f"âŒ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_path}")
        return None
    
    try:
        data = []
        with open(data_path, 'r', encoding='utf-8') as f:
            for line in f:
                item = json.loads(line.strip())
                if 'messages' in item:
                    data.append(item)
        logger.info(f"âœ… åŠ è½½å®Œæˆ: {len(data)} æ¡è®­ç»ƒæ•°æ®")
        return data
    except Exception as e:
        logger.error(f"âŒ æ•°æ®åŠ è½½å¤±è´¥: {e}")
        return None

def load_model_safely(model_id):
    """å®‰å…¨åŠ è½½æ¨¡å‹ï¼Œé¿å…FP8é‡åŒ–å†²çª"""
    from transformers import AutoModelForCausalLM, AutoConfig, BitsAndBytesConfig
    
    # æ¸…ç†GPUç¼“å­˜
    torch.cuda.empty_cache()
    gc.collect()
    
    # åˆ›å»ºå¹²å‡€çš„é…ç½®å¯¹è±¡
    logger.info("ğŸ§¹ åˆ›å»ºå¹²å‡€é…ç½®å¯¹è±¡...")
    config = AutoConfig.from_pretrained(model_id, trust_remote_code=True)
    
    # æ¸…é™¤æ‰€æœ‰é‡åŒ–ç›¸å…³å±æ€§
    for attr in ["quantization_config", "fp8", "use_fp8", "is_fp8"]:
        if hasattr(config, attr):
            delattr(config, attr)
            logger.info(f"  å·²åˆ é™¤å±æ€§: {attr}")
    
    # åœ¨é…ç½®ä¸­è®¾ç½®use_cacheè€Œä¸æ˜¯åœ¨from_pretrainedä¸­
    config.use_cache = False
    
    # ğŸ”§ å…³é”®ä¿®å¤ï¼šå…ˆåŠ è½½æ¨¡å‹åˆ°CPUï¼Œç„¶åå¤„ç†FP8ï¼Œæœ€åé‡åŒ–
    logger.info("ğŸ”§ Step 1: å…ˆåŠ è½½æ¨¡å‹åˆ°CPUï¼ˆæ— é‡åŒ–ï¼‰...")
    model = AutoModelForCausalLM.from_pretrained(
        model_id,
        config=config,
        device_map="cpu",  # å¼ºåˆ¶CPUåŠ è½½
        trust_remote_code=True,
        torch_dtype=torch.float16,  # ä½¿ç”¨float16é¿å…FP8é—®é¢˜
        low_cpu_mem_usage=True,
    )
    
    # ğŸ”§ Step 2: FP8å‚æ•°è½¬æ¢
    logger.info("ğŸ”§ Step 2: è¿›è¡ŒFP8å‚æ•°è½¬æ¢...")
    fp8_converted = 0
    
    try:
        for name, param in model.named_parameters():
            if hasattr(param, 'dtype') and 'float8' in str(param.dtype).lower():
                logger.info(f"ğŸ”§ è½¬æ¢FP8å‚æ•°: {name} ä» {param.dtype} åˆ° torch.float16")
                param.data = param.data.to(torch.float16)
                fp8_converted += 1
                
    except Exception as e:
        logger.warning(f"âš ï¸ FP8å‚æ•°è½¬æ¢é‡åˆ°é—®é¢˜ï¼ˆç»§ç»­ï¼‰ï¼š{e}")
    
    logger.info(f"âœ… FP8è½¬æ¢å®Œæˆï¼è½¬æ¢äº† {fp8_converted} ä¸ªå‚æ•°")
    
    # ğŸ”§ Step 3: ç°åœ¨åº”ç”¨é‡åŒ–å¹¶ç§»åŠ¨åˆ°GPU
    logger.info("ğŸ”§ Step 3: åº”ç”¨é‡åŒ–é…ç½®...")
    
    # è®¾ç½®4-bité‡åŒ–é…ç½®ï¼Œå¯ç”¨CPU offload
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_use_double_quant=True,
        bnb_4bit_compute_dtype=torch.bfloat16,
        llm_int8_enable_fp32_cpu_offload=True,  # å¯ç”¨CPU offload
    )
    
    # æ‰‹åŠ¨é‡åŒ–æ¨¡å‹
    from transformers.quantizers import AutoHfQuantizer
    hf_quantizer = AutoHfQuantizer.from_config(bnb_config)
    model = hf_quantizer.quantize_model(model, device_map="auto")
    
    return model, bnb_config

def setup_70b_model():
    """è®¾ç½®DeepSeek R1 70Bæ¨¡å‹"""
    logger.info("ğŸš€ è®¾ç½®DeepSeek R1 70Bæ¨¡å‹...")
    try:
        from transformers import AutoTokenizer
        from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
        
        # æ¨¡å‹ID
        model_id = "deepseek-ai/DeepSeek-R1"
        
        # ä¸´æ—¶ç¦ç”¨ç¦»çº¿æ¨¡å¼
        for var in ["HF_HUB_OFFLINE", "TRANSFORMERS_OFFLINE"]:
            if var in os.environ:
                logger.info(f"ğŸ”“ ä¸´æ—¶ç¦ç”¨: {var}")
                os.environ.pop(var, None)
        
        # åŠ è½½tokenizer
        logger.info("ğŸ”¹ åŠ è½½tokenizer...")
        tokenizer = AutoTokenizer.from_pretrained(
            model_id, 
            trust_remote_code=True,
            padding_side="right"
        )
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        # å®‰å…¨åŠ è½½æ¨¡å‹
        model, bnb_config = load_model_safely(model_id)
        
        # å‡†å¤‡k-bitè®­ç»ƒ
        logger.info("ğŸ”§ å‡†å¤‡æ¨¡å‹è¿›è¡ŒQLoRAè®­ç»ƒ...")
        model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=True)
        
        # å¯ç”¨è¾“å…¥æ¢¯åº¦
        if hasattr(model, "enable_input_require_grads"):
            model.enable_input_require_grads()
        
        # é…ç½®LoRA
        lora_config = LoraConfig(
            r=8,
            lora_alpha=16,
            target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
            lora_dropout=0.05,
            bias="none",
            task_type="CAUSAL_LM",
        )
        
        # åº”ç”¨LoRA
        logger.info("ğŸ¯ åº”ç”¨LoRAé€‚é…å™¨...")
        model = get_peft_model(model, lora_config)
        model.print_trainable_parameters()
        
        # å…¨å±€åŒ–tokenizerä¾›åç»­ä½¿ç”¨
        globals()['tokenizer'] = tokenizer
        
        return model, tokenizer, lora_config

    except ImportError as e:
        logger.error(f"âŒ ç¼ºå°‘ä¾èµ–: {e}")
        logger.info("ğŸ’¡ è¯·å®‰è£…: pip install transformers peft bitsandbytes accelerate")
        return None, None, None
    except Exception as e:
        logger.error(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return None, None, None

def format_chat(example):
    """ä½¿ç”¨tokenizerå†…ç½®çš„èŠå¤©æ¨¡æ¿æ ¼å¼åŒ–æ•°æ®"""
    messages = example["messages"]
    tk = globals().get("tokenizer", None)
    if tk is None:
        raise RuntimeError("tokenizer not initialized in globals()")
    return {"text": tk.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=False,
    )}

def train_70b_qlora():
    """ä¸»è®­ç»ƒå‡½æ•°"""
    logger.info("ğŸ¯ å¼€å§‹DeepSeek R1 70B QLoRAå¾®è°ƒ")
    logger.info("=" * 60)
    
    # 1. ç¯å¢ƒæ£€æµ‹
    if not detect_environment():
        return False
    
    # 2. åŠ è½½æ•°æ®
    training_data = load_training_data()
    if not training_data:
        return False
    
    # 3. è®¾ç½®æ¨¡å‹
    model, tokenizer, lora_config = setup_70b_model()
    if model is None:
        return False
    
    # ç¡®ä¿tokenizerå…¨å±€å¯ç”¨
    globals()['tokenizer'] = tokenizer
    
    try:
        from datasets import Dataset
        from transformers import DataCollatorForLanguageModeling, Trainer, TrainingArguments
        from transformers.trainer_utils import get_last_checkpoint
        
        # 4. å‡†å¤‡æ•°æ®é›†
        dataset = Dataset.from_list(training_data)
        dataset = dataset.map(format_chat)
        
        # 5. åˆ†è¯
        def tokenize_function(examples):
            return tokenizer(
                examples["text"],
                truncation=True,
                max_length=4096,
                padding=False,
            )
        
        tokenized_dataset = dataset.map(
            tokenize_function,
            remove_columns=dataset.column_names,
            desc="Tokenizing",
        )
        
        # 6. æ•°æ®æ•´ç†å™¨
        data_collator = DataCollatorForLanguageModeling(
            tokenizer=tokenizer,
            mlm=False,
        )
        
        # 7. è®­ç»ƒå‚æ•°
        training_args = TrainingArguments(
            output_dir="./deepseek_r1_70b_tarot_lora",
            num_train_epochs=3,
            per_device_train_batch_size=1,  # æ ¹æ®æ˜¾å­˜è°ƒæ•´
            gradient_accumulation_steps=16,
            learning_rate=2e-5,
            weight_decay=0.01,
            warmup_ratio=0.03,
            lr_scheduler_type="linear",
            save_strategy="epoch",
            logging_steps=10,
            fp16=False,
            bf16=True,  # ä½¿ç”¨bfloat16
            tf32=True,   # åœ¨Ampere GPUä¸Šå¯ç”¨tf32
            gradient_checkpointing=True,
            report_to="none",
            optim="paged_adamw_8bit",  # ä½¿ç”¨åˆ†é¡µä¼˜åŒ–å™¨
            max_grad_norm=0.3,
        )
        
        # 8. åˆå§‹åŒ–Trainer
        trainer = Trainer(
            model=model,
            args=training_args,
            train_dataset=tokenized_dataset,
            data_collator=data_collator,
        )
        
        # 9. è®­ç»ƒ
        logger.info("ğŸ‹ï¸ å¼€å§‹è®­ç»ƒ...")
        trainer.train()
        
        # 10. ä¿å­˜æ¨¡å‹
        logger.info("ğŸ’¾ ä¿å­˜æ¨¡å‹...")
        trainer.save_model()
        tokenizer.save_pretrained(training_args.output_dir)
        
        # 11. æµ‹è¯•æ¨¡å‹
        logger.info("ğŸ§ª æµ‹è¯•æ¨¡å‹...")
        test_messages = [
            {"role": "user", "content": "è¯·è§£è¯»å¡”ç½—ç‰Œï¼šæ„šäºº(æ­£ä½)"}
        ]
        inputs = tokenizer.apply_chat_template(
            test_messages,
            return_tensors="pt"
        ).to(model.device)
        
        outputs = model.generate(
            inputs,
            max_new_tokens=300,
            temperature=0.7,
            top_p=0.9,
            do_sample=True,
            pad_token_id=tokenizer.eos_token_id
        )
        
        response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        logger.info("ğŸ“ æµ‹è¯•è¾“å‡º:")
        logger.info("-" * 50)
        logger.info(response)
        logger.info("-" * 50)
        
        logger.info("âœ… è®­ç»ƒå®Œæˆï¼")
        return True
        
    except Exception as e:
        logger.error(f"âŒ è®­ç»ƒå¤±è´¥: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return False

if __name__ == "__main__":
    print("ğŸŒŸ DeepSeek R1 70B QLoRA å¡”ç½—ç‰Œå¾®è°ƒç³»ç»Ÿ")
    print("=" * 60)
    
    # æ¸…é™¤CUDAç¼“å­˜
    torch.cuda.empty_cache()
    gc.collect()
    
    success = train_70b_qlora()
    if success:
        print("ğŸ‰ è®­ç»ƒæˆåŠŸå®Œæˆï¼")
        print("ğŸ“ æ¨¡å‹ä¿å­˜åœ¨: ./deepseek_r1_70b_tarot_lora/")
    else:
        print("âŒ è®­ç»ƒå¤±è´¥ï¼Œè¯·æ£€æŸ¥æ—¥å¿—æ–‡ä»¶ training.log")
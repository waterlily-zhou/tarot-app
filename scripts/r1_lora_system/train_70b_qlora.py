#!/usr/bin/env python3
"""
DeepSeek R1 70B QLoRA å¾®è°ƒè„šæœ¬ - ä¼˜åŒ–ç‰ˆ
"""
import os
import sys
import torch
import json
from pathlib import Path
import gc
from datetime import datetime

def detect_environment():
    print("ğŸ” æ£€æµ‹ç¯å¢ƒ...")
    if torch.cuda.is_available():
        device_count = torch.cuda.device_count()
        for i in range(device_count):
            gpu_name = torch.cuda.get_device_name(i)
            total_memory = torch.cuda.get_device_properties(i).total_memory / (1024**3)
            print(f"GPU {i}: {gpu_name} | æ˜¾å­˜: {total_memory:.1f}GB")
        return True
    print("âŒ éœ€è¦CUDA GPUç¯å¢ƒ")
    return False

def load_training_data(data_path="training_data.jsonl"):
    print(f"ğŸ“‚ åŠ è½½è®­ç»ƒæ•°æ®: {data_path}")
    if not os.path.exists(data_path):
        print(f"âŒ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_path}")
        return None
    
    try:
        data = []
        with open(data_path, 'r', encoding='utf-8') as f:
            for line in f:
                item = json.loads(line.strip())
                # ä¿ç•™åŸå§‹messagesç»“æ„ âœ…
                if 'messages' in item:
                    data.append(item)
        print(f"âœ… åŠ è½½å®Œæˆ: {len(data)} æ¡è®­ç»ƒæ•°æ®")
        return data
    except Exception as e:
        print(f"âŒ æ•°æ®åŠ è½½å¤±è´¥: {e}")
        return None

def setup_70b_model():
    print("ğŸš€ è®¾ç½®DeepSeek R1 70Bæ¨¡å‹...")
    
    # ğŸ”§ è°ƒè¯•ç‰ˆæœ¬ï¼šè¯¦ç»†æ£€æŸ¥ç¼“å­˜æ–‡ä»¶çŠ¶æ€
    print("ğŸ”§ è°ƒè¯•æ¨¡å¼ï¼šè¯¦ç»†æ£€æŸ¥ç¼“å­˜çŠ¶æ€...")
    
    try:
        import glob
        import re
        import os
        
        # å…ˆæ£€æŸ¥ç¼“å­˜æ ¹ç›®å½•
        cache_root = "/home/ubuntu/.cache/huggingface"
        print(f"ğŸ“ ç¼“å­˜æ ¹ç›®å½•: {cache_root}")
        print(f"ğŸ“ ç¼“å­˜æ ¹ç›®å½•å­˜åœ¨: {os.path.exists(cache_root)}")
        
        if os.path.exists(cache_root):
            # åˆ—å‡ºæ‰€æœ‰å­ç›®å½•
            print("ğŸ“‚ ç¼“å­˜ç›®å½•ç»“æ„:")
            for root, dirs, files in os.walk(cache_root):
                level = root.replace(cache_root, '').count(os.sep)
                indent = ' ' * 2 * level
                print(f"{indent}{os.path.basename(root)}/")
                if level < 3:  # é™åˆ¶æ·±åº¦é¿å…è¾“å‡ºè¿‡å¤š
                    subindent = ' ' * 2 * (level + 1)
                    for file in files[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ªæ–‡ä»¶
                        print(f"{subindent}{file}")
                    if len(files) > 5:
                        print(f"{subindent}... (+{len(files)-5} more files)")
        
        # å°è¯•å¤šç§å¯èƒ½çš„è·¯å¾„æ¨¡å¼
        patterns = [
            "/home/ubuntu/.cache/huggingface/modules/transformers_modules/*/modeling_deepseek.py",
            "/home/ubuntu/.cache/huggingface/hub/models--*/snapshots/*/modeling_deepseek.py",
            "/home/ubuntu/.cache/huggingface/hub/*/modeling_deepseek.py",
            "/home/ubuntu/.cache/huggingface/*/modeling_deepseek.py"
        ]
        
        all_modeling_files = []
        for pattern in patterns:
            files = glob.glob(pattern)
            print(f"ğŸ” æœç´¢æ¨¡å¼ '{pattern}': æ‰¾åˆ° {len(files)} ä¸ªæ–‡ä»¶")
            all_modeling_files.extend(files)
        
        # å»é‡
        all_modeling_files = list(set(all_modeling_files))
        print(f"ğŸ“‹ æ€»å…±æ‰¾åˆ° {len(all_modeling_files)} ä¸ªmodeling_deepseek.pyæ–‡ä»¶")
        
        for file_path in all_modeling_files:
            print(f"ğŸ“„ æ£€æŸ¥æ–‡ä»¶: {file_path}")
            print(f"ğŸ“„ æ–‡ä»¶å­˜åœ¨: {os.path.exists(file_path)}")
            
            if os.path.exists(file_path):
                # æ£€æŸ¥æ–‡ä»¶æƒé™
                print(f"ğŸ“„ æ–‡ä»¶æƒé™: {oct(os.stat(file_path).st_mode)}")
                
                # è¯»å–æ–‡ä»¶å†…å®¹
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                # æ£€æŸ¥æ–‡ä»¶å¤§å°å’Œå…³é”®å†…å®¹
                print(f"ğŸ“„ æ–‡ä»¶å¤§å°: {len(content)} å­—ç¬¦")
                
                # æ™ºèƒ½ç»Ÿè®¡ï¼šåªç»Ÿè®¡æœªæ³¨é‡Šçš„assertè¯­å¥
                lines = content.split('\n')
                active_assert_count = 0
                commented_assert_count = 0
                
                for line in lines:
                    if 'assert not self.training' in line:
                        if line.strip().startswith('#'):
                            commented_assert_count += 1
                            print(f"âœ… å·²æ³¨é‡Šçš„assert: {line.strip()}")
                        else:
                            active_assert_count += 1
                            print(f"âš ï¸ æ´»è·ƒçš„assert: {line.strip()}")
                
                print(f"ğŸ” æ´»è·ƒassert: {active_assert_count}, å·²æ³¨é‡Šassert: {commented_assert_count}")
                assert_count = active_assert_count
                
                if assert_count > 0:
                    print(f"ğŸ”§ ä¿®å¤æ–‡ä»¶ {file_path}...")
                    
                    # æ£€æŸ¥æ˜¯å¦å·²ç»è¢«ä¿®å¤è¿‡ï¼ˆé¿å…é‡å¤å¤„ç†ï¼‰
                    if '# å·²æ³¨é‡Šï¼šå…è®¸è®­ç»ƒæ¨¡å¼' in content:
                        print(f"âš ï¸ æ–‡ä»¶å·²è¢«ä¿®å¤è¿‡ï¼Œå°è¯•æ¢å¤åŸæ–‡ä»¶...")
                        
                        # å°è¯•ä»å¤‡ä»½æ¢å¤
                        backup_path = file_path + '.backup'
                        if os.path.exists(backup_path):
                            with open(backup_path, 'r', encoding='utf-8') as f:
                                content = f.read()
                            print(f"âœ… å·²ä»å¤‡ä»½æ¢å¤: {backup_path}")
                        else:
                            print(f"âš ï¸ å¤‡ä»½æ–‡ä»¶ä¸å­˜åœ¨ï¼Œæ‰‹åŠ¨æ¸…ç†é‡å¤æ³¨é‡Š...")
                            # æ‰‹åŠ¨æ¸…ç†é‡å¤æ³¨é‡Š
                            lines = content.split('\n')
                            cleaned_lines = []
                            for line in lines:
                                if '# å·²æ³¨é‡Šï¼šå…è®¸è®­ç»ƒæ¨¡å¼' in line:
                                    # å°è¯•æ¢å¤åŸå§‹assertè¯­å¥
                                    if 'assert not self.training' in line:
                                        # æ‰¾åˆ°åŸå§‹ç¼©è¿›
                                        indent_match = line.split('#')[0]
                                        cleaned_line = indent_match + 'assert not self.training'
                                        cleaned_lines.append(cleaned_line)
                                        print(f"ğŸ”„ æ¢å¤è¡Œ: {cleaned_line.strip()}")
                                    else:
                                        cleaned_lines.append(line)
                                else:
                                    cleaned_lines.append(line)
                            content = '\n'.join(cleaned_lines)
                    
                    # é‡æ–°è®¡ç®—assertæ•°é‡
                    assert_count = content.count('assert not self.training')
                    print(f"ğŸ” æ¸…ç†åæ‰¾åˆ° {assert_count} ä¸ªassertè¯­å¥")
                    
                    if assert_count > 0:
                        # å¤‡ä»½åŸæ–‡ä»¶ï¼ˆè¦†ç›–ä¹‹å‰çš„å¤‡ä»½ï¼‰
                        backup_path = file_path + '.backup'
                        with open(backup_path, 'w', encoding='utf-8') as f:
                            f.write(content)
                        print(f"ğŸ’¾ å·²é‡æ–°å¤‡ä»½åˆ°: {backup_path}")
                        
                        # è¶…çº§è°ƒè¯•ï¼šæŸ¥çœ‹æ¯ä¸€è¡Œå†…å®¹
                        import re
                        
                        lines = content.split('\n')
                        modified_lines = []
                        changes_made = 0
                        
                        print("ğŸ” è¶…çº§è°ƒè¯•æ¨¡å¼ï¼šæŸ¥çœ‹åŒ…å«'training'çš„æ‰€æœ‰è¡Œ")
                        for i, line in enumerate(lines):
                            if 'training' in line.lower():
                                print(f"  ç¬¬{i+1}è¡Œ: '{line}'")
                                print(f"    åŸå§‹å­—èŠ‚: {repr(line)}")
                                print(f"    stripped: '{line.strip()}'")
                                if 'assert' in line:
                                    print(f"    ğŸ¯ è¿™è¡ŒåŒ…å«assert!")
                        
                        print("\nğŸ”§ å°è¯•å¤šç§åŒ¹é…æ¨¡å¼...")
                        patterns = [
                            r'^(\s*)assert\s+not\s+self\.training(.*)$',
                            r'^\s*assert\s+not\s+self\.training.*$',
                            r'assert\s+not\s+self\.training',
                            r'.*assert.*not.*self\.training.*',
                        ]
                        
                        for i, line in enumerate(lines):
                            line_matched = False
                            
                            for j, pattern in enumerate(patterns):
                                if re.search(pattern, line, re.IGNORECASE):
                                    print(f"ğŸ¯ ç¬¬{i+1}è¡ŒåŒ¹é…æ¨¡å¼{j+1}: '{line.strip()}'")
                                    line_matched = True
                                    
                                    # åªç”¨ç¬¬ä¸€ä¸ªæ¨¡å¼è¿›è¡Œå®é™…ä¿®æ”¹
                                    if j == 0 and not line.strip().startswith('#'):
                                        print(f"ğŸ”§ åº”ç”¨ä¿®æ”¹...")
                                        modified_line = '        # ' + line.strip() + '  # å·²æ³¨é‡Šï¼šå…è®¸è®­ç»ƒæ¨¡å¼'
                                        modified_lines.append(modified_line)
                                        changes_made += 1
                                        break
                            
                            if not line_matched:
                                modified_lines.append(line)
                            elif not any(re.search(patterns[0], line, re.IGNORECASE) for _ in [1]):
                                modified_lines.append(line)
                        
                        new_content = '\n'.join(modified_lines)
                        print(f"ğŸ“Š æœ¬æ¬¡ä¿®æ”¹äº† {changes_made} è¡Œ")
                        
                        # å†™å›æ–‡ä»¶
                        with open(file_path, 'w', encoding='utf-8') as f:
                            f.write(new_content)
                        
                        # éªŒè¯ä¿®æ”¹
                        with open(file_path, 'r', encoding='utf-8') as f:
                            verify_content = f.read()
                        
                        final_assert_count = verify_content.count('assert not self.training')
                        comment_count = verify_content.count('# assert not self.training')
                        
                        print(f"âœ… ä¿®å¤å®Œæˆï¼å‰©ä½™æ–­è¨€: {final_assert_count}")
                        print(f"âœ… æ³¨é‡Šæ•°é‡: {comment_count}")
                    else:
                        print(f"âœ… æ— éœ€ä¿®å¤")
                else:
                    print(f"âœ… æ–‡ä»¶æ­£å¸¸ï¼Œæ— éœ€ä¿®å¤")
                    
        if not all_modeling_files:
            print("âŒ æœªæ‰¾åˆ°ä»»ä½•modeling_deepseek.pyæ–‡ä»¶ï¼")
            print("ğŸ” å°è¯•æ‰‹åŠ¨æŸ¥æ‰¾...")
            
            # æ‰‹åŠ¨é€’å½’æŸ¥æ‰¾
            for root, dirs, files in os.walk("/home/ubuntu/.cache"):
                for file in files:
                    if file == "modeling_deepseek.py":
                        full_path = os.path.join(root, file)
                        print(f"ğŸ¯ æ‰‹åŠ¨æ‰¾åˆ°: {full_path}")
            
    except Exception as e:
        print(f"âŒ è°ƒè¯•å¤±è´¥: {e}")
        import traceback
        print("ğŸ” è¯¦ç»†é”™è¯¯:")
        traceback.print_exc()
    
    try:
        from transformers import (
            AutoTokenizer, 
            AutoModelForCausalLM, 
            AutoConfig,
            BitsAndBytesConfig,
            DataCollatorForLanguageModeling,
            get_linear_schedule_with_warmup
        )
        from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
        from datasets import Dataset
        from accelerate import infer_auto_device_map, dispatch_model
        import os
        import torch
        
        # ä½¿ç”¨æœ¬åœ°å¹²å‡€å¿«ç…§è·¯å¾„
        model_path = os.path.expanduser("~/.cache/huggingface/hub/models--deepseek-ai--DeepSeek-R1-0528/snapshots")
        # æ‰¾åˆ°æœ€æ–°çš„å¿«ç…§ç›®å½•ï¼ˆæŒ‰ä¿®æ”¹æ—¶é—´æ’åºï¼Œä¼˜å…ˆé€‰æ‹©æœ€æ–°çš„ï¼‰
        if os.path.exists(model_path):
            snapshot_dirs = [os.path.join(model_path, d) for d in os.listdir(model_path)
                             if os.path.isdir(os.path.join(model_path, d))]
            if snapshot_dirs:
                snapshot_dirs.sort(key=lambda p: os.path.getmtime(p), reverse=True)
                model_path = snapshot_dirs[0]
            else:
                raise FileNotFoundError("No snapshot found in cache")
        else:
            raise FileNotFoundError(f"Cache directory not found: {model_path}")
        
        print(f"âœ… ä½¿ç”¨æœ¬åœ°å¿«ç…§: {model_path}")

        # å¿«ç…§å®Œæ•´æ€§é¢„æ£€ï¼šè‹¥åˆ†ç‰‡ä¸å…¨ï¼Œåœ¨çº¿è¡¥å…¨ä¸‹è½½
        try:
            import glob as _glob
            import re as _re
            shard_paths = sorted(_glob.glob(os.path.join(model_path, 'model-*-of-*.safetensors')))
            if shard_paths:
                total = int(_re.search(r'-of-(\d+)', os.path.basename(shard_paths[0])).group(1))
                have_nums = {int(_re.search(r'model-(\d+)-of-', os.path.basename(p)).group(1)) for p in shard_paths}
                missing = [i for i in range(1, total + 1) if i not in have_nums]
                if missing:
                    print(f"âš ï¸ æœ¬åœ°å¿«ç…§ç¼ºå°‘ {len(missing)}/{total} ä¸ªåˆ†ç‰‡ï¼Œå‡†å¤‡åœ¨çº¿è¡¥å…¨...")
                    # ä¸´æ—¶å…³é—­ç¦»çº¿å˜é‡
                    import os as _os
                    for _k in ["HF_HUB_OFFLINE", "TRANSFORMERS_OFFLINE", "HF_DATASETS_OFFLINE"]:
                        if _k in _os.environ:
                            print(f"ğŸ”“ ä¸´æ—¶å…³é—­ç¯å¢ƒå˜é‡: {_k}")
                            _os.environ.pop(_k, None)
                    # ä½¿ç”¨ huggingface_hub è¡¥å…¨ä¸‹è½½
                    from huggingface_hub import snapshot_download as _snapshot_download
                    new_path = _snapshot_download(
                        repo_id="deepseek-ai/DeepSeek-R1-0528",
                        local_files_only=False,
                        resume_download=True,
                        allow_patterns=["*.json", "*.py", "*.safetensors", "*.bin", "*.model", "tokenizer*", "*.txt"],
                        local_dir_use_symlinks=False,
                    )
                    print(f"âœ… å¿«ç…§è¡¥å…¨å®Œæˆ: {new_path}")
                    model_path = new_path
        except Exception as _e:
            print(f"âš ï¸ å¿«ç…§é¢„æ£€/è¡¥å…¨å¤±è´¥ï¼ˆå¿½ç•¥ç»§ç»­ï¼‰: {_e}")
        
        # ä½¿ç”¨4bité‡åŒ–ï¼ˆQLoRAæ ‡å‡†ï¼‰ï¼Œæ˜¾è‘—é™ä½æ˜¾å­˜å ç”¨
        bnb_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_use_double_quant=True,
            bnb_4bit_compute_dtype=torch.bfloat16,
        )
        
        # åŠ è½½tokenizer
        print("ğŸ”¹ åŠ è½½tokenizer...")
        tokenizer = AutoTokenizer.from_pretrained(
            model_path, 
            trust_remote_code=True,
            padding_side="right"
        )
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        # ç®€åŒ–ï¼šç›´æ¥åŠ è½½åˆ°GPUï¼Œé¿å…å¤æ‚è®¾å¤‡æ˜ å°„å¯¼è‡´çš„metaè®¾å¤‡é—®é¢˜
        print("â³ ç›´æ¥åŠ è½½æ¨¡å‹åˆ°GPUï¼ˆé¿å…metaè®¾å¤‡ï¼‰...")
        
        # ç«‹å³ä¿®å¤ finegrained_fp8 è®¾å¤‡ä¸Šä¸‹æ–‡é—®é¢˜
        try:
            import transformers.integrations.finegrained_fp8 as _fg
            from contextlib import nullcontext as _nullcontext
            class _DummyAccel:
                @staticmethod
                def device(_dev):
                    return _nullcontext()
            # Override accelerator module to a no-op
            _fg.torch_accelerator_module = _DummyAccel
            print("âœ… å·²é¢„å…ˆç¦ç”¨ finegrained_fp8 CUDA è®¾å¤‡ä¸Šä¸‹æ–‡")
        except Exception as _e:
            print(f"âš ï¸ é¢„å…ˆç¦ç”¨ finegrained_fp8 å¤±è´¥ï¼ˆå¿½ç•¥ï¼‰ï¼š{_e}")

        try:
            import os as _os
            _os.environ.setdefault("PYTORCH_CUDA_ALLOC_CONF", "expandable_segments:True")

            # ç®€åŒ–ï¼šç›´æ¥ä½¿ç”¨ auto è®¾å¤‡æ˜ å°„ï¼Œè®©transformersè‡ªåŠ¨å¤„ç†
            model = AutoModelForCausalLM.from_pretrained(
              model_path,
              quantization_config=bnb_config,
              device_map="auto",
              trust_remote_code=True,
              torch_dtype=torch.bfloat16,
              use_cache=False,
              local_files_only=True,
              # æ·»åŠ ä»¥ä¸‹ä¸¤ä¸ªå…³é”®å‚æ•°
              low_cpu_mem_usage=False    # ç¦ç”¨å†…å­˜ä¼˜åŒ–æ¨¡å¼ï¼ˆé¿å…æ‡’åˆå§‹åŒ–ï¼‰
            )
            
            # ===== æ–°å¢è®¾å¤‡ä¿®å¤ä»£ç å— =====
            print("ğŸ”§ å¼ºåˆ¶è®¾å¤‡ä¸€è‡´æ€§ä¿®å¤...")
            try:
                # ç¡®ä¿æ‰€æœ‰å‚æ•°ä¸åœ¨metaè®¾å¤‡ä¸Š
                meta_names = []
                for name, param in model.named_parameters():
                    if getattr(param, 'is_meta', False):
                        meta_names.append(name)
                        
                if meta_names:
                    print("âŒ æ£€æµ‹åˆ°ä»¥ä¸‹ meta è®¾å¤‡å‚æ•°ï¼ˆå°†ä¸­æ­¢ä»¥é¿å…è®­ç»ƒæœŸå´©æºƒï¼‰ï¼š")
                    for n in meta_names[:50]:
                        print(f"   - {n}")
                    if len(meta_names) > 50:
                        print(f"   ... å…± {len(meta_names)} é¡¹")
                    raise RuntimeError("å­˜åœ¨ meta è®¾å¤‡å‚æ•°ã€‚è¯·æ£€æŸ¥åŠ è½½è·¯å¾„/è®¾å¤‡æ˜ å°„ï¼Œç¡®ä¿ low_cpu_mem_usage=False ä¸”æœªèµ°æ‡’åˆå§‹åŒ–ã€‚")
                print("âœ… æœªå‘ç° meta è®¾å¤‡å‚æ•°")
                
                # ç¡®ä¿æ‰€æœ‰å¯è®­ç»ƒå‚æ•°åœ¨GPUä¸Š
                trainable_on_cpu = 0
                for name, param in model.named_parameters():
                    if param.requires_grad and param.device.type == "cpu":
                        print(f"ğŸ”§ ç§»åŠ¨å¯è®­ç»ƒå‚æ•°åˆ°GPU: {name}")
                        param.data = param.data.to("cuda:0")
                        trainable_on_cpu += 1
                        
                print(f"âœ… ç§»åŠ¨äº† {trainable_on_cpu} ä¸ªCPUä¸Šçš„å¯è®­ç»ƒå‚æ•°åˆ°GPU")
                
                # å¼ºåˆ¶æ¨¡å‹åˆ°GPUï¼ˆç¡®ä¿æ‰€æœ‰ç»„ä»¶éƒ½åœ¨CUDAä¸Šï¼‰
                model = model.to("cuda:0")
                print("âœ… æ¨¡å‹å·²å¼ºåˆ¶ç§»åŠ¨åˆ° cuda:0")
                
            except Exception as e:
                print(f"âš ï¸ è®¾å¤‡ä¿®å¤å¤±è´¥: {e}")
                import traceback
                traceback.print_exc()
            # ===== ç»“æŸæ–°å¢ä»£ç å— =====

            # éªŒè¯æ¨¡å‹è®¾å¤‡
            try:
                first_param_device = next(model.parameters()).device
                print(f"âœ… æ¨¡å‹é¦–ä¸ªå‚æ•°è®¾å¤‡: {first_param_device}")
            except Exception as e:
                print(f"âš ï¸ è®¾å¤‡éªŒè¯å¤±è´¥: {e}")
                
        except Exception as e:
            import traceback as _tb
            print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {repr(e)}")
            _tb.print_exc()
            return None, None, None
        
        print(f"âœ… æ¨¡å‹å·²åŠ è½½ - ä½¿ç”¨ä¿å®ˆæ˜ å°„ç­–ç•¥")

        # è§„èŒƒé‡åŒ–æ ‡è®°ï¼šå½»åº•æ¸…ç† fp8 ç—•è¿¹ï¼Œé¿å… Trainer è¯¯åˆ¤
        try:
            def _sanitize_quantization_metadata(_model):
                # é¡¶å±‚ä¸å­æ¨¡å—å¯èƒ½çš„ fp8 æ ‡å¿—ä½
                for attr in ["is_fp8", "use_fp8", "fp8", "is_quantized_fp8"]:
                    if hasattr(_model, attr):
                        try:
                            setattr(_model, attr, False)
                        except Exception:
                            pass

                # é¡¶å±‚ quantization_config
                qc = getattr(_model, "quantization_config", None)
                try:
                    # ç”¨ 4bit é…ç½®æ˜¾å¼è¦†ç›–ï¼ˆé¿å… None é€ æˆä¸‹æ¸¸ .to_dict è®¿é—®æŠ¥é”™ï¼‰
                    _model.quantization_config = bnb_config
                except Exception:
                    pass

                # config å†…çš„ quantization_config å­—æ®µ
                cfg = getattr(_model, "config", None)
                if cfg is not None:
                    try:
                        # å°† config.quantization_config ä¹ŸæŒ‡å‘ 4bit é…ç½®ï¼Œé¿å… None
                        if hasattr(cfg, "quantization_config"):
                            try:
                                setattr(cfg, "quantization_config", bnb_config)
                            except Exception:
                                pass
                    except Exception:
                        pass

                # å­æ¨¡å—ä¸Šçš„ fp8/æ ‡è®°
                try:
                    for m in _model.modules():
                        for attr in ["is_fp8", "use_fp8", "fp8", "is_quantized_fp8"]:
                            if hasattr(m, attr):
                                try:
                                    setattr(m, attr, False)
                                except Exception:
                                    pass
                except Exception:
                    pass

                # æ ‡æ³¨ä¸ºé‡åŒ–ä½†é fp8
                if hasattr(_model, "is_quantized"):
                    try:
                        _model.is_quantized = True
                    except Exception:
                        pass

            _sanitize_quantization_metadata(model)
            print("âœ… å·²æ¸…ç†é‡åŒ–å…ƒæ•°æ®ï¼Œé¿å…è¢«è¯¯åˆ¤ä¸º fp8")
        except Exception as _e:
            print(f"âš ï¸ é‡åŒ–å…ƒæ•°æ®è§„èŒƒåŒ–å¤±è´¥ï¼ˆå¿½ç•¥ï¼‰ï¼š{_e}")

        # æŒ‰ k-bit è®­ç»ƒæœ€ä½³å®è·µå‡†å¤‡æ¨¡å‹ï¼ˆå¿…é¡»åœ¨æ³¨å…¥ LoRA ä¹‹å‰ï¼‰
        try:
            # ä¸æ¢¯åº¦æ£€æŸ¥ç‚¹é…åˆï¼šç¦ç”¨ç¼“å­˜
            if hasattr(model, "config"):
                model.config.use_cache = False

            # å¼€å¯æ¨¡å‹çº§æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼ˆå†—ä½™è°ƒç”¨ä¹Ÿå®‰å…¨ï¼‰
            if hasattr(model, "gradient_checkpointing_enable"):
                model.gradient_checkpointing_enable()

            # peft å»ºè®®ï¼šä¸º k-bit è®­ç»ƒåšé¢„å¤„ç†ï¼Œå¹¶ç¡®ä¿è¾“å…¥éœ€è¦æ¢¯åº¦
            model = prepare_model_for_kbit_training(
                model, use_gradient_checkpointing=True
            )

            if hasattr(model, "enable_input_require_grads"):
                model.enable_input_require_grads()

            print("âœ… å·²å®Œæˆ k-bit è®­ç»ƒå‡†å¤‡å¹¶å¯ç”¨è¾“å…¥æ¢¯åº¦")
        except Exception as e:
            print(f"âš ï¸ k-bit è®­ç»ƒå‡†å¤‡å¤±è´¥ï¼ˆå°†ç»§ç»­ï¼‰ï¼š{e}")
        
        # ğŸ”§ FP8é‡åŒ–å…¼å®¹æ€§ä¿®å¤ - è‡ªåŠ¨è½¬æ¢ä¸å…¼å®¹çš„æ•°æ®ç±»å‹
        print("ğŸ”§ æ£€æŸ¥å¹¶ä¿®å¤FP8é‡åŒ–å…¼å®¹æ€§é—®é¢˜...")
        fp8_converted = 0
        
        # FP8å‚æ•°è½¬æ¢ + dropoutä¿®å¤
        try:
            import torch
            for name, param in model.named_parameters():
                if hasattr(param, 'dtype') and 'float8' in str(param.dtype).lower():
                    print(f"ğŸ”§ è½¬æ¢FP8å‚æ•°: {name} ä» {param.dtype} åˆ° torch.float16")
                    param.data = param.data.to(torch.float16)
                    fp8_converted += 1
        except Exception as e:
            print(f"âš ï¸ FP8å‚æ•°è½¬æ¢å¤±è´¥ï¼ˆå¿½ç•¥ï¼‰ï¼š{e}")
        
        # ç¦ç”¨æ‰€æœ‰dropoutæ¨¡å—çš„fusedæ¨¡å¼
        try:
            for name, module in model.named_modules():
                if hasattr(module, 'p') and hasattr(module, 'inplace'):  # è¿™æ˜¯Dropoutæ¨¡å—
                    if hasattr(module, 'fused'):
                        module.fused = False
                        print(f"ğŸ”§ ç¦ç”¨fused dropout: {name}")
        except Exception as e:
            print(f"âš ï¸ ç¦ç”¨fused dropoutå¤±è´¥ï¼ˆå¿½ç•¥ï¼‰ï¼š{e}")
        
        if fp8_converted > 0:
            print(f"âœ… å·²è½¬æ¢ {fp8_converted} ä¸ªFP8å‚æ•°ä¸ºè®­ç»ƒå…¼å®¹æ ¼å¼")
        else:
            print("âœ… æœªå‘ç°FP8å‚æ•°ï¼Œæˆ–å·²ç»æ˜¯å…¼å®¹æ ¼å¼")
        
        # ç®€åŒ–ï¼šä¾èµ– apply_moe_training_patch å¤„ç†é—¨æ§/MoE è·¯å¾„ï¼Œé¿å…è‡ªå®šä¹‰è¡¥ä¸å¼•å…¥è®¾å¤‡ä¸ä¸€è‡´
        
        # å…¨å±€åŒ–tokenizerä¾›format_chatä½¿ç”¨
        globals()['tokenizer'] = tokenizer
        
        # æ­£ç¡®çš„LoRAé…ç½® âœ…
        lora_config = LoraConfig(
            r=8,                    # é™ä½rankèŠ‚çœæ˜¾å­˜
            lora_alpha=16,
            target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],  # DeepSeek V3æ¶æ„
            lora_dropout=0.05,
            bias="none",
            task_type="CAUSAL_LM",
        )
        
        # åº”ç”¨LoRA
        model = get_peft_model(model, lora_config)
        model.print_trainable_parameters()

        # å°† LoRA é€‚é…å™¨æƒé‡å¯¹é½åˆ°å…¶ base_layer æ‰€åœ¨è®¾å¤‡ï¼Œé¿å… meta/cuda æ··ç”¨å¯¼è‡´çš„æ¢¯åº¦è®¾å¤‡é”™è¯¯
        try:
            moved = 0
            meta_params_fixed = 0
            
            # å½»åº•æ£€æŸ¥å¹¶ä¿®å¤æ‰€æœ‰metaè®¾å¤‡é—®é¢˜
            for name, param in model.named_parameters():
                # å°†æ‰€æœ‰metaè®¾å¤‡ä¸Šçš„å‚æ•°ç§»åŠ¨åˆ°cuda:0
                if hasattr(param, 'device') and (str(param.device) == 'meta' or param.device.type == 'meta'):
                    print(f"ğŸ”§ ä¿®å¤metaè®¾å¤‡å‚æ•°: {name} -> cuda:0")
                    # åˆ›å»ºå®é™…çš„å‚æ•°ï¼ˆä¸æ˜¯metaï¼‰
                    if param.requires_grad:
                        # å¯è®­ç»ƒå‚æ•°ï¼šéšæœºåˆå§‹åŒ–
                        new_param = torch.randn(param.shape, device='cuda:0', dtype=param.dtype) * 0.01
                    else:
                        # éå¯è®­ç»ƒå‚æ•°ï¼šé›¶åˆå§‹åŒ–
                        new_param = torch.zeros(param.shape, device='cuda:0', dtype=param.dtype)
                    param.data = new_param
                    meta_params_fixed += 1
                elif hasattr(param, 'device') and param.device.type == 'cpu' and param.requires_grad:
                    # å°†å¯è®­ç»ƒå‚æ•°ç§»åˆ°GPUä¸Šï¼Œé¿å…CPU/GPUæ··åˆè®­ç»ƒ
                    print(f"ğŸ”§ ç§»åŠ¨å¯è®­ç»ƒå‚æ•°åˆ°GPU: {name}")
                    param.data = param.data.to('cuda:0')
                    moved += 1
                
            for module in model.modules():
                base = getattr(module, 'base_layer', None)
                if base is None or not hasattr(base, 'weight'):
                    continue
                target_device = base.weight.device
                # ç¡®ä¿target_deviceä¸æ˜¯meta
                if str(target_device) == 'meta':
                    target_device = torch.device('cuda:0')
                    
                for attr in ('lora_A', 'lora_B', 'lora_embedding_A', 'lora_embedding_B'):
                    sub = getattr(module, attr, None)
                    if sub is None:
                        continue
                    # sub å¯èƒ½æ˜¯ ModuleDict/ParameterDict/Module
                    try:
                        for _, submod in getattr(sub, 'items', lambda: [])():
                            try:
                                submod.to(target_device)
                                moved += 1
                            except Exception:
                                pass
                    except TypeError:
                        # é dictï¼Œç›´æ¥ to
                        try:
                            sub.to(target_device)
                            moved += 1
                        except Exception:
                            pass
            if meta_params_fixed > 0:
                print(f"ğŸ”§ ä¿®å¤äº† {meta_params_fixed} ä¸ªmetaè®¾å¤‡å‚æ•°")
            print(f"ğŸ”§ å·²å¯¹é½ LoRA é€‚é…å™¨è®¾å¤‡ï¼Œç§»åŠ¨å­æ¨¡å—: {moved}")
        except Exception as _e:
            print(f"âš ï¸ LoRA è®¾å¤‡å¯¹é½å¤±è´¥ï¼ˆå¿½ç•¥ç»§ç»­ï¼‰ï¼š{_e}")
        
        # ===== æ–°å¢LoRAè®¾å¤‡æ£€æŸ¥ =====
        print("ğŸ” éªŒè¯LoRAè®¾å¤‡ä¸€è‡´æ€§...")
        try:
            lora_devices = set()
            base_devices = set()
            for name, module in model.named_modules():
                if hasattr(module, 'base_layer'):
                    try:
                        base_devices.add(str(module.base_layer.weight.device))
                    except Exception:
                        pass
                    # éå†è¯¥æ¨¡å—çš„å‚æ•°ï¼Œç»Ÿè®¡åŒ…å« lora_ çš„å‚æ•°è®¾å¤‡
                    for p_name, p in getattr(module, 'named_parameters', lambda: [])():
                        if 'lora_' in p_name:
                            try:
                                lora_devices.add(str(p.device))
                            except Exception:
                                pass
            print(f"ğŸ“Š LoRAè®¾å¤‡: {lora_devices}")
            print(f"ğŸ“Š åŸºç¡€å±‚è®¾å¤‡: {base_devices}")
            if len(lora_devices) > 1 or len(base_devices) > 1:
                print("âš ï¸ æ£€æµ‹åˆ°è®¾å¤‡ä¸ä¸€è‡´ï¼å¼ºåˆ¶å¯¹é½...")
                for module in model.modules():
                    if hasattr(module, 'base_layer'):
                        target_device = getattr(module.base_layer.weight, 'device', None)
                        if target_device is not None:
                            try:
                                module.to(target_device)
                            except Exception:
                                pass
            print("âœ… LoRAè®¾å¤‡ä¸€è‡´æ€§éªŒè¯å®Œæˆ")
        except Exception as e:
            print(f"âš ï¸ LoRAè®¾å¤‡æ£€æŸ¥å¤±è´¥: {e}")
        # ===== ç»“æŸæ–°å¢ä»£ç å— =====

        # ğŸ”§ ç¡®ä¿æ‰€æœ‰å¯è®­ç»ƒå‚æ•°çš„æ¢¯åº¦è¿½è¸ªæ­£ç¡®è®¾ç½®
        print("ğŸ”§ éªŒè¯å¹¶ä¿®å¤å‚æ•°æ¢¯åº¦è¿½è¸ª...")
        grad_fixed = 0
        no_grad_params = []
        
        for name, param in model.named_parameters():
            if param.requires_grad:
                # æ£€æŸ¥å‚æ•°æ˜¯å¦çœŸçš„å¯ä»¥è®¡ç®—æ¢¯åº¦
                if not param.requires_grad or param.grad_fn is None and param.is_leaf:
                    print(f"ğŸ”§ ä¿®å¤æ¢¯åº¦è¿½è¸ª: {name}")
                    param.requires_grad_(True)
                    grad_fixed += 1
            else:
                no_grad_params.append(name)
        
        if grad_fixed > 0:
            print(f"âœ… ä¿®å¤äº† {grad_fixed} ä¸ªå‚æ•°çš„æ¢¯åº¦è¿½è¸ª")
        
        print(f"ğŸ“Š æ¢¯åº¦çŠ¶æ€: {sum(p.requires_grad for p in model.parameters())} ä¸ªå¯è®­ç»ƒå‚æ•°")
        
        # ğŸ”§ éªŒè¯é—¨æ§æ¨¡å—è¡¥ä¸æ˜¯å¦ç”Ÿæ•ˆ
        print("ğŸ” éªŒè¯DeepseekExpertsGateè®­ç»ƒæ¨¡å¼è¡¥ä¸...")
        try:
            for name, module in model.named_modules():
                if "DeepseekExpertsGate" in type(module).__name__:
                    print(f"âœ… éªŒè¯é—¨æ§æ¨¡å— {name} æ”¯æŒè®­ç»ƒæ¨¡å¼")
                    module.train()  # åº”è¯¥ä¸ä¼šå¼•å‘é”™è¯¯
                    break
        except Exception as e:
            print(f"âš ï¸ é—¨æ§æ¨¡å—éªŒè¯å¤±è´¥: {e}")
        
        return model, tokenizer, lora_config
        
    except ImportError as e:
        print(f"âŒ ç¼ºå°‘ä¾èµ–: {e}")
        print("ğŸ’¡ è¯·å®‰è£…: pip install transformers peft bitsandbytes trl")
        return None, None, None
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return None, None, None

def format_chat(example):
    """ä½¿ç”¨tokenizerå†…ç½®çš„èŠå¤©æ¨¡æ¿æ ¼å¼åŒ–æ•°æ® âœ…"""
    messages = example["messages"]
    tk = globals().get("tokenizer", None)
    if tk is None:
        raise RuntimeError("tokenizer not initialized in globals()")
    return {"text": tk.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=False,
    )}

# --- Runtime patch: fix MoE training forward path so that `y` is defined in training mode ---
def apply_moe_training_patch(model):
    """Ensure DeepSeek MoE blocks have a valid training forward path.

    Some upstream DeepSeek implementations define `y` only under `if not self.training:`
    and then unconditionally use `y` later, which raises `UnboundLocalError` during training.
    This patch overrides such modules' `forward` to define a differentiable training path
    using `shared_experts(identity)` and keep the original inference path for eval.
    """
    import types

    def module_needs_patch(module):
        return (
            hasattr(module, "gate")
            and hasattr(module, "shared_experts")
            and hasattr(module, "moe_infer")
            and callable(getattr(module, "forward", None))
        )

    num_patched = 0
    for name, submodule in model.named_modules():
        try:
            if module_needs_patch(submodule):
                original_forward = submodule.forward  # noqa: F841 (kept for potential debugging)

                def patched_forward(self, hidden_states):
                    identity = hidden_states
                    orig_shape = getattr(hidden_states, "shape", None)
                    # Gate routing (used for eval path and to keep shapes consistent)
                    topk_idx, topk_weight = self.gate(hidden_states)
                    hidden_states_flat = hidden_states.view(-1, hidden_states.shape[-1])

                    if self.training:
                        # Differentiable training path: rely on shared experts only
                        y = self.shared_experts(identity)
                        return y
                    else:
                        # Preserve original eval behavior
                        y = self.moe_infer(hidden_states_flat, topk_idx, topk_weight)
                        if orig_shape is not None:
                            y = y.view(*orig_shape)
                        if getattr(self.config, "n_shared_experts", None) is not None:
                            y = y + self.shared_experts(identity)
                        return y

                submodule.forward = types.MethodType(patched_forward, submodule)
                num_patched += 1
        except Exception:
            # Best-effort patching; skip modules that fail heuristics
            continue

    print(f"ğŸ§© å·²åº”ç”¨MoEè®­ç»ƒè¡¥ä¸: {num_patched} ä¸ªæ¨¡å—")

# --- Runtime patch: disable finegrained FP8 CUDA device context when running on CPU ---
def disable_finegrained_fp8_device_context_if_cpu(model):
    try:
        first_device = next(model.parameters()).device
    except Exception:
        import torch as _torch
        first_device = _torch.device('cuda' if _torch.cuda.is_available() else 'cpu')
    if getattr(first_device, 'type', 'cpu') != 'cuda':
        try:
            import transformers.integrations.finegrained_fp8 as _fg
            from contextlib import nullcontext as _nullcontext
            class _DummyAccel:
                @staticmethod
                def device(_dev):
                    return _nullcontext()
            # Override accelerator module to a no-op on CPU
            _fg.torch_accelerator_module = _DummyAccel
            print("âœ… å·²ç¦ç”¨ finegrained_fp8 CUDA è®¾å¤‡ä¸Šä¸‹æ–‡ï¼ˆCPUè¿è¡Œç¯å¢ƒï¼‰")
        except Exception as _e:
            print(f"âš ï¸ ç¦ç”¨ finegrained_fp8 å¤±è´¥ï¼ˆå¿½ç•¥ï¼‰ï¼š{_e}")

def train_70b_qlora():
    print("ğŸ¯ å¼€å§‹DeepSeek R1 70B QLoRAå¾®è°ƒ")
    print("="*60)
    
    # 1. ç¯å¢ƒæ£€æµ‹
    if not detect_environment():
        return False
    
    # 2. åŠ è½½æ•°æ®
    training_data = load_training_data()
    if not training_data:
        return False
    
    # 3. è®¾ç½®æ¨¡å‹
    model, tokenizer, lora_config = setup_70b_model()
    if model is None:
        return False
    # 3.1  å°†tokenizeræå‡ä¸ºå…¨å±€ï¼Œä¾› format_chat ä½¿ç”¨
    globals()['tokenizer'] = tokenizer
    
    try:
        from datasets import Dataset
        from transformers import DataCollatorForLanguageModeling, get_linear_schedule_with_warmup
        
        # 4. å‡†å¤‡æ•°æ®é›†
        dataset = Dataset.from_list(training_data)

        # ä½¿ç”¨è‡ªå®šä¹‰è®­ç»ƒå¾ªç¯ï¼Œç›´æ¥å¤„ç†æ–‡æœ¬æ ¼å¼åŒ–

        # 5. è‡ªå®šä¹‰è®­ç»ƒè¶…å‚ï¼ˆé¿å…è§¦å‘HF Trainerçš„fp8æ£€æŸ¥ï¼‰
        num_train_epochs = 3
        per_device_train_batch_size = 2
        gradient_accumulation_steps = 16
        learning_rate = 2e-5
        warmup_ratio = 0.03
        max_grad_norm = 0.3
        output_dir = "./deepseek_r1_70b_tarot_lora"

        # 6. æ–‡æœ¬æ ¼å¼åŒ–ä¸åˆ†è¯
        print("ğŸ§¾ æ­£åœ¨æ ¼å¼åŒ–ä¸åˆ†è¯æ•°æ®...")
        dataset = dataset.map(lambda ex: format_chat(ex))
        def _tokenize_fn(ex):
            enc = tokenizer(
                ex["text"],
                truncation=True,
                max_length=4096,
                padding=False,
                return_tensors=None,
            )
            return enc
        keep_cols = set(["input_ids", "attention_mask"]) | set([c for c in dataset.column_names if c in ("input_ids","attention_mask")])
        dataset = dataset.map(_tokenize_fn, remove_columns=[c for c in dataset.column_names if c not in keep_cols])

        # 7. DataLoader ä¸ collator
        collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)
        import torch.utils.data as tud
        train_loader = tud.DataLoader(dataset, batch_size=per_device_train_batch_size, shuffle=True, collate_fn=collator)
        
        # ===== æ–°å¢å‰å‘ä¼ æ’­åˆå§‹åŒ– =====
        print("ğŸ”§ æ‰§è¡Œå‰å‘ä¼ æ’­ä»¥åˆå§‹åŒ–æ‰€æœ‰å‚æ•°...")
        try:
            # å–ç¬¬ä¸€ä¸ªbatchä½œä¸ºåˆå§‹åŒ–è¾“å…¥
            init_batch = next(iter(train_loader))
            init_batch = {k: v.to(model_device) for k, v in init_batch.items()}
            
            # ä½¿ç”¨no_gradé¿å…è®¡ç®—æ¢¯åº¦
            with torch.no_grad():
                outputs = model(**init_batch)
                print(f"âœ… å‰å‘ä¼ æ’­æˆåŠŸï¼Œloss: {outputs.loss.item():.4f}")
                
            # é‡Šæ”¾å†…å­˜
            del init_batch, outputs
            torch.cuda.empty_cache()
            gc.collect()
            print("âœ… å‚æ•°åˆå§‹åŒ–å®Œæˆï¼Œç¼“å­˜å·²æ¸…é™¤")
        except Exception as e:
            print(f"âš ï¸ å‰å‘ä¼ æ’­åˆå§‹åŒ–å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
        # ===== ç»“æŸæ–°å¢ä»£ç å— =====

        # 8. ä¼˜åŒ–å™¨ä¸è°ƒåº¦å™¨
        import math
        from torch.optim import AdamW
        total_steps = math.ceil(len(train_loader) * num_train_epochs / gradient_accumulation_steps)
        optimizer = AdamW([p for p in model.parameters() if p.requires_grad], lr=learning_rate)
        scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=int(total_steps * warmup_ratio), num_training_steps=total_steps)

        # 9. æ³¨å…¥MoEè®­ç»ƒè¡¥ä¸
        try:
            apply_moe_training_patch(model)
        except Exception as _e:
            print(f"âš ï¸ MoEè®­ç»ƒè¡¥ä¸æ³¨å…¥å¤±è´¥ï¼ˆå¿½ç•¥ç»§ç»­ï¼‰ï¼š{_e}")
        # ä¸ºåŠ é€Ÿè¿›å…¥è®­ç»ƒï¼Œè·³è¿‡å¥è¯Š

        # 10. è‡ªå®šä¹‰è®­ç»ƒå¾ªç¯
        print("ğŸ‹ï¸ ä½¿ç”¨è‡ªå®šä¹‰è®­ç»ƒå¾ªç¯å¼€å§‹è®­ç»ƒ...")
        model.train()
        global_step = 0
        # ä»¥æ¨¡å‹å‚æ•°å®é™…è®¾å¤‡ä¸ºå‡†ï¼Œå†³å®šæ˜¯å¦å¯ç”¨cuda autocast
        try:
            model_device = next(model.parameters()).device
        except Exception:
            model_device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        use_cuda = (getattr(model_device, 'type', 'cpu') == 'cuda')
        print(f"ğŸ–¥ï¸ model device: {model_device}, use_cuda_autocast={use_cuda}")
        for epoch in range(num_train_epochs):
            print(f"ğŸ“† Epoch {epoch+1}/{num_train_epochs}")
            optimizer.zero_grad(set_to_none=True)
            for step, batch in enumerate(train_loader, start=1):
                # å°† batch å¼ é‡è¿ç§»åˆ°ä¸æ¨¡å‹åŒè®¾å¤‡
                # å†æŒ‰éœ€è¦å¼€å¯ autocastï¼ˆä»…åœ¨ cuda ä¸‹ï¼‰
                with torch.autocast(device_type="cuda", dtype=torch.bfloat16, enabled=use_cuda):
                    # DataCollatorForLanguageModeling å·²ç»è®¾ç½®äº† labelsï¼Œä¸éœ€è¦é‡å¤ä¼ é€’
                    try:
                        batch = {k: (v.to(model_device) if hasattr(v, 'to') else v) for k, v in batch.items()}
                    except Exception:
                        pass
                    outputs = model(**batch)  # type: ignore
                    loss = outputs.loss / gradient_accumulation_steps
                loss.backward()

                # ===== æ–°å¢æ¢¯åº¦éªŒè¯ =====
                # æ£€æŸ¥æ¢¯åº¦è®¾å¤‡ä¸€è‡´æ€§
                invalid_grads = []
                valid_grads = 0

                for name, param in model.named_parameters():
                    if param.requires_grad and param.grad is not None:
                        if param.grad.device != param.device:
                            print(f"âš ï¸ æ¢¯åº¦è®¾å¤‡ä¸ä¸€è‡´: {name} "
                                  f"(å‚æ•°è®¾å¤‡: {param.device}, æ¢¯åº¦è®¾å¤‡: {param.grad.device})")
                            invalid_grads.append(name)
                        else:
                            valid_grads += 1

                if invalid_grads:
                    print(f"âŒ å‘ç° {len(invalid_grads)} ä¸ªæ¢¯åº¦è®¾å¤‡ä¸ä¸€è‡´çš„å‚æ•°!")
                    # å°è¯•ä¿®å¤ï¼šå°†æ¢¯åº¦ç§»åŠ¨åˆ°å‚æ•°æ‰€åœ¨è®¾å¤‡
                    for name in invalid_grads:
                        param = dict(model.named_parameters())[name]
                        param.grad = param.grad.to(param.device)
                    print("ğŸ› ï¸ å·²å°è¯•ä¿®å¤æ¢¯åº¦è®¾å¤‡")
                else:
                    print(f"âœ… æ¢¯åº¦è®¾å¤‡ä¸€è‡´: {valid_grads} ä¸ªå‚æ•°")
                # ===== ç»“æŸæ–°å¢ä»£ç å— =====

                if step % gradient_accumulation_steps == 0:
                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)
                    optimizer.step()
                    scheduler.step()
                    optimizer.zero_grad(set_to_none=True)
                    global_step += 1
                    if global_step % 10 == 0:
                        scaled = (loss.detach().float() * gradient_accumulation_steps).item()
                        print(f"step {global_step}/{total_steps}, loss={scaled:.4f}")

        # 11. ä¿å­˜æ¨¡å‹
        print("ğŸ’¾ ä¿å­˜LoRAæƒé‡...")
        os.makedirs(output_dir, exist_ok=True)
        model.save_pretrained(output_dir)
        try:
            tokenizer.save_pretrained(output_dir)
        except Exception:
            pass
        
        # 9. æµ‹è¯•
        print("ğŸ§ª æµ‹è¯•æ¨¡å‹...")
        test_messages = [
            {"role": "user", "content": "è¯·è§£è¯»å¡”ç½—ç‰Œï¼šæ„šäºº(æ­£ä½)"}
        ]
        inputs = tokenizer.apply_chat_template(
            test_messages,
            return_tensors="pt"
        ).to(model.device)
        
        outputs = model.generate(
            inputs,
            max_new_tokens=300,
            temperature=0.7,
            top_p=0.9,
            do_sample=True,
            pad_token_id=tokenizer.eos_token_id
        )
        
        response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        print("ğŸ“ æµ‹è¯•è¾“å‡º:")
        print("-" * 50)
        print(response)
        print("-" * 50)
        
        print("âœ… 70B QLoRAè®­ç»ƒå®Œæˆï¼")
        return True
        
    except Exception as e:
        print(f"âŒ è®­ç»ƒå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    print("ğŸŒŸ DeepSeek R1 70B QLoRA å¡”ç½—ç‰Œå¾®è°ƒç³»ç»Ÿ")
    print("ğŸ”— ä¸“ä¸ºLambda GPUäº‘å¹³å°ä¼˜åŒ–")
    print()
    
    success = train_70b_qlora()
    if success:
        print("ğŸ‰ è®­ç»ƒæˆåŠŸå®Œæˆï¼")
        print("ğŸ“ æ¨¡å‹ä¿å­˜åœ¨: ./deepseek_r1_70b_tarot_lora/")
    else:
        print("âŒ è®­ç»ƒå¤±è´¥ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯")
#!/usr/bin/env python3
"""
DeepSeek R1 70B QLoRA å¾®è°ƒè„šæœ¬ - ä¼˜åŒ–ç‰ˆ
"""
import os
import sys
import torch
import json
from pathlib import Path
import gc
from datetime import datetime

def detect_environment():
    print("ğŸ” æ£€æµ‹ç¯å¢ƒ...")
    if torch.cuda.is_available():
        device_count = torch.cuda.device_count()
        for i in range(device_count):
            gpu_name = torch.cuda.get_device_name(i)
            total_memory = torch.cuda.get_device_properties(i).total_memory / (1024**3)
            print(f"GPU {i}: {gpu_name} | æ˜¾å­˜: {total_memory:.1f}GB")
        return True
    print("âŒ éœ€è¦CUDA GPUç¯å¢ƒ")
    return False

def load_training_data(data_path="training_data.jsonl"):
    print(f"ğŸ“‚ åŠ è½½è®­ç»ƒæ•°æ®: {data_path}")
    if not os.path.exists(data_path):
        print(f"âŒ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_path}")
        return None
    
    try:
        data = []
        with open(data_path, 'r', encoding='utf-8') as f:
            for line in f:
                item = json.loads(line.strip())
                # ä¿ç•™åŸå§‹messagesç»“æ„ âœ…
                if 'messages' in item:
                    data.append(item)
        print(f"âœ… åŠ è½½å®Œæˆ: {len(data)} æ¡è®­ç»ƒæ•°æ®")
        return data
    except Exception as e:
        print(f"âŒ æ•°æ®åŠ è½½å¤±è´¥: {e}")
        return None

def setup_70b_model():
    print("ğŸš€ è®¾ç½®DeepSeek R1 70Bæ¨¡å‹...")
    try:
        from transformers import (
            AutoTokenizer, 
            AutoModelForCausalLM, 
            BitsAndBytesConfig,
            TrainingArguments
        )
        from peft import LoraConfig, get_peft_model
        from trl import SFTTrainer
        from datasets import Dataset
        
        # ä½¿ç”¨å®˜æ–¹æ¨¡å‹åç§° âœ…
        model_name = "deepseek-ai/DeepSeek-R1-0528"
        print(f"âœ… ä½¿ç”¨æ¨¡å‹: {model_name}")
        
        # åŠ è½½tokenizer
        tokenizer = AutoTokenizer.from_pretrained(
            model_name, 
            trust_remote_code=True,
            padding_side="right"
        )
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        # 4bité‡åŒ–é…ç½®
        bnb_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_use_double_quant=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_compute_dtype=torch.bfloat16  # æ›´ç¨³å®š âœ…
        )
        
        # åŠ è½½æ¨¡å‹
        print("ğŸ“¥ åŠ è½½70Bæ¨¡å‹ (4bité‡åŒ–)...")
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            quantization_config=bnb_config,
            device_map="auto",
            trust_remote_code=True,
            torch_dtype=torch.bfloat16,  # æ›´ç¨³å®š âœ…
            attn_implementation="flash_attention_2",  # åŠ é€Ÿ20% âœ…
            use_cache=False
        )
        
        # æ­£ç¡®çš„LoRAé…ç½® âœ…
        lora_config = LoraConfig(
            r=8,                    # é™ä½rankèŠ‚çœæ˜¾å­˜
            lora_alpha=16,
            target_modules=["query_key_value", "dense"],  # GPT-NeoXæ¶æ„
            lora_dropout=0.05,
            bias="none",
            task_type="CAUSAL_LM",
        )
        
        # åº”ç”¨LoRA
        model = get_peft_model(model, lora_config)
        model.print_trainable_parameters()
        return model, tokenizer
        
    except ImportError as e:
        print(f"âŒ ç¼ºå°‘ä¾èµ–: {e}")
        print("ğŸ’¡ è¯·å®‰è£…: pip install transformers peft bitsandbytes trl")
        return None, None
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return None, None

def format_chat(example):
    """ä½¿ç”¨tokenizerå†…ç½®çš„èŠå¤©æ¨¡æ¿æ ¼å¼åŒ–æ•°æ® âœ…"""
    messages = example["messages"]
    return {"text": tokenizer.apply_chat_template(
        messages, 
        tokenize=False, 
        add_generation_prompt=False
    )}

def train_70b_qlora():
    print("ğŸ¯ å¼€å§‹DeepSeek R1 70B QLoRAå¾®è°ƒ")
    print("="*60)
    
    # 1. ç¯å¢ƒæ£€æµ‹
    if not detect_environment():
        return False
    
    # 2. åŠ è½½æ•°æ®
    training_data = load_training_data()
    if not training_data:
        return False
    
    # 3. è®¾ç½®æ¨¡å‹
    model, tokenizer = setup_70b_model()
    if model is None:
        return False
    # 3.1  å°†tokenizeræå‡ä¸ºå…¨å±€ï¼Œä¾› format_chat ä½¿ç”¨
    globals()['tokenizer'] = tokenizer
    
    try:
        from transformers import TrainingArguments
        from trl import SFTTrainer
        from datasets import Dataset
        
        # 4. å‡†å¤‡æ•°æ®é›†
        dataset = Dataset.from_list(training_data)
        dataset = dataset.map(format_chat)  # æ­£ç¡®æ ¼å¼åŒ– âœ…
        print(f"ğŸ“Š æ ¼å¼åŒ–æ•°æ®é›†: {len(dataset)} æ¡æ•°æ®")
        
        # 5. ä¼˜åŒ–çš„è®­ç»ƒå‚æ•° âœ…
        training_args = TrainingArguments(
            output_dir="./deepseek_r1_70b_tarot_lora",
            num_train_epochs=3,
            per_device_train_batch_size=2,  # H100å¯æ‰¿å—
            gradient_accumulation_steps=16,  # åˆç†å€¼
            gradient_checkpointing=True,
            gradient_checkpointing_kwargs={"use_reentrant": False},
            
            # å­¦ä¹ ç‡é…ç½®
            learning_rate=2e-5,  # å°æ•°æ®é›†ç”¨ä½å­¦ä¹ ç‡
            lr_scheduler_type="cosine",
            warmup_ratio=0.03,
            
            # æ—¥å¿—ä¸ä¿å­˜
            logging_steps=10,
            save_strategy="steps",
            save_steps=200,
            save_total_limit=2,
            report_to="tensorboard",
            
            # ä¼˜åŒ–
            optim="paged_adamw_8bit",  # å†…å­˜ä¼˜åŒ–
            max_grad_norm=0.3,
            bf16=True,  # H100æ”¯æŒ
            
            # å…¶ä»–
            dataloader_num_workers=4,
            remove_unused_columns=True,
            push_to_hub=False,
        )
        
        # 6. åˆ›å»ºè®­ç»ƒå™¨
        trainer = SFTTrainer(
            model=model,
            args=training_args,
            train_dataset=dataset,
            tokenizer=tokenizer,
            dataset_text_field="text",
            max_seq_length=2048,  # æ”¯æŒé•¿è§£è¯» âœ…
            packing=True,  # é«˜æ•ˆåˆ©ç”¨ä¸Šä¸‹æ–‡
        )
        
        # 7. å¼€å§‹è®­ç»ƒ
        print("ğŸ‹ï¸ å¼€å§‹70B QLoRAè®­ç»ƒ...")
        print("â±ï¸ é¢„è®¡æ—¶é—´: 5-7å°æ—¶")
        print("ğŸ’° é¢„è®¡æˆæœ¬: $12-18 (Lambda H100 PCIe)")
        
        # è®­ç»ƒ
        trainer.train()
        
        # 8. ä¿å­˜æ¨¡å‹
        print("ğŸ’¾ ä¿å­˜LoRAæƒé‡...")
        trainer.save_model()
        
        # 9. æµ‹è¯•
        print("ğŸ§ª æµ‹è¯•æ¨¡å‹...")
        test_messages = [
            {"role": "user", "content": "è¯·è§£è¯»å¡”ç½—ç‰Œï¼šæ„šäºº(æ­£ä½)"}
        ]
        inputs = tokenizer.apply_chat_template(
            test_messages,
            return_tensors="pt"
        ).to(model.device)
        
        outputs = model.generate(
            inputs,
            max_new_tokens=300,
            temperature=0.7,
            top_p=0.9,
            do_sample=True,
            pad_token_id=tokenizer.eos_token_id
        )
        
        response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        print("ğŸ“ æµ‹è¯•è¾“å‡º:")
        print("-" * 50)
        print(response)
        print("-" * 50)
        
        print("âœ… 70B QLoRAè®­ç»ƒå®Œæˆï¼")
        return True
        
    except Exception as e:
        print(f"âŒ è®­ç»ƒå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    print("ğŸŒŸ DeepSeek R1 70B QLoRA å¡”ç½—ç‰Œå¾®è°ƒç³»ç»Ÿ")
    print("ğŸ”— ä¸“ä¸ºLambda GPUäº‘å¹³å°ä¼˜åŒ–")
    print()
    
    success = train_70b_qlora()
    if success:
        print("ğŸ‰ è®­ç»ƒæˆåŠŸå®Œæˆï¼")
        print("ğŸ“ æ¨¡å‹ä¿å­˜åœ¨: ./deepseek_r1_70b_tarot_lora/")
    else:
        print("âŒ è®­ç»ƒå¤±è´¥ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯")
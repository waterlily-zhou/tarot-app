#!/usr/bin/env python3
"""
DeepSeek R1 70B QLoRAå¾®è°ƒè„šæœ¬ - æ¸…æ´ç‰ˆæœ¬
ä½¿ç”¨DeepSeekå›¢é˜Ÿæä¾›çš„ç»ˆæFP8è§£å†³æ–¹æ¡ˆ
"""

import json
import os
import torch
from transformers import (
    AutoTokenizer, 
    AutoModelForCausalLM, 
    AutoConfig,
    BitsAndBytesConfig,
    DataCollatorForLanguageModeling,
    get_linear_schedule_with_warmup
)
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
from datasets import Dataset
from torch.utils.data import DataLoader
import torch.nn.functional as F
from tqdm import tqdm

def load_training_data(data_path):
    """åŠ è½½è®­ç»ƒæ•°æ®"""
    print(f"ğŸ“‚ åŠ è½½è®­ç»ƒæ•°æ®: {data_path}")
    
    try:
        data = []
        with open(data_path, 'r', encoding='utf-8') as f:
            for line in f:
                item = json.loads(line.strip())
                if 'messages' in item:
                    data.append(item)
        print(f"âœ… åŠ è½½å®Œæˆ: {len(data)} æ¡è®­ç»ƒæ•°æ®")
        return data
    except Exception as e:
        print(f"âŒ æ•°æ®åŠ è½½å¤±è´¥: {e}")
        return None

def setup_70b_model():
    """è®¾ç½®DeepSeek R1 70Bæ¨¡å‹ - DeepSeekç»ˆæè§£å†³æ–¹æ¡ˆ"""
    print("ğŸš€ è®¾ç½®DeepSeek R1 70Bæ¨¡å‹...")
    print("ğŸš€ å®æ–½DeepSeekå›¢é˜Ÿç»ˆæFP8è§£å†³æ–¹æ¡ˆ...")
    
    try:
        # ğŸ”¥ DeepSeekç»ˆæè§£å†³æ–¹æ¡ˆï¼šä½¿ç”¨æœ¬åœ°ç¼“å­˜è·¯å¾„
        model_id = "deepseek-ai/DeepSeek-R1"  # ä½¿ç”¨ä¸»ç‰ˆæœ¬ï¼Œé¿å…FP8é…ç½®
        
        # ä½¿ç”¨æˆ‘ä»¬å‘ç°çš„æ­£ç¡®ç¼“å­˜è·¯å¾„
        cache_path = "/home/ubuntu/.cache/huggingface/models--deepseek-ai--DeepSeek-R1/snapshots/56d4cbbb4d29f4355bab4b9a39ccb717a14ad5ad"
        
        # æ£€æŸ¥ç¼“å­˜æ˜¯å¦å­˜åœ¨
        if os.path.exists(cache_path):
            model_path = cache_path
            print(f"âœ… ä½¿ç”¨æœ¬åœ°ç¼“å­˜: {model_path}")
        else:
            model_path = model_id
            print(f"âš ï¸ æœ¬åœ°ç¼“å­˜ä¸å­˜åœ¨ï¼Œä½¿ç”¨æ¨¡å‹ID: {model_path}")
        
        # ä¸´æ—¶ç¦ç”¨ç¦»çº¿æ¨¡å¼
        offline_vars = {}
        for var in ["HF_HUB_OFFLINE", "TRANSFORMERS_OFFLINE", "HF_DATASETS_OFFLINE"]:
            if var in os.environ:
                offline_vars[var] = os.environ.pop(var)
                print(f"ğŸ”“ ä¸´æ—¶ç¦ç”¨: {var}")
        
        print(f"ğŸš€ ä½¿ç”¨DeepSeek-R1ä¸»ç‰ˆæœ¬: {model_path}")
        
        # DeepSeekç»ˆæé…ç½®æ¸…ç†æ–¹æ¡ˆ
        print("ğŸš€ å®æ–½DeepSeekç»ˆæFP8è§£å†³æ–¹æ¡ˆ...")
        config = AutoConfig.from_pretrained(model_path, local_files_only=True, trust_remote_code=True)
        
        # ğŸ”¥ ç»ˆææ–¹æ¡ˆï¼šç”¨bitsandbytesé…ç½®æ›¿æ¢FP8é…ç½®
        try:
            # æ£€æŸ¥æ˜¯å¦æœ‰FP8é…ç½®éœ€è¦æ›¿æ¢
            has_fp8 = False
            if hasattr(config, "quantization_config") and config.quantization_config is not None:
                qc = config.quantization_config
                if hasattr(qc, "quant_method") and qc.quant_method == "fp8":
                    has_fp8 = True
                elif isinstance(qc, dict) and qc.get("quant_method") == "fp8":
                    has_fp8 = True
            
            if has_fp8:
                print("ğŸ”„ æ£€æµ‹åˆ°FP8é…ç½®ï¼Œæ›¿æ¢ä¸ºbitsandbytesé…ç½®")
                # åˆ›å»ºä¼ªé€ çš„bitsandbytesé…ç½®å­—å…¸
                fake_bnb_config = {
                    "quant_method": "bitsandbytes",
                    "load_in_4bit": True,
                    "bnb_4bit_quant_type": "nf4",
                    "bnb_4bit_use_double_quant": True,
                    "bnb_4bit_compute_dtype": "bfloat16"
                }
                
                # åˆ›å»ºä¸€ä¸ªé…ç½®å¯¹è±¡
                class FakeBnBConfig:
                    def __init__(self):
                        self.quant_method = "bitsandbytes"
                        self.load_in_4bit = True
                        self.bnb_4bit_quant_type = "nf4"
                        self.bnb_4bit_use_double_quant = True
                        self.bnb_4bit_compute_dtype = "bfloat16"
                    
                    def to_dict(self):
                        return fake_bnb_config
                
                config.quantization_config = FakeBnBConfig()
                print("âœ… å·²æ›¿æ¢ä¸ºè™šå‡çš„bitsandbytesé…ç½®")
            else:
                print("âœ… æœªæ£€æµ‹åˆ°FP8é…ç½®ï¼Œä¿æŒåŸé…ç½®")
            
            # æ¸…é™¤å¯èƒ½å­˜åœ¨çš„FP8æ ‡å¿—
            for attr in ["fp8", "use_fp8", "is_fp8"]:
                if hasattr(config, attr):
                    delattr(config, attr)
                    print(f"ğŸ§¹ å·²åˆ é™¤å±æ€§: {attr}")
            
            print("âœ… é…ç½®å¤„ç†å®Œæˆ")
        except Exception as e:
            print(f"âš ï¸ é…ç½®å¤„ç†å¤±è´¥: {e}")
            # å¦‚æœå¤±è´¥ï¼Œè‡³å°‘æ¸…é™¤quantization_config
            if hasattr(config, "quantization_config"):
                config.quantization_config = None

        # åˆ›å»ºæˆ‘ä»¬è‡ªå·±çš„4bité‡åŒ–é…ç½®
        bnb_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_use_double_quant=True,
            bnb_4bit_compute_dtype=torch.bfloat16,
        )
        
        # æš‚æ—¶ç¦ç”¨æ‰€æœ‰é‡åŒ–é…ç½®æ£€æŸ¥ï¼ˆå¢å¼ºç‰ˆï¼‰
        import transformers.quantizers.auto as qa_module
        original_merge = qa_module.AutoHfQuantizer.merge_quantization_configs
        original_supports = qa_module.AutoHfQuantizer.supports_quant_method
        
        def dummy_merge(*args, **kwargs):
            print("ğŸ›‘ è·³è¿‡é‡åŒ–é…ç½®åˆå¹¶")
            return None
            
        def dummy_supports(quantization_config_dict):
            print("ğŸ›‘ è·³è¿‡é‡åŒ–æ–¹æ³•æ”¯æŒæ£€æŸ¥")
            return True
            
        qa_module.AutoHfQuantizer.merge_quantization_configs = staticmethod(dummy_merge)
        qa_module.AutoHfQuantizer.supports_quant_method = staticmethod(dummy_supports)
        
        # é¢å¤–patchï¼šç›´æ¥ä¿®æ”¹modeling_utilsä¸­çš„é‡åŒ–æ£€æŸ¥
        try:
            import transformers.modeling_utils as mu
            original_check = getattr(mu, '_check_and_enable_sdpa', None)
            
            # åˆ›å»ºä¸€ä¸ªå®‰å…¨çš„é…ç½®æ£€æŸ¥å‡½æ•°
            def safe_quantization_check(config):
                if hasattr(config, 'quantization_config') and config.quantization_config is None:
                    return False
                return False
                
            # ä¸´æ—¶æ›¿æ¢é…ç½®æ£€æŸ¥
            if hasattr(mu, 'PreTrainedModel'):
                mu.PreTrainedModel._is_quantized_training_enabled = lambda self: False
            print("ğŸ›‘ å·²ç¦ç”¨é‡åŒ–è®­ç»ƒæ£€æŸ¥")
        except Exception as patch_e:
            print(f"âš ï¸ é¢å¤–è¡¥ä¸å¤±è´¥ï¼ˆå¿½ç•¥ï¼‰: {patch_e}")
        
        try:
            # åŠ è½½æ¨¡å‹æ—¶å¼ºåˆ¶è¦†ç›–æ‰€æœ‰é‡åŒ–è®¾ç½®ï¼ˆDeepSeekå»ºè®®ï¼‰
            print("ğŸ”§ åŠ è½½æ¨¡å‹...")
            model = AutoModelForCausalLM.from_pretrained(
              model_path,
              config=config,  # ä½¿ç”¨æˆ‘ä»¬æ¸…ç†è¿‡çš„é…ç½®
              quantization_config=bnb_config,  # å¼ºåˆ¶ä½¿ç”¨æˆ‘ä»¬çš„4bité…ç½®
              device_map="auto",
              trust_remote_code=True,
              torch_dtype=torch.bfloat16,
              use_cache=False,
              local_files_only=False,  # å…è®¸åœ¨çº¿è®¿é—®è·å–æ­£ç¡®é…ç½®
              ignore_mismatched_sizes=True,  # å¿½ç•¥å¯èƒ½çš„é…ç½®ä¸åŒ¹é…
              low_cpu_mem_usage=True,  # å¯ç”¨å†…å­˜ä¼˜åŒ–
            )
            print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼Œä½¿ç”¨DeepSeekæ–¹æ¡ˆçš„4bit NF4é‡åŒ–")
            
            # DeepSeekå¤‡é€‰æ–¹æ¡ˆï¼šåº”ç”¨é‡åŒ–è‡ªç”±åŒ…è£…å™¨
            print("ğŸ”§ åº”ç”¨DeepSeeké‡åŒ–è‡ªç”±åŒ…è£…å™¨...")
            try:
                from transformers import ModelingMixin
                
                class QuantFreeModel(ModelingMixin):
                    def __init__(self, base_model):
                        super().__init__()
                        self.model = base_model
                        self.config = base_model.config
                        
                        # æ¸…é™¤é‡åŒ–æ ‡å¿—
                        self.is_quantized = False
                        if hasattr(self.config, "quantization_config"):
                            self.config.quantization_config = None
                        
                        # æ¸…é™¤æ‰€æœ‰é‡åŒ–ç›¸å…³å±æ€§
                        for attr in ["fp8", "use_fp8", "is_fp8", "quantization"]:
                            if hasattr(self.config, attr):
                                delattr(self.config, attr)
                    
                    def forward(self, *args, **kwargs):
                        return self.model(*args, **kwargs)
                    
                    def __getattr__(self, name):
                        return getattr(self.model, name)
                
                # åº”ç”¨åŒ…è£…å™¨
                model = QuantFreeModel(model)
                print("âœ… å·²åº”ç”¨é‡åŒ–è‡ªç”±åŒ…è£…å™¨")
            except Exception as wrapper_e:
                print(f"âš ï¸ åŒ…è£…å™¨åº”ç”¨å¤±è´¥ï¼Œç»§ç»­ä½¿ç”¨åŸæ¨¡å‹: {wrapper_e}")
                
        finally:
            # æ¢å¤åŸå§‹å‡½æ•°
            qa_module.AutoHfQuantizer.merge_quantization_configs = original_merge
            qa_module.AutoHfQuantizer.supports_quant_method = original_supports
        
        # åŠ è½½tokenizer
        print("ğŸ”¹ åŠ è½½tokenizer...")
        tokenizer = AutoTokenizer.from_pretrained(
            model_path, 
            trust_remote_code=True,
            padding_side="right"
        )
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        # å‡†å¤‡æ¨¡å‹è¿›è¡ŒQLoRAè®­ç»ƒ
        print("ğŸ”§ å‡†å¤‡æ¨¡å‹è¿›è¡ŒQLoRAè®­ç»ƒ...")
        model = prepare_model_for_kbit_training(model)
        model.config.use_cache = False
        model.gradient_checkpointing_enable()
        
        # é…ç½®LoRA
        lora_config = LoraConfig(
            r=16,
            lora_alpha=32,
            target_modules=["q_proj", "k_proj", "v_proj", "o_proj"],
            lora_dropout=0.1,
            bias="none",
            task_type="CAUSAL_LM"
        )
        
        # åº”ç”¨LoRA
        model = get_peft_model(model, lora_config)
        model.enable_input_require_grads()
        
        print("âœ… æ¨¡å‹è®¾ç½®å®Œæˆï¼")
        return model, tokenizer, lora_config
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹è®¾ç½®å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None, None, None

def preprocess_data(examples, tokenizer):
    """é¢„å¤„ç†è®­ç»ƒæ•°æ®"""
    texts = []
    for item in examples:
        if 'messages' in item:
            # è½¬æ¢ä¸ºæ–‡æœ¬æ ¼å¼
            text = ""
            for msg in item['messages']:
                role = msg.get('role', '')
                content = msg.get('content', '')
                text += f"{role}: {content}\n"
            texts.append(text)
    
    # Tokenize
    tokenized = tokenizer(
        texts,
        truncation=True,
        padding=True,
        max_length=2048,
        return_tensors="pt"
    )
    
    # Labelsä¸input_idsç›¸åŒï¼ˆè¯­è¨€å»ºæ¨¡ä»»åŠ¡ï¼‰
    tokenized["labels"] = tokenized["input_ids"].clone()
    
    return tokenized

def train_70b_qlora():
    """ä¸»è®­ç»ƒå‡½æ•°"""
    print("ğŸ¯ å¼€å§‹DeepSeek R1 70B QLoRAå¾®è°ƒ")
    print("="*60)
    
    # æ£€æŸ¥ç¯å¢ƒ
    if not torch.cuda.is_available():
        print("âŒ éœ€è¦CUDAæ”¯æŒ")
        return False
    
    print("ğŸ” æ£€æµ‹ç¯å¢ƒ...")
    device = torch.cuda.current_device()
    gpu_name = torch.cuda.get_device_name(device)
    gpu_memory = torch.cuda.get_device_properties(device).total_memory / 1024**3
    print(f"GPU {device}: {gpu_name} | æ˜¾å­˜: {gpu_memory:.1f}GB")
    
    # åŠ è½½è®­ç»ƒæ•°æ®
    training_data = load_training_data("training_data.jsonl")
    if not training_data:
        return False
    
    # è®¾ç½®æ¨¡å‹
    model, tokenizer, lora_config = setup_70b_model()
    if model is None:
        return False
    
    # é¢„å¤„ç†æ•°æ®
    print("ğŸ”§ é¢„å¤„ç†è®­ç»ƒæ•°æ®...")
    processed_data = preprocess_data(training_data, tokenizer)
    dataset = Dataset.from_dict(processed_data)
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)
    dataloader = DataLoader(
        dataset,
        batch_size=1,
        shuffle=True,
        collate_fn=data_collator
    )
    
    # è®¾ç½®ä¼˜åŒ–å™¨
    optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)
    
    # è®­ç»ƒå¾ªç¯
    print("ğŸš€ å¼€å§‹è®­ç»ƒ...")
    model.train()
    
    total_steps = len(dataloader)
    print(f"ğŸ“Š æ€»è®­ç»ƒæ­¥æ•°: {total_steps}")
    
    for epoch in range(3):  # 3ä¸ªepoch
        print(f"\nğŸ“ˆ Epoch {epoch + 1}/3")
        epoch_loss = 0
        
        for step, batch in enumerate(tqdm(dataloader, desc=f"Epoch {epoch + 1}")):
            # ç§»åŠ¨æ•°æ®åˆ°GPU
            batch = {k: v.to(model.device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}
            
            # å‰å‘ä¼ æ’­
            outputs = model(**batch)
            loss = outputs.loss
            
            # åå‘ä¼ æ’­
            loss.backward()
            optimizer.step()
            optimizer.zero_grad()
            
            epoch_loss += loss.item()
            
            if step % 10 == 0:
                print(f"Step {step}, Loss: {loss.item():.4f}")
        
        avg_loss = epoch_loss / len(dataloader)
        print(f"âœ… Epoch {epoch + 1} å®Œæˆï¼Œå¹³å‡æŸå¤±: {avg_loss:.4f}")
    
    # ä¿å­˜æ¨¡å‹
    print("ğŸ’¾ ä¿å­˜æ¨¡å‹...")
    model.save_pretrained("./deepseek_r1_tarot_lora")
    tokenizer.save_pretrained("./deepseek_r1_tarot_lora")
    
    print("ğŸ‰ è®­ç»ƒå®Œæˆï¼")
    return True

if __name__ == "__main__":
    success = train_70b_qlora()
    if success:
        print("ğŸ‰ DeepSeek R1 QLoRAå¾®è°ƒæˆåŠŸå®Œæˆï¼")
    else:
        print("âŒ è®­ç»ƒå¤±è´¥")
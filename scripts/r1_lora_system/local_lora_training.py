#!/usr/bin/env python3
"""
æœ¬åœ°R1-Distill LoRAè®­ç»ƒè„šæœ¬ - MacBook Air M4ä¼˜åŒ–ç‰ˆ
ä¿æŠ¤æ•°æ®éšç§ï¼Œæ‰€æœ‰è®­ç»ƒåœ¨æœ¬åœ°å®Œæˆ
"""
import os
import sys
import torch
import json
import sqlite3
from pathlib import Path
import gc
from datetime import datetime

# è®¾ç½®MPSä¼˜åŒ–
os.environ["PYTORCH_ENABLE_MPS_FALLBACK"] = "1"
os.environ["PYTORCH_MPS_HIGH_WATERMARK_RATIO"] = "0.0"

def check_m4_compatibility():
    """æ£€æŸ¥M4ç¡¬ä»¶å…¼å®¹æ€§"""
    print("ğŸ” æ£€æŸ¥MacBook M4ç¡¬ä»¶å…¼å®¹æ€§...")
    
    # æ£€æŸ¥MPSå¯ç”¨æ€§
    if not torch.backends.mps.is_available():
        print("âŒ MPSä¸å¯ç”¨ï¼Œæ— æ³•ä½¿ç”¨M4 GPUåŠ é€Ÿ")
        return False
    
    print("âœ… MPSå¯ç”¨ï¼Œå¯ä»¥ä½¿ç”¨M4 GPUåŠ é€Ÿ")
    
    # æ£€æŸ¥å†…å­˜
    try:
        import psutil
        memory = psutil.virtual_memory()
        total_gb = memory.total / (1024**3)
        available_gb = memory.available / (1024**3)
        
        print(f"ğŸ’¾ æ€»å†…å­˜: {total_gb:.1f}GB")
        print(f"ğŸ’¾ å¯ç”¨å†…å­˜: {available_gb:.1f}GB")
        
        # å†…å­˜å»ºè®®
        if available_gb < 8:
            print("âŒ å¯ç”¨å†…å­˜ä¸è¶³8GBï¼Œæ— æ³•è®­ç»ƒ")
            return False
        elif available_gb < 12:
            print("âš ï¸ å†…å­˜æœ‰é™ï¼Œå»ºè®®ä½¿ç”¨1.5Bæ¨¡å‹ + 4bité‡åŒ–")
            return "small"
        elif available_gb < 16:
            print("âš ï¸ å†…å­˜åˆšå¥½å¤Ÿç”¨ï¼Œå»ºè®®ä½¿ç”¨7Bæ¨¡å‹ + 4bité‡åŒ–")
            return "medium"
        else:
            print("âœ… å†…å­˜å……è¶³ï¼Œå¯ä»¥è®­ç»ƒ7Bæ¨¡å‹")
            return "large"
            
    except ImportError:
        print("ğŸ’¡ å»ºè®®å®‰è£…psutil: pip install psutil")
    
    return True

def choose_model_config(memory_status):
    """æ ¹æ®å†…å­˜æƒ…å†µé€‰æ‹©æ¨¡å‹é…ç½®"""
    if memory_status == "small":
        return {
            "model_name": "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B", 
            "use_4bit": True,
            "batch_size": 1,
            "gradient_accumulation": 8,
            "max_seq_length": 512,
            "gradient_checkpointing": True
        }
    elif memory_status == "medium":
        return {
            "model_name": "deepseek-ai/DeepSeek-R1-Distill-Qwen-7B",
            "use_4bit": True, 
            "batch_size": 1,
            "gradient_accumulation": 4,
            "max_seq_length": 1024,
            "gradient_checkpointing": True
        }
    else:
        return {
            "model_name": "deepseek-ai/DeepSeek-R1-Distill-Qwen-7B",
            "use_4bit": False,
            "batch_size": 2,
            "gradient_accumulation": 2,
            "max_seq_length": 1024,
            "gradient_checkpointing": False
        }

def setup_4bit_config():
    """è®¾ç½®4bité‡åŒ–é…ç½®"""
    try:
        from transformers import BitsAndBytesConfig
        return BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_use_double_quant=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_compute_dtype=torch.float16
        )
    except ImportError:
        print("âŒ éœ€è¦å®‰è£…bitsandbytes: pip install bitsandbytes")
        return None

def install_dependencies():
    """å®‰è£…LoRAè®­ç»ƒä¾èµ–"""
    print("ğŸ“¦ å®‰è£…LoRAè®­ç»ƒä¾èµ–...")
    
    dependencies = [
        "transformers>=4.36.0",
        "peft>=0.7.0", 
        "trl>=0.7.0",
        "datasets>=2.14.0",
        "accelerate>=0.24.0",
        "bitsandbytes>=0.41.0",  # MPSå…¼å®¹ç‰ˆæœ¬
        "torch>=2.1.0",
    ]
    
    import subprocess
    for dep in dependencies:
        try:
            print(f"  å®‰è£… {dep}...")
            subprocess.check_call([
                sys.executable, "-m", "pip", "install", dep, "--quiet"
            ])
        except subprocess.CalledProcessError as e:
            print(f"âŒ å®‰è£…å¤±è´¥ {dep}: {e}")
            return False
    
    print("âœ… ä¾èµ–å®‰è£…å®Œæˆ")
    return True

def prepare_local_dataset():
    """å‡†å¤‡æœ¬åœ°è®­ç»ƒæ•°æ® - ä¸ä¸Šä¼ ä»»ä½•å†…å®¹"""
    print("ğŸ“š å‡†å¤‡æœ¬åœ°è®­ç»ƒæ•°æ®...")
    
    # è¯»å–æœ¬åœ°æ•°æ®åº“
    db_path = "data/deepseek_tarot_knowledge.db"
    if not Path(db_path).exists():
        print(f"âŒ æœ¬åœ°æ•°æ®åº“ä¸å­˜åœ¨: {db_path}")
        return None
    
    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()
    cursor.execute("SELECT person, question, cards, spread, content FROM readings")
    readings = cursor.fetchall()
    conn.close()
    
    if len(readings) < 50:
        print(f"âš ï¸ æ•°æ®é‡è¾ƒå°‘({len(readings)}æ¡)ï¼Œå»ºè®®è‡³å°‘50æ¡ä»¥ä¸Š")
        
    print(f"ğŸ“Š æ‰¾åˆ° {len(readings)} æ¡æœ¬åœ°è§£è¯»æ•°æ®")
    
    # æ„å»ºè®­ç»ƒæ•°æ®
    training_data = []
    for person, question, cards, spread, content in readings:
        # æ ‡å‡†åŒ–è¾“å…¥æ ¼å¼
        instruction = f"""ä½œä¸ºä¸“ä¸šå¡”ç½—å¸ˆï¼Œè¯·ä¸ºä»¥ä¸‹å’¨è¯¢æä¾›æ·±åº¦è§£è¯»ï¼š

å’¨è¯¢è€…ï¼š{person}
é—®é¢˜ï¼š{question}
ç‰Œé˜µï¼š{spread or 'è‡ªç”±ç‰Œé˜µ'}
æŠ½åˆ°çš„ç‰Œï¼š{cards}

è¯·è¿ç”¨ä½ çš„ä¸“ä¸šçŸ¥è¯†å’Œç›´è§‰è¿›è¡Œè§£è¯»ã€‚"""

        training_data.append({
            "instruction": instruction,
            "output": content,
            "metadata": {
                "person": person,
                "cards": cards,
                "length": len(content)
            }
        })
    
    # ä¿å­˜åˆ°æœ¬åœ°
    output_file = "data/local_training_data.json"
    os.makedirs("data", exist_ok=True)
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(training_data, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… è®­ç»ƒæ•°æ®ä¿å­˜åˆ°: {output_file}")
    print(f"ğŸ”’ æ•°æ®å®Œå…¨ä¿å­˜åœ¨æœ¬åœ°ï¼Œä¸ä¼šä¸Šä¼ åˆ°ä»»ä½•æœåŠ¡å™¨")
    
    return training_data

def setup_m4_lora_config():
    """M4ä¼˜åŒ–çš„LoRAé…ç½®"""
    try:
        from peft import LoraConfig, TaskType
    except ImportError:
        print("âŒ è¯·å…ˆå®‰è£…PEFT: pip install peft")
        return None
    
    # é’ˆå¯¹M4çš„ä¼˜åŒ–é…ç½®
    config = LoraConfig(
        task_type=TaskType.CAUSAL_LM,
        inference_mode=False,
        
        # æ ¸å¿ƒå‚æ•° - å¹³è¡¡æ€§èƒ½å’Œè´¨é‡
        r=16,              # é€‚ä¸­çš„rankï¼ŒM4å¯ä»¥å¤„ç†
        lora_alpha=32,     # 2å€çš„alpha
        lora_dropout=0.1,  # é€‚ä¸­çš„dropout
        
        # ç›®æ ‡æ¨¡å— - å…³æ³¨æ ¸å¿ƒæ³¨æ„åŠ›å±‚
        target_modules=[
            "q_proj", "k_proj", "v_proj", "o_proj",  # æ³¨æ„åŠ›å±‚
            "gate_proj", "up_proj", "down_proj"       # FFNå±‚
        ],
        
        # å…¶ä»–é…ç½®
        bias="none",
        modules_to_save=None,  # èŠ‚çœå†…å­˜
    )
    
    print("âš™ï¸ LoRAé…ç½®ä¼˜åŒ–ä¸ºM4ç¡¬ä»¶")
    return config

def train_local_lora():
    """æœ¬åœ°LoRAè®­ç»ƒä¸»å‡½æ•°"""
    print("ğŸš€ å¼€å§‹æœ¬åœ°R1-Distill LoRAè®­ç»ƒ")
    print("="*50)
    
    # 1. ç¡¬ä»¶æ£€æŸ¥
    memory_status = check_m4_compatibility()
    if memory_status == False:
        return False
    
    # 2. é€‰æ‹©æ¨¡å‹é…ç½®
    config = choose_model_config(memory_status)
    print(f"\nâš™ï¸ é€‰æ‹©é…ç½®ï¼š{config['model_name']}")
    print(f"ğŸ’¾ 4bité‡åŒ–ï¼š{'æ˜¯' if config['use_4bit'] else 'å¦'}")
    print(f"ğŸ“¦ æ‰¹å¤„ç†å¤§å°ï¼š{config['batch_size']}")
    
    # 3. å®‰è£…ä¾èµ–
    if not install_dependencies():
        return False
    
    # 4. å‡†å¤‡æ•°æ®
    training_data = prepare_local_dataset()
    if not training_data:
        return False
    
    try:
        from transformers import (
            AutoTokenizer, AutoModelForCausalLM,
            TrainingArguments, Trainer, DataCollatorForLanguageModeling
        )
        from peft import get_peft_model, TaskType
        from datasets import Dataset
        from trl import SFTTrainer
    except ImportError as e:
        print(f"âŒ å¯¼å…¥å¤±è´¥: {e}")
        return False
    
    # 5. åŠ è½½åŸºç¡€æ¨¡å‹
    print(f"ğŸ“¥ åŠ è½½{config['model_name']}...")
    
    try:
        # å…ˆåŠ è½½tokenizer
        tokenizer = AutoTokenizer.from_pretrained(
            config['model_name'], 
            trust_remote_code=True
        )
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        # æ¿€è¿›å†…å­˜ä¼˜åŒ–
        aggressive_memory_optimization()
        
        # è®¾ç½®é‡åŒ–é…ç½®
        quantization_config = None
        if config['use_4bit']:
            print("ğŸ”„ è®¾ç½®4bité‡åŒ–ä»¥èŠ‚çœå†…å­˜...")
            quantization_config = setup_4bit_config()
            if quantization_config is None:
                return False
        
        # ç›‘æ§å†…å­˜
        if not monitor_memory_usage():
            print("ğŸ’¡ è¯·å…³é—­æ›´å¤šåº”ç”¨é‡Šæ”¾å†…å­˜")
            return False
        
        # åŠ è½½æ¨¡å‹ - M4ä¼˜åŒ–
        model = AutoModelForCausalLM.from_pretrained(
            config['model_name'],
            torch_dtype=torch.float16,  # M4æ”¯æŒFP16
            device_map="auto" if config['use_4bit'] else None,
            quantization_config=quantization_config,
            trust_remote_code=True,
            low_cpu_mem_usage=True,
            use_cache=False             # è®­ç»ƒæ—¶å…³é—­cache
        )
        
        # å¯ç”¨gradient checkpointing
        if config['gradient_checkpointing']:
            model.gradient_checkpointing_enable()
            print("âœ… å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ä»¥èŠ‚çœå†…å­˜")
        
        # å¦‚æœæ²¡æœ‰é‡åŒ–ï¼Œæ‰‹åŠ¨ç§»åŠ¨åˆ°MPS
        if not config['use_4bit']:
            print("ğŸ”„ å°†æ¨¡å‹ç§»åŠ¨åˆ°M4 GPU...")
            model = model.to("mps")
        
        print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼å‚æ•°é‡: {model.num_parameters():,}")
        
        # æ¸…ç†å†…å­˜å¹¶ç›‘æ§
        gc.collect()
        if torch.backends.mps.is_available():
            torch.mps.empty_cache()
        
        monitor_memory_usage()
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        print("ğŸ’¡ å°è¯•å…³é—­å…¶ä»–åº”ç”¨é‡Šæ”¾å†…å­˜")
        return False
    
    # 6. åº”ç”¨LoRA
    lora_config = setup_m4_lora_config()
    if not lora_config:
        return False
    
    model = get_peft_model(model, lora_config)
    model.print_trainable_parameters()
    
    # 7. å‡†å¤‡æ•°æ®é›†
    dataset = Dataset.from_list(training_data)
    
    def formatting_func(examples):
        """æ ¼å¼åŒ–è®­ç»ƒæ•°æ®"""
        texts = []
        for instruction, output in zip(examples["instruction"], examples["output"]):
            text = f"{instruction}\n\n{output}"
            texts.append(text)
        return {"text": texts}
    
    dataset = dataset.map(formatting_func, batched=True)
    
    # 8. è®­ç»ƒé…ç½® - å†…å­˜ä¼˜åŒ–
    training_args = TrainingArguments(
        output_dir="./models/local_tarot_lora",
        
        # å­¦ä¹ ç‡
        learning_rate=2e-4,
        lr_scheduler_type="cosine",
        warmup_steps=50,
        
        # æ‰¹æ¬¡é…ç½® - æ ¹æ®å†…å­˜åŠ¨æ€è°ƒæ•´
        per_device_train_batch_size=config['batch_size'],
        gradient_accumulation_steps=config['gradient_accumulation'],
        
        # è®­ç»ƒæ­¥æ•°
        num_train_epochs=3,
        max_steps=300,  # å‡å°‘æ­¥æ•°ä»¥èŠ‚çœæ—¶é—´
        
        # ä¿å­˜ç­–ç•¥
        save_strategy="steps",
        save_steps=100,
        save_total_limit=2,  # åªä¿ç•™2ä¸ªcheckpoint
        
        # ä¼˜åŒ–å™¨
        optim="adamw_torch",
        weight_decay=0.01,
        
        # MPSä¼˜åŒ–
        fp16=True,
        dataloader_num_workers=0,  # MPSä¸æ”¯æŒå¤šè¿›ç¨‹
        dataloader_pin_memory=False,
        
        # ç›‘æ§
        logging_steps=20,
        report_to="none",
        
        # å…¶ä»–
        remove_unused_columns=False,
        push_to_hub=False,  # ä¸ä¸Šä¼ åˆ°äº‘ç«¯
    )
    
    # 9. åˆ›å»ºè®­ç»ƒå™¨
    trainer = SFTTrainer(
        model=model,
        args=training_args,
        train_dataset=dataset,
        tokenizer=tokenizer,
        max_seq_length=config['max_seq_length'],  # æ ¹æ®å†…å­˜è°ƒæ•´
        peft_config=lora_config,
        dataset_text_field="text",
        packing=False,
    )
    
    # 10. å¼€å§‹è®­ç»ƒ
    print("ğŸ‹ï¸ å¼€å§‹æœ¬åœ°è®­ç»ƒ...")
    print(f"ğŸ“Š è®­ç»ƒæ•°æ®: {len(training_data)}æ¡")
    print(f"ğŸ”’ æ‰€æœ‰æ•°æ®ä¿æŒåœ¨æœ¬åœ°ï¼Œç»ä¸ä¸Šä¼ ")
    print("ğŸ’¡ å»ºè®®è®­ç»ƒæœŸé—´å…³é—­æµè§ˆå™¨ç­‰å¤§å†…å­˜åº”ç”¨")
    
    try:
        # æœ€åä¸€æ¬¡å†…å­˜æ¸…ç†
        gc.collect()
        if torch.backends.mps.is_available():
            torch.mps.empty_cache()
        
        # è®­ç»ƒ
        trainer.train()
        
        # ä¿å­˜æ¨¡å‹
        print("ğŸ’¾ ä¿å­˜æœ¬åœ°LoRAæ¨¡å‹...")
        trainer.save_model()
        tokenizer.save_pretrained("./models/local_tarot_lora")
        
        print("ğŸ‰ æœ¬åœ°è®­ç»ƒå®Œæˆï¼")
        print(f"ğŸ“ æ¨¡å‹ä¿å­˜åœ¨: ./models/local_tarot_lora")
        print("ğŸ”’ æ‰€æœ‰æ¨¡å‹æ–‡ä»¶éƒ½åœ¨æœ¬åœ°ï¼Œæœªä¸Šä¼ ä»»ä½•æ•°æ®")
        
        return True
        
    except Exception as e:
        print(f"âŒ è®­ç»ƒå¤±è´¥: {e}")
        print("ğŸ’¡ å°è¯•è§£å†³æ–¹æ¡ˆ:")
        print("  1. å…³é—­æµè§ˆå™¨å’Œå…¶ä»–åº”ç”¨")
        print("  2. é‡å¯Terminalé‡Šæ”¾å†…å­˜")
        print("  3. é€‰æ‹©æ›´å°çš„æ¨¡å‹")
        return False

def test_local_model():
    """æµ‹è¯•æœ¬åœ°è®­ç»ƒçš„æ¨¡å‹"""
    print("ğŸ§ª æµ‹è¯•æœ¬åœ°è®­ç»ƒçš„LoRAæ¨¡å‹...")
    
    model_path = "./models/local_tarot_lora"
    if not Path(model_path).exists():
        print("âŒ æœ¬åœ°æ¨¡å‹ä¸å­˜åœ¨ï¼Œè¯·å…ˆè®­ç»ƒ")
        return
    
    try:
        from transformers import AutoTokenizer, AutoModelForCausalLM
        from peft import PeftModel
        
        # åŠ è½½æ¨¡å‹
        base_model_name = "deepseek-ai/DeepSeek-R1-Distill-Qwen-7B"
        tokenizer = AutoTokenizer.from_pretrained(model_path)
        
        base_model = AutoModelForCausalLM.from_pretrained(
            base_model_name,
            torch_dtype=torch.float16,
            device_map="mps",
            trust_remote_code=True
        )
        
        model = PeftModel.from_pretrained(base_model, model_path)
        
        # æµ‹è¯•è§£è¯»
        test_prompt = """ä½œä¸ºä¸“ä¸šå¡”ç½—å¸ˆï¼Œè¯·ä¸ºä»¥ä¸‹å’¨è¯¢æä¾›æ·±åº¦è§£è¯»ï¼š

å’¨è¯¢è€…ï¼šæµ‹è¯•ç”¨æˆ·
é—®é¢˜ï¼šå½“å‰çš„äººç”Ÿæ–¹å‘
ç‰Œé˜µï¼šä¸‰ç‰ŒæŒ‡å¼•
æŠ½åˆ°çš„ç‰Œï¼šæ„šäºº(æ­£ä½) | åŠ›é‡(æ­£ä½) | æ˜Ÿå¸å(æ­£ä½)

è¯·è¿ç”¨ä½ çš„ä¸“ä¸šçŸ¥è¯†å’Œç›´è§‰è¿›è¡Œè§£è¯»ã€‚"""

        inputs = tokenizer(test_prompt, return_tensors="pt").to("mps")
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=500,
                temperature=0.7,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id
            )
        
        result = tokenizer.decode(outputs[0], skip_special_tokens=True)
        generated_text = result[len(test_prompt):].strip()
        
        print("ğŸ¯ æœ¬åœ°æ¨¡å‹è§£è¯»ç»“æœï¼š")
        print("-" * 50)
        print(generated_text)
        print("-" * 50)
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")

def estimate_training_time():
    """ä¼°ç®—è®­ç»ƒæ—¶é—´"""
    print("â±ï¸ è®­ç»ƒæ—¶é—´ä¼°ç®—ï¼ˆM4 MacBook Airï¼‰ï¼š")
    print("  - æ•°æ®å‡†å¤‡: 10-20åˆ†é’Ÿ")
    print("  - æ¨¡å‹ä¸‹è½½: 15-30åˆ†é’Ÿï¼ˆé¦–æ¬¡ï¼‰")
    print("  - LoRAè®­ç»ƒ: 2-4å°æ—¶")
    print("  - æ€»è€—æ—¶: 3-5å°æ—¶")
    print("  - ç”µè´¹æˆæœ¬: ~$1-2")

def aggressive_memory_optimization():
    """æ¿€è¿›å†…å­˜ä¼˜åŒ–"""
    import os
    # è®¾ç½®æ›´æ¿€è¿›çš„å†…å­˜ç®¡ç†
    os.environ["PYTORCH_MPS_HIGH_WATERMARK_RATIO"] = "0.0"
    os.environ["PYTORCH_MPS_LOW_WATERMARK_RATIO"] = "0.0"
    
    # ç¦ç”¨ä¸€äº›ä¸å¿…è¦çš„åŠŸèƒ½
    os.environ["TOKENIZERS_PARALLELISM"] = "false"
    
    print("ğŸ”§ å¯ç”¨æ¿€è¿›å†…å­˜ä¼˜åŒ–")

def monitor_memory_usage():
    """ç›‘æ§å†…å­˜ä½¿ç”¨"""
    try:
        import psutil
        memory = psutil.virtual_memory()
        available_gb = memory.available / (1024**3)
        used_percent = memory.percent
        
        if available_gb < 2:
            print(f"âš ï¸ å†…å­˜å‘Šæ€¥! å¯ç”¨: {available_gb:.1f}GB")
            return False
        elif available_gb < 4:
            print(f"âš ï¸ å†…å­˜ç´§å¼ : {available_gb:.1f}GB ({used_percent:.1f}%ä½¿ç”¨)")
        else:
            print(f"âœ… å†…å­˜æ­£å¸¸: {available_gb:.1f}GB ({used_percent:.1f}%ä½¿ç”¨)")
        
        return True
    except ImportError:
        return True

if __name__ == "__main__":
    print("ğŸ  æœ¬åœ°R1-Distill LoRAè®­ç»ƒç³»ç»Ÿ")
    print("ğŸ”’ å®Œå…¨ä¿æŠ¤æ•°æ®éšç§ï¼Œä¸ä¸Šä¼ ä»»ä½•å†…å®¹")
    print()
    
    while True:
        print("\né€‰æ‹©æ“ä½œï¼š")
        print("1. æ£€æŸ¥ç¡¬ä»¶å…¼å®¹æ€§")
        print("2. ä¼°ç®—è®­ç»ƒæ—¶é—´")
        print("3. å¼€å§‹æœ¬åœ°è®­ç»ƒ")
        print("4. æµ‹è¯•æœ¬åœ°æ¨¡å‹")
        print("5. é€€å‡º")
        
        choice = input("\nè¯·é€‰æ‹© (1-5): ").strip()
        
        if choice == "1":
            check_m4_compatibility()
        elif choice == "2":
            estimate_training_time()
        elif choice == "3":
            train_local_lora()
        elif choice == "4":
            test_local_model()
        elif choice == "5":
            print("ğŸ‘‹ å†è§ï¼")
            break
        else:
            print("âŒ æ— æ•ˆé€‰æ‹©ï¼Œè¯·é‡è¯•") 
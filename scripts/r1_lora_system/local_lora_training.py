#!/usr/bin/env python3
"""
æœ¬åœ°R1-Distill LoRAè®­ç»ƒè„šæœ¬ - MacBook Air M4ä¼˜åŒ–ç‰ˆ
ä¿æŠ¤æ•°æ®éšç§ï¼Œæ‰€æœ‰è®­ç»ƒåœ¨æœ¬åœ°å®Œæˆ
"""
import os
import sys
import torch
import json
import sqlite3
from pathlib import Path
import gc
from datetime import datetime

# è®¾ç½®MPSä¼˜åŒ–
os.environ["PYTORCH_ENABLE_MPS_FALLBACK"] = "1"
os.environ["PYTORCH_MPS_HIGH_WATERMARK_RATIO"] = "0.0"

def check_m4_compatibility():
    """æ£€æŸ¥M4ç¡¬ä»¶å…¼å®¹æ€§"""
    print("ğŸ” æ£€æŸ¥MacBook M4ç¡¬ä»¶å…¼å®¹æ€§...")
    
    # æ£€æŸ¥MPSå¯ç”¨æ€§
    if not torch.backends.mps.is_available():
        print("âŒ MPSä¸å¯ç”¨ï¼Œæ— æ³•ä½¿ç”¨M4 GPUåŠ é€Ÿ")
        return False
    
    print("âœ… MPSå¯ç”¨ï¼Œå¯ä»¥ä½¿ç”¨M4 GPUåŠ é€Ÿ")
    
    # æ£€æŸ¥å†…å­˜
    try:
        import psutil
        memory = psutil.virtual_memory()
        total_gb = memory.total / (1024**3)
        available_gb = memory.available / (1024**3)
        
        print(f"ğŸ’¾ æ€»å†…å­˜: {total_gb:.1f}GB")
        print(f"ğŸ’¾ å¯ç”¨å†…å­˜: {available_gb:.1f}GB")
        
        # å†…å­˜å»ºè®®
        if available_gb < 7:  # é™ä½é˜ˆå€¼ä»8GBåˆ°7GB
            print("âŒ å¯ç”¨å†…å­˜ä¸è¶³7GBï¼Œæ— æ³•è®­ç»ƒ")
            return False
        elif available_gb < 8:  # æ–°å¢è¶…çº§çœå†…å­˜æ¨¡å¼
            print("âš ï¸ å†…å­˜ç´§å¼ ï¼Œä½¿ç”¨è¶…çº§çœå†…å­˜æ¨¡å¼ï¼š1.5B + 4bit + æ¿€è¿›ä¼˜åŒ–")
            return "tiny"
        elif available_gb < 14:
            print("âš ï¸ å†…å­˜æœ‰é™ï¼Œå»ºè®®ä½¿ç”¨1.5Bæ¨¡å‹ (æ— 4bit)")
            return "small"
        elif available_gb < 20:
            print("âš ï¸ å†…å­˜åˆšå¥½å¤Ÿç”¨ï¼Œå»ºè®®ä½¿ç”¨7Bæ¨¡å‹ (æ— 4bit)")
            return "medium"
        else:
            print("âœ… å†…å­˜å……è¶³ï¼Œå¯ä»¥è®­ç»ƒ7Bæ¨¡å‹")
            return "large"
            
    except ImportError:
        print("ğŸ’¡ å»ºè®®å®‰è£…psutil: pip install psutil")
    
    return True

def choose_model_config(memory_status):
    """æ ¹æ®å†…å­˜æƒ…å†µé€‰æ‹©æ¨¡å‹é…ç½®"""
    if memory_status == "70b":  # æ–°å¢70B QLoRAé…ç½®
        return {
            "model_name": "deepseek-ai/DeepSeek-R1-Distill-Qwen-70B",
            "use_4bit": True,
            "batch_size": 1,
            "gradient_accumulation": 32,  # æ›´å¤§çš„æ¢¯åº¦ç´¯ç§¯æ¨¡æ‹Ÿå¤§æ‰¹æ¬¡
            "max_seq_length": 512,  # æ§åˆ¶åºåˆ—é•¿åº¦
            "gradient_checkpointing": True,
            "lora_r": 16,  # LoRA rank
            "lora_alpha": 32,  # LoRA scaling
            "lora_dropout": 0.1
        }
    else:
        return {
            "model_name": "deepseek-ai/DeepSeek-R1-Distill-Qwen-7B",
            "use_4bit": False,
            "batch_size": 2,
            "gradient_accumulation": 2,
            "max_seq_length": 1024,
            "gradient_checkpointing": False
        }

def setup_4bit_config():
    """è®¾ç½®4bité‡åŒ–é…ç½®"""
    try:
        from transformers import BitsAndBytesConfig
        return BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_use_double_quant=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_compute_dtype=torch.float16
        )
    except ImportError:
        print("âŒ éœ€è¦å®‰è£…bitsandbytes: pip install bitsandbytes")
        return None

def install_dependencies():
    """å®‰è£…LoRAè®­ç»ƒä¾èµ–"""
    print("ğŸ“¦ å®‰è£…LoRAè®­ç»ƒä¾èµ–...")
    
    dependencies = [
        "transformers>=4.36.0",
        "peft>=0.7.0", 
        "trl>=0.7.0",
        "datasets>=2.14.0",
        "accelerate>=0.24.0",
        "bitsandbytes>=0.41.0",  # MPSå…¼å®¹ç‰ˆæœ¬
        "torch>=2.1.0",
    ]
    
    import subprocess
    for dep in dependencies:
        try:
            print(f"  å®‰è£… {dep}...")
            subprocess.check_call([
                sys.executable, "-m", "pip", "install", dep, "--quiet"
            ])
        except subprocess.CalledProcessError as e:
            print(f"âŒ å®‰è£…å¤±è´¥ {dep}: {e}")
            return False
    
    print("âœ… ä¾èµ–å®‰è£…å®Œæˆ")
    return True

def setup_m4_lora_config():
    """M4ä¼˜åŒ–çš„LoRAé…ç½®"""
    try:
        from peft import LoraConfig, TaskType
    except ImportError:
        print("âŒ è¯·å…ˆå®‰è£…PEFT: pip install peft")
        return None
    
    # é’ˆå¯¹M4çš„ä¼˜åŒ–é…ç½®
    config = LoraConfig(
        task_type=TaskType.CAUSAL_LM,
        inference_mode=False,
        
        # æ ¸å¿ƒå‚æ•° - å¹³è¡¡æ€§èƒ½å’Œè´¨é‡
        r=16,              # é€‚ä¸­çš„rankï¼ŒM4å¯ä»¥å¤„ç†
        lora_alpha=32,     # 2å€çš„alpha
        lora_dropout=0.1,  # é€‚ä¸­çš„dropout
        
        # ç›®æ ‡æ¨¡å— - å…³æ³¨æ ¸å¿ƒæ³¨æ„åŠ›å±‚
        target_modules=[
            "q_proj", "k_proj", "v_proj", "o_proj",  # æ³¨æ„åŠ›å±‚
            "gate_proj", "up_proj", "down_proj"       # FFNå±‚
        ],
        
        # å…¶ä»–é…ç½®
        bias="none",
        modules_to_save=None,  # èŠ‚çœå†…å­˜
    )
    
    print("âš™ï¸ LoRAé…ç½®ä¼˜åŒ–ä¸ºM4ç¡¬ä»¶")
    return config

def train_local_lora():
    """æœ¬åœ°LoRAè®­ç»ƒä¸»å‡½æ•°"""
    print("ğŸš€ å¼€å§‹æœ¬åœ°R1-Distill LoRAè®­ç»ƒ")
    print("="*50)
    
    # 1. ç¡¬ä»¶æ£€æŸ¥
    hardware_status = detect_hardware_environment()
    if hardware_status == "cpu":
        print("âŒ ä¸æ”¯æŒCPUè®­ç»ƒï¼Œè¯·ä½¿ç”¨GPUç¯å¢ƒ")
        return False
    
    # 2. é€‰æ‹©æ¨¡å‹é…ç½®
    config = choose_model_config(hardware_status)
    print(f"\nâš™ï¸ é€‰æ‹©é…ç½®ï¼š{config['model_name']}")
    print(f"ğŸ’¾ 4bité‡åŒ–ï¼š{'æ˜¯' if config['use_4bit'] else 'å¦'}")
    print(f"ğŸ“¦ æ‰¹å¤„ç†å¤§å°ï¼š{config['batch_size']}")
    
    # 3. å®‰è£…ä¾èµ–
    if not install_dependencies():
        return False
    
    # 4. å‡†å¤‡æ•°æ®
    training_data = prepare_local_dataset()
    if not training_data:
        return False
    
    try:
        from transformers import (
            AutoTokenizer, AutoModelForCausalLM,
            TrainingArguments, Trainer, DataCollatorForLanguageModeling
        )
        from peft import get_peft_model, TaskType
        from datasets import Dataset
        from trl import SFTTrainer
    except ImportError as e:
        print(f"âŒ å¯¼å…¥å¤±è´¥: {e}")
        return False
    
    # 5. åŠ è½½åŸºç¡€æ¨¡å‹
    print(f"ğŸ“¥ åŠ è½½{config['model_name']}...")
    
    try:
        # å…ˆåŠ è½½tokenizer
        tokenizer = AutoTokenizer.from_pretrained(
            config['model_name'], 
            trust_remote_code=True
        )
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        # æ¿€è¿›å†…å­˜ä¼˜åŒ–
        aggressive_memory_optimization()
        
        # è®¾ç½®é‡åŒ–é…ç½®
        quantization_config = None
        if config['use_4bit']:
            print("ğŸ”„ è®¾ç½®4bité‡åŒ–ä»¥èŠ‚çœå†…å­˜...")
            quantization_config = setup_4bit_config()
            if quantization_config is None:
                return False
        
        # ç›‘æ§å†…å­˜
        if not monitor_memory_usage():
            print("ğŸ’¡ è¯·å…³é—­æ›´å¤šåº”ç”¨é‡Šæ”¾å†…å­˜")
            return False
        
        # åŠ è½½æ¨¡å‹ - M4ä¼˜åŒ–
        model = AutoModelForCausalLM.from_pretrained(
            config['model_name'],
            torch_dtype=torch.float16,  # M4æ”¯æŒFP16
            device_map="auto" if config['use_4bit'] else None,
            quantization_config=quantization_config,
            trust_remote_code=True,
            low_cpu_mem_usage=True,
            use_cache=False             # è®­ç»ƒæ—¶å…³é—­cache
        )
        
        # å¯ç”¨gradient checkpointing
        if config['gradient_checkpointing']:
            model.gradient_checkpointing_enable()
            print("âœ… å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ä»¥èŠ‚çœå†…å­˜")
        
        # æ ¹æ®ç¯å¢ƒé€‰æ‹©è®¾å¤‡
        device = "cuda" if torch.cuda.is_available() else ("mps" if torch.backends.mps.is_available() else "cpu")
        if not config['use_4bit'] and device != "cpu":
            print(f"ğŸ”„ å°†æ¨¡å‹ç§»åŠ¨åˆ° {device.upper()}...")
            model = model.to(device)
        
        print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼å‚æ•°é‡: {model.num_parameters():,}")
        
        # é…ç½®LoRA
        print("ğŸ”§ é…ç½®LoRAé€‚é…å™¨...")
        from peft import LoraConfig, get_peft_model, TaskType
        
        lora_config = LoraConfig(
            r=config.get('lora_r', 16),
            lora_alpha=config.get('lora_alpha', 32),
            target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
            lora_dropout=config.get('lora_dropout', 0.1),
            bias="none",
            task_type=TaskType.CAUSAL_LM,
        )
        
        model = get_peft_model(model, lora_config)
        model.print_trainable_parameters()
        
        # æ¸…ç†å†…å­˜å¹¶ç›‘æ§
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        elif torch.backends.mps.is_available():
            torch.mps.empty_cache()
        
        monitor_memory_usage()
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        print("ğŸ’¡ å°è¯•å…³é—­å…¶ä»–åº”ç”¨é‡Šæ”¾å†…å­˜")
        return False
    
    # 6. åº”ç”¨LoRA
    lora_config = setup_m4_lora_config()
    if not lora_config:
        return False
    
    model = get_peft_model(model, lora_config)
    model.print_trainable_parameters()
    
    # 7. å‡†å¤‡æ•°æ®é›†
    from datasets import Dataset
    dataset = Dataset.from_list(training_data)
    print(f"âœ… æ•°æ®é›†å‡†å¤‡å®Œæˆ: {len(dataset)} æ¡æ•°æ®")
    
    # æ•°æ®å·²ç»æ˜¯æ­£ç¡®çš„æ ¼å¼ï¼ŒåŒ…å«textå­—æ®µï¼Œæ— éœ€é¢å¤–å¤„ç†
    
    # 8. è®­ç»ƒé…ç½® - å†…å­˜ä¼˜åŒ–
    training_args = TrainingArguments(
        output_dir="./models/local_tarot_lora",
        
        # å­¦ä¹ ç‡
        learning_rate=2e-4,
        lr_scheduler_type="cosine",
        warmup_steps=50,
        
        # æ‰¹æ¬¡é…ç½® - æ ¹æ®å†…å­˜åŠ¨æ€è°ƒæ•´
        per_device_train_batch_size=config['batch_size'],
        gradient_accumulation_steps=config['gradient_accumulation'],
        
        # è®­ç»ƒæ­¥æ•°
        num_train_epochs=3,
        max_steps=300,  # å‡å°‘æ­¥æ•°ä»¥èŠ‚çœæ—¶é—´
        
        # ä¿å­˜ç­–ç•¥
        save_strategy="steps",
        save_steps=100,
        save_total_limit=2,  # åªä¿ç•™2ä¸ªcheckpoint
        
        # ä¼˜åŒ–å™¨
        optim="adamw_torch",
        weight_decay=0.01,
        
        # MPSä¼˜åŒ–
        fp16=False,
        dataloader_num_workers=0,  # MPSä¸æ”¯æŒå¤šè¿›ç¨‹
        dataloader_pin_memory=False,
        
        # ç›‘æ§
        logging_steps=20,
        report_to="none",
        
        # å…¶ä»–
        remove_unused_columns=False,
        push_to_hub=False,  # ä¸ä¸Šä¼ åˆ°äº‘ç«¯
    )
    
    # 9. åˆ›å»ºæ•°æ®æ”¶é›†å™¨
    # 9. åˆ›å»ºè®­ç»ƒå™¨ (é€‚é…trl>=0.20.0æ¥å£)
    trainer = SFTTrainer(
        model=model,
        args=training_args,
        train_dataset=dataset,
        processing_class=tokenizer,
        peft_config=lora_config,
    )
    
    # 10. å¼€å§‹è®­ç»ƒ
    print("ğŸ‹ï¸ å¼€å§‹æœ¬åœ°è®­ç»ƒ...")
    print(f"ğŸ“Š è®­ç»ƒæ•°æ®: {len(training_data)}æ¡")
    print(f"ğŸ”’ æ‰€æœ‰æ•°æ®ä¿æŒåœ¨æœ¬åœ°ï¼Œç»ä¸ä¸Šä¼ ")
    print("ğŸ’¡ å»ºè®®è®­ç»ƒæœŸé—´å…³é—­æµè§ˆå™¨ç­‰å¤§å†…å­˜åº”ç”¨")
    
    try:
        # æœ€åä¸€æ¬¡å†…å­˜æ¸…ç†
        gc.collect()
        if torch.backends.mps.is_available():
            torch.mps.empty_cache()
        
        # è®­ç»ƒ
        trainer.train()
        
        # ä¿å­˜æ¨¡å‹
        print("ğŸ’¾ ä¿å­˜æœ¬åœ°LoRAæ¨¡å‹...")
        trainer.save_model()
        tokenizer.save_pretrained("./models/local_tarot_lora")
        
        print("ğŸ‰ æœ¬åœ°è®­ç»ƒå®Œæˆï¼")
        print(f"ğŸ“ æ¨¡å‹ä¿å­˜åœ¨: ./models/local_tarot_lora")
        print("ğŸ”’ æ‰€æœ‰æ¨¡å‹æ–‡ä»¶éƒ½åœ¨æœ¬åœ°ï¼Œæœªä¸Šä¼ ä»»ä½•æ•°æ®")
        
        return True
        
    except Exception as e:
        print(f"âŒ è®­ç»ƒå¤±è´¥: {e}")
        print("ğŸ’¡ å°è¯•è§£å†³æ–¹æ¡ˆ:")
        print("  1. å…³é—­æµè§ˆå™¨å’Œå…¶ä»–åº”ç”¨")
        print("  2. é‡å¯Terminalé‡Šæ”¾å†…å­˜")
        print("  3. é€‰æ‹©æ›´å°çš„æ¨¡å‹")
        return False

def test_local_model():
    """æµ‹è¯•æœ¬åœ°è®­ç»ƒçš„æ¨¡å‹"""
    print("ğŸ§ª æµ‹è¯•æœ¬åœ°è®­ç»ƒçš„LoRAæ¨¡å‹...")
    
    model_path = "./models/local_tarot_lora"
    if not Path(model_path).exists():
        print("âŒ æœ¬åœ°æ¨¡å‹ä¸å­˜åœ¨ï¼Œè¯·å…ˆè®­ç»ƒ")
        return
    
    try:
        from transformers import AutoTokenizer, AutoModelForCausalLM
        from peft import PeftModel
        
        # åŠ è½½æ¨¡å‹ - ä½¿ç”¨ä¸è®­ç»ƒæ—¶ç›¸åŒçš„1.5Bæ¨¡å‹
        base_model_name = "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B"
        tokenizer = AutoTokenizer.from_pretrained(model_path)
        
        base_model = AutoModelForCausalLM.from_pretrained(
            base_model_name,
            torch_dtype=torch.float16,
            device_map="auto",
            trust_remote_code=True,
            low_cpu_mem_usage=True
        )
        
        model = PeftModel.from_pretrained(base_model, model_path)
        
        # æµ‹è¯•è§£è¯»
        test_prompt = """ä½œä¸ºä¸“ä¸šå¡”ç½—å¸ˆï¼Œè¯·ä¸ºä»¥ä¸‹å’¨è¯¢æä¾›æ·±åº¦è§£è¯»ï¼š

å’¨è¯¢è€…ï¼šæµ‹è¯•ç”¨æˆ·
é—®é¢˜ï¼šå½“å‰çš„äººç”Ÿæ–¹å‘
ç‰Œé˜µï¼šä¸‰ç‰ŒæŒ‡å¼•
æŠ½åˆ°çš„ç‰Œï¼šæ„šäºº(æ­£ä½) | åŠ›é‡(æ­£ä½) | æ˜Ÿå¸å(æ­£ä½)

è¯·è¿ç”¨ä½ çš„ä¸“ä¸šçŸ¥è¯†å’Œç›´è§‰è¿›è¡Œè§£è¯»ã€‚"""

        inputs = tokenizer(test_prompt, return_tensors="pt").to("mps")
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=500,
                temperature=0.7,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id
            )
        
        result = tokenizer.decode(outputs[0], skip_special_tokens=True)
        generated_text = result[len(test_prompt):].strip()
        
        print("ğŸ¯ æœ¬åœ°æ¨¡å‹è§£è¯»ç»“æœï¼š")
        print("-" * 50)
        print(generated_text)
        print("-" * 50)
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")

def estimate_training_time():
    """ä¼°ç®—è®­ç»ƒæ—¶é—´"""
    hardware_status = detect_hardware_environment()
    
    if hardware_status == "70b":
        print("â±ï¸ è®­ç»ƒæ—¶é—´ä¼°ç®—ï¼ˆH100 GPU - 70Bæ¨¡å‹ï¼‰ï¼š")
        print("  - æ•°æ®å‡†å¤‡: 5-10åˆ†é’Ÿ")
        print("  - æ¨¡å‹ä¸‹è½½: 30-60åˆ†é’Ÿï¼ˆé¦–æ¬¡ï¼‰")
        print("  - QLoRAè®­ç»ƒ: 3-6å°æ—¶")
        print("  - æ€»è€—æ—¶: 4-7å°æ—¶")
        print("  - Lambdaæˆæœ¬: ~$10-18")
    elif hardware_status in ["medium", "large"]:
        print("â±ï¸ è®­ç»ƒæ—¶é—´ä¼°ç®—ï¼ˆA100 GPU - 7Bæ¨¡å‹ï¼‰ï¼š")
        print("  - æ•°æ®å‡†å¤‡: 5-10åˆ†é’Ÿ")
        print("  - æ¨¡å‹ä¸‹è½½: 10-20åˆ†é’Ÿï¼ˆé¦–æ¬¡ï¼‰")
        print("  - LoRAè®­ç»ƒ: 1-3å°æ—¶")
        print("  - æ€»è€—æ—¶: 2-4å°æ—¶")
        print("  - Lambdaæˆæœ¬: ~$3-6")
    else:
        print("â±ï¸ è®­ç»ƒæ—¶é—´ä¼°ç®—ï¼ˆM4 MacBook Airï¼‰ï¼š")
        print("  - æ•°æ®å‡†å¤‡: 10-20åˆ†é’Ÿ")
        print("  - æ¨¡å‹ä¸‹è½½: 15-30åˆ†é’Ÿï¼ˆé¦–æ¬¡ï¼‰")
        print("  - LoRAè®­ç»ƒ: 2-4å°æ—¶")
        print("  - æ€»è€—æ—¶: 3-5å°æ—¶")
        print("  - ç”µè´¹æˆæœ¬: ~$1-2")

def aggressive_memory_optimization():
    """æ¿€è¿›å†…å­˜ä¼˜åŒ–"""
    import os
    # è®¾ç½®æ›´æ¿€è¿›çš„å†…å­˜ç®¡ç†
    os.environ["PYTORCH_MPS_HIGH_WATERMARK_RATIO"] = "0.0"
    os.environ["PYTORCH_MPS_LOW_WATERMARK_RATIO"] = "0.0"
    
    # ç¦ç”¨ä¸€äº›ä¸å¿…è¦çš„åŠŸèƒ½
    os.environ["TOKENIZERS_PARALLELISM"] = "false"
    
    print("ğŸ”§ å¯ç”¨æ¿€è¿›å†…å­˜ä¼˜åŒ–")

def monitor_memory_usage():
    """ç›‘æ§å†…å­˜ä½¿ç”¨"""
    try:
        import psutil
        memory = psutil.virtual_memory()
        available_gb = memory.available / (1024**3)
        used_percent = memory.percent
        
        if available_gb < 2:
            print(f"âš ï¸ å†…å­˜å‘Šæ€¥! å¯ç”¨: {available_gb:.1f}GB")
            return False
        elif available_gb < 4:
            print(f"âš ï¸ å†…å­˜ç´§å¼ : {available_gb:.1f}GB ({used_percent:.1f}%ä½¿ç”¨)")
        else:
            print(f"âœ… å†…å­˜æ­£å¸¸: {available_gb:.1f}GB ({used_percent:.1f}%ä½¿ç”¨)")
        
        return True
    except ImportError:
        return True

def detect_hardware_environment():
    """æ£€æµ‹ç¡¬ä»¶ç¯å¢ƒ"""
    print("ğŸ” æ£€æµ‹ç¡¬ä»¶ç¯å¢ƒ...")
    
    if torch.cuda.is_available():
        gpu_name = torch.cuda.get_device_name(0)
        total_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)
        print(f"âœ… CUDA GPU: {gpu_name}")
        print(f"ğŸ’¾ GPUå†…å­˜: {total_memory:.1f}GB")
        
        # æ ¹æ®æ˜¾å­˜æ¨èé…ç½®
        if total_memory >= 75:  # H100ç­‰å¤§æ˜¾å­˜å¡
            return "70b"
        elif total_memory >= 35:  # A100ç­‰ä¸­ç­‰æ˜¾å­˜å¡  
            return "medium"
        else:
            return "small"
            
    elif torch.backends.mps.is_available():
        print("âœ… Apple Silicon MPS")
        return check_m4_compatibility()
    else:
        print("âŒ ä»…æ”¯æŒCPUï¼Œä¸å»ºè®®è®­ç»ƒ")
        return "cpu"

if __name__ == "__main__":
    print("ğŸ  R1-Distill LoRAè®­ç»ƒç³»ç»Ÿ")
    print("ğŸ”’ å®Œå…¨ä¿æŠ¤æ•°æ®éšç§ï¼Œä¸ä¸Šä¼ ä»»ä½•å†…å®¹")
    print("ğŸ¯ æ”¯æŒ70B QLoRAé«˜æ•ˆå¾®è°ƒ")
    print()
    
    while True:
        print("\né€‰æ‹©æ“ä½œï¼š")
        print("1. æ£€æµ‹ç¡¬ä»¶ç¯å¢ƒ")
        print("2. ä¼°ç®—è®­ç»ƒæ—¶é—´") 
        print("3. å¼€å§‹LoRAè®­ç»ƒ")
        print("4. æµ‹è¯•è®­ç»ƒæ¨¡å‹")
        print("5. é€€å‡º")
        
        choice = input("\nè¯·é€‰æ‹© (1-5): ").strip()
        
        if choice == "1":
            detect_hardware_environment()
        elif choice == "2":
            estimate_training_time()
        elif choice == "3":
            train_local_lora()
        elif choice == "4":
            test_local_model()
        elif choice == "5":
            print("ğŸ‘‹ å†è§ï¼")
            break
        else:
            print("âŒ æ— æ•ˆé€‰æ‹©ï¼Œè¯·é‡è¯•") 
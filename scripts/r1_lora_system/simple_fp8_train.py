#!/usr/bin/env python3
"""
ç®€åŒ–çš„FP8è½¬æ¢è®­ç»ƒè„šæœ¬
ä¸“æ³¨äºFP8å‚æ•°è½¬æ¢ï¼Œç»•è¿‡å¤æ‚çš„é‡åŒ–é…ç½®æ£€æŸ¥
"""
import os
import sys
import torch
import gc
from transformers import (
    AutoTokenizer, AutoModelForCausalLM, AutoConfig,
    BitsAndBytesConfig, DataCollatorForLanguageModeling
)
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
from datasets import Dataset
import json

# å¼ºåˆ¶å®æ—¶è¾“å‡º
sys.stdout.reconfigure(line_buffering=True)
sys.stderr.reconfigure(line_buffering=True)
os.environ['PYTHONUNBUFFERED'] = '1'

def load_training_data():
    """åŠ è½½è®­ç»ƒæ•°æ®"""
    print("ğŸ“‚ åŠ è½½è®­ç»ƒæ•°æ®: training_data.jsonl")
    with open("training_data.jsonl", "r", encoding="utf-8") as f:
        data = [json.loads(line.strip()) for line in f if line.strip()]
    print(f"âœ… åŠ è½½å®Œæˆ: {len(data)} æ¡è®­ç»ƒæ•°æ®")
    return data

def apply_fp8_conversion(model):
    """åº”ç”¨FP8å‚æ•°è½¬æ¢ - è¿™æ˜¯æ ¸å¿ƒä¿®å¤é€»è¾‘"""
    print("ğŸ”§ æ£€æŸ¥å¹¶ä¿®å¤FP8é‡åŒ–å…¼å®¹æ€§é—®é¢˜...")
    fp8_converted = 0
    
    try:
        for name, param in model.named_parameters():
            if hasattr(param, 'dtype') and 'float8' in str(param.dtype).lower():
                print(f"ğŸ”§ è½¬æ¢FP8å‚æ•°: {name} ä» {param.dtype} åˆ° torch.float16")
                param.data = param.data.to(torch.float16)
                fp8_converted += 1
    except Exception as e:
        print(f"âš ï¸ FP8å‚æ•°è½¬æ¢å¤±è´¥ï¼ˆå¿½ç•¥ï¼‰ï¼š{e}")
    
    print(f"âœ… FP8è½¬æ¢å®Œæˆï¼Œè½¬æ¢äº† {fp8_converted} ä¸ªå‚æ•°")
    return model

def simple_model_loading():
    """ç®€åŒ–çš„æ¨¡å‹åŠ è½½"""
    print("ğŸš€ å¼€å§‹ç®€åŒ–æ¨¡å‹åŠ è½½æµç¨‹...")
    
    # åŸºç¡€é…ç½®
    model_name = "deepseek-ai/DeepSeek-R1"
    
    # 1. åŠ è½½tokenizer
    print("ğŸ”¹ åŠ è½½tokenizer...")
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    # 2. è®¾ç½®QLoRAé…ç½®
    print("âš™ï¸ è®¾ç½®QLoRAé…ç½®...")
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_use_double_quant=True,
        bnb_4bit_compute_dtype=torch.bfloat16,
    )
    
    # 3. åŠ è½½æ¨¡å‹ - ä½¿ç”¨æœ€ç®€å•çš„æ–¹å¼
    print("ğŸ”§ åŠ è½½æ¨¡å‹...")
    try:
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            quantization_config=bnb_config,
            device_map="auto",
            trust_remote_code=True,
            torch_dtype=torch.bfloat16,
            low_cpu_mem_usage=True,
        )
        print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
        
        # 4. åº”ç”¨FP8è½¬æ¢ - è¿™æ˜¯å…³é”®æ­¥éª¤ï¼
        model = apply_fp8_conversion(model)
        
        # 5. å‡†å¤‡PEFT
        print("ğŸ”§ å‡†å¤‡PEFTè®­ç»ƒ...")
        model = prepare_model_for_kbit_training(model)
        
        lora_config = LoraConfig(
            r=32,
            lora_alpha=64,
            target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
            lora_dropout=0.1,
            bias="none",
            task_type="CAUSAL_LM",
        )
        
        model = get_peft_model(model, lora_config)
        print("âœ… PEFTè®¾ç½®å®Œæˆ")
        
        return model, tokenizer
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None, None

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¯ å¼€å§‹ç®€åŒ–FP8è½¬æ¢è®­ç»ƒ")
    print("=" * 60)
    
    # æ£€æŸ¥GPU
    if torch.cuda.is_available():
        gpu_info = torch.cuda.get_device_properties(0)
        print(f"GPU 0: {gpu_info.name} | æ˜¾å­˜: {gpu_info.total_memory/1024**3:.1f}GB")
    
    # æ¸…ç†GPUç¼“å­˜
    torch.cuda.empty_cache()
    gc.collect()
    
    # åŠ è½½æ•°æ®
    training_data = load_training_data()
    
    # åŠ è½½æ¨¡å‹
    model, tokenizer = simple_model_loading()
    
    if model is None:
        print("âŒ æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œè®­ç»ƒç»ˆæ­¢")
        return
    
    print("âœ… ç®€åŒ–FP8è½¬æ¢è®­ç»ƒæµç¨‹å®Œæˆï¼")
    print("ğŸ” å¯ä»¥ç»§ç»­æ·»åŠ è®­ç»ƒå¾ªç¯...")

if __name__ == "__main__":
    main()
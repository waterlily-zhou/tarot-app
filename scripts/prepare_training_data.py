#!/usr/bin/env python3
"""
æœ€ç»ˆä¿®å¤ç‰ˆæ•°æ®å¤„ç†è„šæœ¬ - ç¡®ä¿tokené•¿åº¦ç¬¦åˆè¦æ±‚
"""

import json
import re
from pathlib import Path
from typing import Dict, List, Optional

class TarotDataProcessorFinal:
    def __init__(self, readings_dir: str, max_chars: int = 6000):  # å¤§å¹…é™ä½å­—ç¬¦é™åˆ¶
        self.readings_dir = Path(readings_dir)
        self.output_dir = Path("data/finetune")
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.max_chars = max_chars  # 6000å­—ç¬¦å¤§çº¦å¯¹åº”4000-5000ä¸ªä¸­æ–‡token
        
    def parse_md_file(self, md_path: Path) -> Optional[Dict]:
        """è§£æå•ä¸ªMDæ–‡ä»¶"""
        content = md_path.read_text(encoding='utf-8')
        lines = content.split('\n')
        
        # æå–æ ‡é¢˜ - ç¡®ä¿æ€»æ˜¯æœ‰æ ‡é¢˜
        title = lines[0].replace('# ', '').strip() if lines else md_path.stem
        
        metadata = {}
        content_start = 0
        
        # è§£æå¤šç‰Œé˜µæƒ…å†µ
        cards_data = {}
        spread_data = {}
        
        for i, line in enumerate(lines[1:], 1):
            if line.startswith('Cards'):
                if ':' in line:
                    cards_key, cards_content = line.split(':', 1)
                    cards_key = cards_key.strip()
                    cards_data[cards_key] = self._parse_cards(cards_content.strip())
                    
            elif line.startswith('Card '):
                if ':' in line:
                    cards_key, cards_content = line.split(':', 1)
                    cards_key = cards_key.strip().replace('Card ', 'Cards ')
                    cards_data[cards_key] = self._parse_cards(cards_content.strip())
                    
            elif line.startswith('Spread'):
                if ':' in line:
                    spread_key, spread_content = line.split(':', 1)
                    spread_key = spread_key.strip()
                    spread_data[spread_key] = spread_content.strip()
                    
            elif line.startswith('Person'):
                metadata['person'] = line.split(':', 1)[1].strip()
            elif line.startswith('Date'):
                metadata['date'] = line.split(':', 1)[1].strip()
            elif line.startswith('Reader'):
                metadata['reader'] = line.split(':', 1)[1].strip()
            elif line.strip() == '' and i > 8:
                if not content_start:
                    content_start = i + 1
        
        if not content_start:
            content_start = 10
        
        # æå–æ­£æ–‡
        content_lines = []
        for line in lines[content_start:]:
            if not line.startswith('![') and line.strip():
                content_lines.append(line)
        
        analysis = '\n'.join(content_lines).strip()
        
        if not analysis:
            return None
        
        # åˆå¹¶æ‰€æœ‰ç‰Œé˜µæ•°æ®
        all_cards = []
        all_spreads = []
        
        if cards_data:
            for key in sorted(cards_data.keys()):
                all_cards.extend(cards_data[key])
        
        if spread_data:
            for key in sorted(spread_data.keys()):
                all_spreads.append(spread_data[key])
        
        return {
            'title': title,
            'person': metadata.get('person', ''),
            'cards': all_cards,
            'cards_detail': cards_data,
            'spread': '; '.join(all_spreads) if all_spreads else '',
            'spread_detail': spread_data,
            'date': metadata.get('date', ''),
            'reader': metadata.get('reader', ''),
            'analysis': analysis,
            'has_structured_cards': bool(cards_data),
            'has_structured_spreads': bool(spread_data),
            'source_file': md_path.name
        }
    
    def _parse_cards(self, cards_text: str) -> List[str]:
        """è§£æå¡ç‰Œä¿¡æ¯"""
        if not cards_text:
            return []
            
        cards = re.split(r'[,ï¼Œ;ï¼›ã€]', cards_text)
        
        cleaned_cards = []
        for card in cards:
            card = card.strip()
            if card and not card.startswith('Card') and not card.startswith('Spread'):
                cleaned_cards.append(card)
        
        return cleaned_cards
    
    def split_long_text_aggressive(self, text: str, max_chars: int = 6000) -> List[str]:
        """æ›´æ¿€è¿›çš„æ–‡æœ¬æ‹†åˆ†ç­–ç•¥"""
        if len(text) <= max_chars:
            return [text]
        
        print(f"ğŸ”„ æ‹†åˆ†æ–‡æœ¬ ({len(text)} å­—ç¬¦ â†’ ç›®æ ‡ â‰¤{max_chars})")
        
        # 1. é¦–å…ˆå°è¯•æŒ‰äºŒçº§æ ‡é¢˜æ‹†åˆ†
        sections = re.split(r'\n## ', text)
        if len(sections) > 1:
            parts = []
            current_part = sections[0]
            
            for section in sections[1:]:
                section = '## ' + section
                
                # å¦‚æœå•ä¸ªsectionå°±è¶…è¿‡é™åˆ¶ï¼Œéœ€è¦è¿›ä¸€æ­¥æ‹†åˆ†
                if len(section) > max_chars:
                    # å…ˆä¿å­˜å½“å‰éƒ¨åˆ†
                    if current_part.strip():
                        parts.append(current_part.strip())
                    
                    # æ‹†åˆ†è¿™ä¸ªè¶…é•¿section
                    subsections = self._split_by_paragraphs(section, max_chars)
                    parts.extend(subsections)
                    current_part = ""
                elif len(current_part) + len(section) <= max_chars:
                    current_part += '\n' + section
                else:
                    if current_part.strip():
                        parts.append(current_part.strip())
                    current_part = section
            
            if current_part.strip():
                parts.append(current_part.strip())
            
            # éªŒè¯æ‰€æœ‰éƒ¨åˆ†éƒ½ç¬¦åˆé•¿åº¦è¦æ±‚
            final_parts = []
            for part in parts:
                if len(part) <= max_chars:
                    final_parts.append(part)
                else:
                    # è¿›ä¸€æ­¥æ‹†åˆ†
                    sub_parts = self._split_by_paragraphs(part, max_chars)
                    final_parts.extend(sub_parts)
            
            return final_parts
        
        # 2. å¦‚æœæ²¡æœ‰äºŒçº§æ ‡é¢˜ï¼ŒæŒ‰æ®µè½æ‹†åˆ†
        return self._split_by_paragraphs(text, max_chars)
    
    def _split_by_paragraphs(self, text: str, max_chars: int) -> List[str]:
        """æŒ‰æ®µè½æ‹†åˆ†æ–‡æœ¬"""
        paragraphs = text.split('\n\n')
        parts = []
        current_part = ""
        
        for para in paragraphs:
            # å¦‚æœå•ä¸ªæ®µè½å°±è¶…è¿‡é™åˆ¶ï¼Œå¼ºåˆ¶æŒ‰å¥å­æ‹†åˆ†
            if len(para) > max_chars:
                if current_part.strip():
                    parts.append(current_part.strip())
                    current_part = ""
                
                # æŒ‰å¥å­æ‹†åˆ†
                sentences = re.split(r'[ã€‚ï¼ï¼Ÿ\n]', para)
                temp_part = ""
                for sentence in sentences:
                    if sentence.strip():
                        sentence = sentence.strip() + 'ã€‚'
                        if len(temp_part) + len(sentence) <= max_chars:
                            temp_part += sentence
                        else:
                            if temp_part.strip():
                                parts.append(temp_part.strip())
                            temp_part = sentence
                
                if temp_part.strip():
                    parts.append(temp_part.strip())
                    
            elif len(current_part) + len(para) + 2 <= max_chars:
                current_part += "\n\n" + para if current_part else para
            else:
                if current_part.strip():
                    parts.append(current_part.strip())
                current_part = para
        
        if current_part.strip():
            parts.append(current_part.strip())
        
        return parts if parts else [text[:max_chars]]
    
    def create_training_samples(self, parsed_data: List[Dict]) -> List[Dict]:
        """åˆ›å»ºè®­ç»ƒæ ·æœ¬"""
        samples = []
        
        # æŒ‰personåˆ†ç»„
        person_readings = {}
        for reading in parsed_data:
            person = reading['person']
            if person not in person_readings:
                person_readings[person] = []
            person_readings[person].append(reading)
        
        for reading in parsed_data:
            title = reading.get('title', reading.get('source_file', 'unknown'))
            analysis_length = len(reading['analysis'])
            
            if analysis_length > self.max_chars:
                print(f"ğŸ“„ æ‹†åˆ†é•¿æ–‡æœ¬: {title} ({analysis_length} å­—ç¬¦)")
                
                # ä½¿ç”¨æ›´æ¿€è¿›çš„æ‹†åˆ†ç­–ç•¥
                text_parts = self.split_long_text_aggressive(reading['analysis'], self.max_chars)
                
                for i, part in enumerate(text_parts):
                    part_title = f"{title} (ç¬¬{i+1}éƒ¨åˆ†)"
                    
                    # éªŒè¯æ‹†åˆ†åçš„é•¿åº¦
                    if len(part) > self.max_chars:
                        print(f"âš ï¸ æ‹†åˆ†åä»è¿‡é•¿: {part_title} ({len(part)} å­—ç¬¦)ï¼Œå¼ºåˆ¶æˆªæ–­")
                        part = part[:self.max_chars] + "..."
                    
                    instruction = self._create_instruction(reading, is_part=True, part_num=i+1, total_parts=len(text_parts))
                    
                    sample = {
                        "instruction": instruction,
                        "response": part,
                        "metadata": {
                            "person": reading['person'],
                            "cards": reading['cards'],
                            "spread": reading['spread'],
                            "title": part_title,
                            "original_title": title,
                            "is_split": True,
                            "part_num": i+1,
                            "total_parts": len(text_parts),
                            "source_file": reading['source_file']
                        }
                    }
                    samples.append(sample)
            else:
                # æ­£å¸¸é•¿åº¦çš„æ–‡æœ¬
                instruction = self._create_instruction(reading)
                
                sample = {
                    "instruction": instruction,
                    "response": reading['analysis'],
                    "metadata": {
                        "person": reading['person'],
                        "cards": reading['cards'],
                        "spread": reading['spread'],
                        "title": title,
                        "is_split": False,
                        "source_file": reading['source_file']
                    }
                }
                samples.append(sample)
        
        return samples
    
    def _create_instruction(self, reading: Dict, is_part: bool = False, part_num: int = 1, total_parts: int = 1) -> str:
        """åˆ›å»ºæŒ‡ä»¤ - ç®€åŒ–ç‰ˆæœ¬å‡å°‘tokenæ¶ˆè€—"""
        title = reading.get('title', reading.get('source_file', 'unknown'))
        
        # ç®€åŒ–æŒ‡ä»¤ä»¥å‡å°‘tokenæ¶ˆè€—
        if reading['cards']:
            cards_str = "ï¼›".join(reading['cards'][:5])  # æœ€å¤šæ˜¾ç¤º5å¼ ç‰Œ
            spread_str = reading['spread'] if reading['spread'] else "ç‰Œé˜µæœªæŒ‡å®š"
            
            instruction = f"""å¡”ç½—è§£è¯»ï¼š
å’¨è¯¢è€…ï¼š{reading['person']}
é—®é¢˜ï¼š{title}
ç‰Œé˜µï¼š{spread_str}
ç‰Œï¼š{cards_str}"""
        else:
            instruction = f"""å¡”ç½—è§£è¯»ï¼š
å’¨è¯¢è€…ï¼š{reading['person']}
é—®é¢˜ï¼š{title}"""

        if is_part:
            instruction += f"\n(ç¬¬{part_num}/{total_parts}éƒ¨åˆ†)"
        
        instruction += "\n\nè¯·æä¾›ä¸“ä¸šè§£è¯»ï¼š"
        
        return instruction
    
    def process_all_files(self) -> str:
        """å¤„ç†æ‰€æœ‰MDæ–‡ä»¶"""
        parsed_data = []
        
        for md_file in self.readings_dir.glob("*.md"):
            try:
                data = self.parse_md_file(md_file)
                if data:
                    parsed_data.append(data)
                    print(f"âœ… å¤„ç†: {md_file.name} - '{data['title']}' ({len(data['analysis'])} å­—ç¬¦)")
                else:
                    print(f"â­ï¸ è·³è¿‡: {md_file.name} (æ— æœ‰æ•ˆå†…å®¹)")
            except Exception as e:
                print(f"âŒ é”™è¯¯: {md_file.name} - {e}")
        
        # åˆ›å»ºè®­ç»ƒæ ·æœ¬
        samples = self.create_training_samples(parsed_data)
        
        # ä¿å­˜ä¸ºJSONL
        output_file = self.output_dir / "tarot_readings.jsonl"
        with open(output_file, 'w', encoding='utf-8') as f:
            for sample in samples:
                f.write(json.dumps(sample, ensure_ascii=False) + '\n')
        
        print(f"\nğŸ‰ å¤„ç†å®Œæˆï¼")
        print(f"ğŸ“Š æ€»è®¡è§£è¯»: {len(parsed_data)} æ¡")
        print(f"ğŸ“ è®­ç»ƒæ ·æœ¬: {len(samples)} æ¡")
        print(f"ğŸ’¾ è¾“å‡ºæ–‡ä»¶: {output_file}")
        
        return str(output_file)

if __name__ == "__main__":
    processor = TarotDataProcessorFinal("data/Readings 6c05b8c41bcf40f9be4f7dd503141fd2", max_chars=6000)
    processor.process_all_files()
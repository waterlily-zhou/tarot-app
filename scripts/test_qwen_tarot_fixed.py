#!/usr/bin/env python3
"""
ä¿®å¤ç‰ˆå¡”ç½—AIæµ‹è¯•è„šæœ¬ - ç¡®ä¿LoRAæƒé‡è¢«æ­£ç¡®åº”ç”¨
"""

import os
import sys
import torch
import json
from pathlib import Path

# è®¾ç½®ç¯å¢ƒå˜é‡
os.environ["PYTORCH_ENABLE_MPS_FALLBACK"] = "1"
os.environ["TOKENIZERS_PARALLELISM"] = "false"

try:
    from transformers import AutoTokenizer, AutoModelForCausalLM
    from peft import PeftModel, PeftConfig
    print("âœ… ä¾èµ–åº“å¯¼å…¥æˆåŠŸ")
except ImportError as e:
    print(f"âŒ ç¼ºå°‘ä¾èµ–åº“: {e}")
    sys.exit(1)

def load_trained_model():
    """åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹"""
    print("ğŸ¤– åŠ è½½ä½ çš„ä¸“å±å¡”ç½—AIæ¨¡å‹...")
    
    # æ£€æŸ¥è®¾å¤‡
    if torch.backends.mps.is_available():
        device = "mps"
        print("âœ… ä½¿ç”¨ Apple Silicon MPS")
    else:
        device = "cpu"
        print("âš ï¸ ä½¿ç”¨ CPU")
    
    model_path = "./models/qwen-tarot-24gb"
    
    try:
        # 1. é¦–å…ˆä¿®å¤adapteré…ç½®
        print("ğŸ”§ ä¿®å¤LoRAé…ç½®...")
        adapter_config_path = Path(model_path) / "adapter_config.json"
        
        with open(adapter_config_path, 'r') as f:
            config = json.load(f)
        
        # å…³é”®ä¿®å¤ï¼šè®¾ç½®ä¸ºè®­ç»ƒæ¨¡å¼ä»¥æ¿€æ´»LoRA
        config["inference_mode"] = False
        
        with open(adapter_config_path, 'w') as f:
            json.dump(config, f, indent=2)
        
        print("âœ… LoRAé…ç½®å·²ä¿®å¤")
        
        # 2. åŠ è½½åˆ†è¯å™¨
        print("ğŸ“¥ åŠ è½½åˆ†è¯å™¨...")
        tokenizer = AutoTokenizer.from_pretrained(
            model_path, 
            trust_remote_code=True
        )
        
        # 3. åŠ è½½åŸºç¡€æ¨¡å‹
        base_model_name = "Qwen/Qwen1.5-1.8B-Chat"
        print(f"ğŸ“¥ åŠ è½½åŸºç¡€æ¨¡å‹: {base_model_name}")
        
        base_model = AutoModelForCausalLM.from_pretrained(
            base_model_name,
            torch_dtype=torch.float32,
            device_map="cpu",
            trust_remote_code=True,
            low_cpu_mem_usage=True
        )
        
        # 4. åŠ è½½LoRAé€‚é…å™¨ï¼ˆç°åœ¨åº”è¯¥ä¼šæ­£ç¡®æ¿€æ´»ï¼‰
        print("ğŸ“¥ åŠ è½½å¹¶æ¿€æ´»ä½ çš„ä¸ªäººåŒ–é€‚é…å™¨...")
        model = PeftModel.from_pretrained(base_model, model_path)
        
        # ç¡®è®¤LoRAæ˜¯å¦æ¿€æ´»
        print(f"ğŸ” LoRAçŠ¶æ€æ£€æŸ¥:")
        print(f"   - æ˜¯å¦ä¸ºPeftModel: {isinstance(model, PeftModel)}")
        print(f"   - æ˜¯å¦ä¸ºæ¨ç†æ¨¡å¼: {getattr(model.peft_config['default'], 'inference_mode', 'unknown')}")
        
        # 5. ç§»åŠ¨åˆ°è®¾å¤‡
        if device == "mps":
            print("ğŸ”„ ç§»åŠ¨æ¨¡å‹åˆ°MPS...")
            model = model.to("mps")
        
        print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼")
        return model, tokenizer, device
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None, None, None

def test_simple_case(model, tokenizer, device):
    """æµ‹è¯•ä¸€ä¸ªç®€å•æ¡ˆä¾‹"""
    print("\nğŸ§ª å¿«é€Ÿæµ‹è¯•...")
    
    # ä½¿ç”¨è®­ç»ƒæ•°æ®ä¸­çš„æ ¼å¼
    prompt = """å¡”ç½—è§£è¯»ï¼š
å’¨è¯¢è€…ï¼šMel
é—®é¢˜ï¼šå·¥ä½œå‘å±•
ç‰Œé˜µï¼šä¸‰å¼ ç‰Œ
ç‰Œï¼šæ„šäºº(æ­£ä½)ï¼›åŠ›é‡(æ­£ä½)ï¼›æ˜Ÿå¸å(æ­£ä½)

è¯·æä¾›ä¸“ä¸šè§£è¯»ï¼š"""
    
    print("ğŸ¯ è¾“å…¥prompt:")
    print(prompt)
    print("\nğŸ¤– AIè§£è¯»:")
    print("-" * 60)
    
    try:
        # ç”Ÿæˆå›ç­”
        inputs = tokenizer(prompt, return_tensors="pt")
        if device == "mps":
            inputs = {k: v.to("mps") for k, v in inputs.items()}
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=300,
                temperature=0.8,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id,
                repetition_penalty=1.2,
                top_p=0.9
            )
        
        # è§£ç å›ç­”
        full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        ai_response = full_response[len(prompt):].strip()
        
        print(ai_response)
        print("-" * 60)
        
        # ç®€å•è´¨é‡æ£€æŸ¥
        if len(ai_response) < 50:
            print("âš ï¸ ç”Ÿæˆå†…å®¹è¿‡çŸ­")
        elif "æ„šäºº" in ai_response or "åŠ›é‡" in ai_response or "æ˜Ÿå¸" in ai_response:
            print("âœ… ç”Ÿæˆå†…å®¹åŒ…å«ç›¸å…³ç‰Œå")
        else:
            print("âš ï¸ ç”Ÿæˆå†…å®¹å¯èƒ½ä¸å¤Ÿç›¸å…³")
            
    except Exception as e:
        print(f"âŒ ç”Ÿæˆå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

def main():
    print("ğŸ”® ä¿®å¤ç‰ˆå¡”ç½—AIæµ‹è¯•ç¨‹åº")
    print("="*50)
    
    # åŠ è½½æ¨¡å‹
    model, tokenizer, device = load_trained_model()
    if model is None:
        return
    
    # å¿«é€Ÿæµ‹è¯•
    test_simple_case(model, tokenizer, device)
    
    print("\nğŸ‰ æµ‹è¯•å®Œæˆï¼")
    print("ğŸ’¡ å¦‚æœç»“æœä»ç„¶ä¸ç†æƒ³ï¼Œå¯èƒ½éœ€è¦æ£€æŸ¥è®­ç»ƒæ•°æ®æ ¼å¼")

if __name__ == "__main__":
    main() 
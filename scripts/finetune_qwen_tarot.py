#!/usr/bin/env python3
"""
ä¼˜åŒ–ç‰ˆQwenå¾®è°ƒè„šæœ¬ - é’ˆå¯¹24GBå†…å­˜MacBook Air M4
"""

import os
import sys
import torch
import warnings
import json
from pathlib import Path
import gc
warnings.filterwarnings("ignore")

# è®¾ç½®ç¯å¢ƒå˜é‡ä¼˜åŒ–å†…å­˜ä½¿ç”¨
os.environ["PYTORCH_ENABLE_MPS_FALLBACK"] = "1"
os.environ["TOKENIZERS_PARALLELISM"] = "false"
os.environ["PYTORCH_MPS_HIGH_WATERMARK_RATIO"] = "0.0"  # ç¦ç”¨MPSå†…å­˜ä¸Šé™

try:
    from transformers import (
        AutoTokenizer, AutoModelForCausalLM, 
        TrainingArguments, Trainer, DataCollatorForLanguageModeling
    )
    from peft import LoraConfig, get_peft_model, TaskType
    from datasets import Dataset
    print("âœ… æ‰€æœ‰ä¾èµ–åº“å·²æˆåŠŸå¯¼å…¥")
except ImportError as e:
    print(f"âŒ ç¼ºå°‘ä¾èµ–åº“: {e}")
    sys.exit(1)

def check_system_info():
    """æ£€æŸ¥ç³»ç»Ÿä¿¡æ¯"""
    if torch.backends.mps.is_available():
        print("ğŸš€ ä½¿ç”¨ Apple Silicon MPS")
        device = "mps"
    else:
        print("ğŸ’» ä½¿ç”¨ CPU")
        device = "cpu"
    
    # æ£€æŸ¥å¯ç”¨å†…å­˜
    try:
        import psutil
        memory = psutil.virtual_memory()
        print(f"ğŸ’¾ æ€»å†…å­˜: {memory.total / (1024**3):.1f} GB")
        print(f"ğŸ’¾ å¯ç”¨å†…å­˜: {memory.available / (1024**3):.1f} GB")
        print(f"ğŸ’¾ ä½¿ç”¨ç‡: {memory.percent}%")
        
        if memory.available / (1024**3) < 10:
            print("âš ï¸ å¯ç”¨å†…å­˜è¾ƒå°‘ï¼Œå»ºè®®å…³é—­å…¶ä»–åº”ç”¨")
        else:
            print("âœ… å†…å­˜å……è¶³ï¼Œå¯ä»¥è¿è¡Œå¤§æ¨¡å‹")
            
    except ImportError:
        print("ğŸ’¡ å®‰è£… psutil å¯æŸ¥çœ‹è¯¦ç»†å†…å­˜ä¿¡æ¯: pip install psutil")
    
    return device

def clean_memory():
    """æ¸…ç†å†…å­˜"""
    gc.collect()
    if torch.backends.mps.is_available():
        torch.mps.empty_cache()
    print("ğŸ§¹ å†…å­˜æ¸…ç†å®Œæˆ")

def validate_and_clean_data(data_file: str, max_length: int = 4000):
    """éªŒè¯å’Œæ¸…ç†è®­ç»ƒæ•°æ®"""
    print(f"ğŸ” éªŒè¯è®­ç»ƒæ•°æ® (æœ€å¤§é•¿åº¦: {max_length} tokens)...")
    
    model_name = "Qwen/Qwen1.5-7B-Chat"
    print(f"ğŸ“¥ åŠ è½½åˆ†è¯å™¨: {model_name}")
    
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    valid_samples = []
    rejected_samples = []
    
    with open(data_file, 'r', encoding='utf-8') as f:
        for i, line in enumerate(f):
            try:
                sample = json.loads(line)
                
                # æ„å»ºå®Œæ•´æ–‡æœ¬ç”¨äºtokenè®¡ç®—
                full_text = sample['instruction'] + '\n\n' + sample['response']
                
                # è®¡ç®—tokené•¿åº¦
                tokens = tokenizer.encode(full_text, add_special_tokens=True)
                token_length = len(tokens)
                
                if token_length <= max_length:
                    valid_samples.append(sample)
                else:
                    # æ™ºèƒ½æˆªæ–­è€Œä¸æ˜¯æ‹’ç»
                    instruction_tokens = tokenizer.encode(sample['instruction'], add_special_tokens=True)
                    max_response_tokens = max_length - len(instruction_tokens) - 20  # é¢„ç•™ç‰¹æ®Štokenç©ºé—´
                    
                    if max_response_tokens > 200:  # ç¡®ä¿æœ‰è¶³å¤Ÿç©ºé—´ç»™å›ç­”
                        response_tokens = tokenizer.encode(sample['response'], add_special_tokens=False)
                        if len(response_tokens) > max_response_tokens:
                            # æˆªæ–­åˆ°å¥å­è¾¹ç•Œ
                            truncated_tokens = response_tokens[:max_response_tokens]
                            truncated_response = tokenizer.decode(truncated_tokens, skip_special_tokens=True)
                            
                            # æ‰¾åˆ°æœ€åä¸€ä¸ªå¥å·æˆ–è€…æ®µè½è¾¹ç•Œæ¥ä¼˜é›…æˆªæ–­
                            last_period = max(
                                truncated_response.rfind('ã€‚'),
                                truncated_response.rfind('\n\n'),
                                truncated_response.rfind('ï¼'),
                                truncated_response.rfind('ï¼Ÿ')
                            )
                            
                            if last_period > len(truncated_response) * 0.7:  # å¦‚æœå¥å·ä½ç½®åˆç†
                                truncated_response = truncated_response[:last_period + 1]
                            
                            sample['response'] = truncated_response + "\n\n(è§£è¯»å†…å®¹å·²æˆªæ–­)"
                            valid_samples.append(sample)
                            print(f"âœ‚ï¸ ä¼˜é›…æˆªæ–­æ ·æœ¬ {i+1}: {sample.get('metadata', {}).get('title', 'unknown')}")
                        else:
                            valid_samples.append(sample)
                    else:
                        print(f"âŒ æ ·æœ¬ {i+1} æŒ‡ä»¤è¿‡é•¿ï¼Œæ— æ³•å®¹çº³: {sample.get('metadata', {}).get('title', 'unknown')}")
                        rejected_samples.append({
                            'line': i+1,
                            'token_length': token_length,
                            'title': sample.get('metadata', {}).get('title', 'unknown'),
                            'reason': 'instruction_too_long'
                        })
                    
            except Exception as e:
                print(f"âŒ æ ·æœ¬ {i+1} è§£æé”™è¯¯: {e}")
                rejected_samples.append({'line': i+1, 'error': str(e)})
    
    print(f"âœ… æœ‰æ•ˆæ ·æœ¬: {len(valid_samples)}")
    print(f"âŒ æ‹’ç»æ ·æœ¬: {len(rejected_samples)}")
    
    if rejected_samples and len(rejected_samples) <= 5:
        print("\nè¢«æ‹’ç»çš„æ ·æœ¬è¯¦æƒ…:")
        for r in rejected_samples:
            if 'token_length' in r:
                print(f"  ç¬¬{r['line']}è¡Œ: {r['title']} ({r['token_length']} tokens)")
            else:
                print(f"  ç¬¬{r['line']}è¡Œ: {r.get('error', 'unknown error')}")
    
    return valid_samples, tokenizer

def preprocess_function(examples, tokenizer, max_length=4000):
    """é¢„å¤„ç†å‡½æ•°"""
    inputs = []
    labels = []
    
    for instruction, response in zip(examples['instruction'], examples['response']):
        # æ„å»ºå¯¹è¯æ ¼å¼
        text = f"{instruction}\n\n{response}"
        
        # åˆ†è¯ï¼Œä½¿ç”¨ä¸¥æ ¼çš„æˆªæ–­
        tokenized = tokenizer(
            text,
            truncation=True,
            max_length=max_length,
            padding=False,
            return_tensors=None
        )
        
        inputs.append(tokenized['input_ids'])
        labels.append(tokenized['input_ids'].copy())
    
    return {
        'input_ids': inputs,
        'labels': labels
    }

def main():
    # æ£€æŸ¥ç³»ç»Ÿä¿¡æ¯
    device = check_system_info()
    
    # æ¸…ç†å†…å­˜
    clean_memory()
    
    # éªŒè¯æ•°æ®
    data_file = "data/finetune/tarot_readings.jsonl"
    if not Path(data_file).exists():
        print(f"âŒ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_file}")
        print("è¯·å…ˆè¿è¡Œæ•°æ®å¤„ç†è„šæœ¬")
        return
    
    # éªŒè¯å’Œæ¸…ç†æ•°æ®
    valid_samples, tokenizer = validate_and_clean_data(data_file, max_length=4000)
    
    if len(valid_samples) < 10:
        print("âŒ æœ‰æ•ˆæ ·æœ¬å¤ªå°‘ï¼Œæ— æ³•è¿›è¡Œè®­ç»ƒ")
        return
    
    print(f"\nğŸš€ å¼€å§‹å¾®è°ƒï¼Œä½¿ç”¨ {len(valid_samples)} ä¸ªæ ·æœ¬")
    
    # è½¬æ¢ä¸ºDatasetæ ¼å¼
    dataset_dict = {
        'instruction': [s['instruction'] for s in valid_samples],
        'response': [s['response'] for s in valid_samples]
    }
    
    dataset = Dataset.from_dict(dataset_dict)
    
    # é¢„å¤„ç†æ•°æ®
    def preprocess_wrapper(examples):
        return preprocess_function(examples, tokenizer, max_length=4000)
    
    dataset = dataset.map(
        preprocess_wrapper,
        batched=True,
        remove_columns=dataset.column_names
    )
    
    print(f"âœ… æ•°æ®é¢„å¤„ç†å®Œæˆï¼Œæ ·æœ¬æ•°: {len(dataset)}")
    
    # æ¸…ç†å†…å­˜
    clean_memory()
    
    # åŠ è½½æ¨¡å‹ - ä¼˜åŒ–å†…å­˜ä½¿ç”¨
    print(f"ğŸ“¥ åŠ è½½ Qwen1.5-7B æ¨¡å‹...")
    model_name = "Qwen/Qwen1.5-7B-Chat"
    
    try:
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.float32 if device == "mps" else torch.float16,  # MPSä½¿ç”¨float32
            device_map=None,  # å…ˆä¸è‡ªåŠ¨åˆ†é…ï¼Œæ‰‹åŠ¨ç®¡ç†
            trust_remote_code=True,
            low_cpu_mem_usage=True,  # åŠ è½½æ—¶å‡å°‘CPUå†…å­˜ä½¿ç”¨
            attn_implementation="eager"  # ä½¿ç”¨ä¼ ç»Ÿæ³¨æ„åŠ›æœºåˆ¶ï¼Œæ›´ç¨³å®š
        )
        
        # æ‰‹åŠ¨ç§»åŠ¨åˆ°MPS
        if device == "mps":
            model = model.to("mps")
        
        print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼å‚æ•°é‡: {model.num_parameters():,}")
        print(f"ğŸ“Š æ¨¡å‹å¤§å°ä¼°ç®—: ~{model.num_parameters() * 2 / (1024**3):.1f}GB (fp16)")
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        print("ğŸ’¡ å°è¯•è§£å†³æ–¹æ¡ˆ:")
        print("  1. é‡å¯Terminalé‡Šæ”¾å†…å­˜")
        print("  2. è®¾ç½® export PYTORCH_ENABLE_MPS_FALLBACK=1")
        print("  3. ä¸´æ—¶å…³é—­å…¶ä»–åº”ç”¨")
        return
    
    # é…ç½®LoRA - ä½¿ç”¨é€‚ä¸­çš„å‚æ•°
    peft_config = LoraConfig(
        task_type=TaskType.CAUSAL_LM,
        inference_mode=False,
        r=16,  # é€‚ä¸­çš„rank
        lora_alpha=32,
        lora_dropout=0.1,
        target_modules=["q_proj", "v_proj", "k_proj", "o_proj"],  # å…¨éƒ¨æ³¨æ„åŠ›çŸ©é˜µ
        bias="none"
    )
    
    model = get_peft_model(model, peft_config)
    model.print_trainable_parameters()
    
    # æ¸…ç†å†…å­˜
    clean_memory()
    
    # è®­ç»ƒå‚æ•° - é’ˆå¯¹24GBå†…å­˜ä¼˜åŒ–
    training_args = TrainingArguments(
        output_dir="./models/qwen-tarot-24gb",
        num_train_epochs=3,
        per_device_train_batch_size=1,  # å‡å°batch sizeä»¥é€‚åº”MPSå†…å­˜é™åˆ¶
        gradient_accumulation_steps=8,  # å¢åŠ æ¢¯åº¦ç´¯ç§¯è¡¥å¿å°batch size
        warmup_steps=10,
        learning_rate=5e-5,
        fp16=False,  # MPSä¸æ”¯æŒfp16æ··åˆç²¾åº¦
        logging_steps=5,
        save_strategy="epoch",
        eval_strategy="no",  # ä¿®å¤ï¼šä½¿ç”¨æ–°çš„å‚æ•°å
        dataloader_num_workers=0,  # MPSä¸æ”¯æŒå¤šè¿›ç¨‹
        remove_unused_columns=False,
        push_to_hub=False,
        report_to=None,
        max_grad_norm=1.0,
        dataloader_pin_memory=False,
        save_total_limit=2,  # åªä¿ç•™æœ€è¿‘2ä¸ªcheckpoint
        prediction_loss_only=True
    )
    
    # æ•°æ®æ•´ç†å™¨
    data_collator = DataCollatorForLanguageModeling(
        tokenizer=tokenizer,
        mlm=False,
        pad_to_multiple_of=8
    )
    
    # åˆ›å»ºTrainer
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=dataset,
        data_collator=data_collator,
        tokenizer=tokenizer,
    )
    
    # å¼€å§‹è®­ç»ƒ
    print("ğŸ‹ï¸ å¼€å§‹è®­ç»ƒ...")
    print("ğŸ’¡ è®­ç»ƒæœŸé—´è¯·å‹¿è¿è¡Œå…¶ä»–å¤§å‹åº”ç”¨ï¼Œç¡®ä¿å†…å­˜å……è¶³")
    
    try:
        trainer.train()
        
        # ä¿å­˜æ¨¡å‹
        print("ğŸ’¾ ä¿å­˜æ¨¡å‹...")
        trainer.save_model()
        tokenizer.save_pretrained("./models/qwen-tarot-24gb")
        
        print("ğŸ‰ è®­ç»ƒå®Œæˆï¼")
        print(f"ğŸ“ æ¨¡å‹ä¿å­˜åœ¨: ./models/qwen-tarot-24gb")
        print(f"ğŸ¯ ä¸‹ä¸€æ­¥: è¿è¡Œ python scripts/test_qwen_tarot.py æµ‹è¯•æ¨¡å‹")
        
    except Exception as e:
        print(f"âŒ è®­ç»ƒå¤±è´¥: {e}")
        print("ğŸ”§ æ•…éšœæ’é™¤å»ºè®®:")
        print("  1. æ£€æŸ¥æ˜¯å¦æœ‰å…¶ä»–å¤§å‹åº”ç”¨å ç”¨å†…å­˜")
        print("  2. é‡å¯Terminalå’ŒPythonç¯å¢ƒ")
        print("  3. é™ä½batch_sizeåˆ°1")
        print("  4. æ£€æŸ¥MPSæ˜¯å¦æ­£å¸¸å·¥ä½œ")

if __name__ == "__main__":
    main()
#!/usr/bin/env python3
"""
æµ‹è¯•å¾®è°ƒåçš„å¡”ç½—AIæ¨¡å‹
"""

import os
import sys
import torch
from pathlib import Path

# è®¾ç½®ç¯å¢ƒå˜é‡
os.environ["PYTORCH_ENABLE_MPS_FALLBACK"] = "1"
os.environ["TOKENIZERS_PARALLELISM"] = "false"

try:
    from transformers import AutoTokenizer, AutoModelForCausalLM
    from peft import PeftModel
    print("âœ… ä¾èµ–åº“å¯¼å…¥æˆåŠŸ")
except ImportError as e:
    print(f"âŒ ç¼ºå°‘ä¾èµ–åº“: {e}")
    sys.exit(1)

def load_trained_model():
    """åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹"""
    print("ğŸ¤– åŠ è½½ä½ çš„ä¸“å±å¡”ç½—AIæ¨¡å‹...")
    
    # æ£€æŸ¥è®¾å¤‡
    if torch.backends.mps.is_available():
        device = "mps"
        print("âœ… ä½¿ç”¨ Apple Silicon MPS")
    else:
        device = "cpu"
        print("âš ï¸ ä½¿ç”¨ CPU")
    
    model_path = "./models/qwen-tarot-24gb"
    
    # åŠ è½½åŸºç¡€æ¨¡å‹
    base_model_name = "Qwen/Qwen1.5-1.8B-Chat"
    print(f"ğŸ“¥ åŠ è½½åŸºç¡€æ¨¡å‹: {base_model_name}")
    
    try:
        tokenizer = AutoTokenizer.from_pretrained(
            model_path, 
            trust_remote_code=True
        )
        
        base_model = AutoModelForCausalLM.from_pretrained(
            base_model_name,
            torch_dtype=torch.float32,
            device_map="cpu",
            trust_remote_code=True,
            low_cpu_mem_usage=True
        )
        
        # åŠ è½½LoRAé€‚é…å™¨
        print("ğŸ“¥ åŠ è½½ä½ çš„ä¸ªäººåŒ–é€‚é…å™¨...")
        model = PeftModel.from_pretrained(base_model, model_path)
        
        # ğŸ”§ å…³é”®ä¿®å¤ï¼šç¡®ä¿LoRAæƒé‡è¢«åº”ç”¨
        print("ğŸ”§ æ¿€æ´»LoRAé€‚é…å™¨...")
        model = model.merge_and_unload()  # åˆå¹¶LoRAæƒé‡åˆ°åŸºç¡€æ¨¡å‹
        
        # ç§»åŠ¨åˆ°è®¾å¤‡
        if device == "mps":
            model = model.to("mps")
        
        print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼")
        return model, tokenizer, device
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return None, None, None

def test_tarot_reading(model, tokenizer, device):
    """æµ‹è¯•å¡”ç½—è§£è¯»"""
    print("\nğŸ”® å¼€å§‹æµ‹è¯•ä½ çš„å¡”ç½—AI...")
    
    # æµ‹è¯•æ¡ˆä¾‹
    test_cases = [
        {
            "person": "Mel",
            "question": "æˆ‘çš„äº‹ä¸šå‘å±•å¦‚ä½•ï¼Ÿ",
            "cards": "æ„šäºº(æ­£ä½)ï¼›åŠ›é‡(æ­£ä½)ï¼›æ˜Ÿå¸å(æ­£ä½)",
            "spread": "ä¸‰å¼ ç‰Œè§£è¯»"
        },
        {
            "person": "æµ‹è¯•è€…",
            "question": "æ„Ÿæƒ…è¿åŠ¿",
            "cards": "æ‹äºº(æ­£ä½)ï¼›åœ£æ¯äºŒ(æ­£ä½)",
            "spread": "ç®€å•ç‰Œé˜µ"
        }
    ]
    
    for i, case in enumerate(test_cases, 1):
        print(f"\nğŸ“‹ æµ‹è¯•æ¡ˆä¾‹ {i}:")
        print(f"   å’¨è¯¢è€…: {case['person']}")
        print(f"   é—®é¢˜: {case['question']}")
        print(f"   ç‰Œ: {case['cards']}")
        print(f"   ç‰Œé˜µ: {case['spread']}")
        
        # æ„å»ºprompt
        prompt = f"""å¡”ç½—è§£è¯»ï¼š
å’¨è¯¢è€…ï¼š{case['person']}
é—®é¢˜ï¼š{case['question']}
ç‰Œé˜µï¼š{case['spread']}
ç‰Œï¼š{case['cards']}

è¯·æä¾›ä¸“ä¸šè§£è¯»ï¼š"""
        
        print(f"\nğŸ¤– AIè§£è¯»:")
        print("-" * 50)
        
        try:
            # ç”Ÿæˆå›ç­”
            inputs = tokenizer(prompt, return_tensors="pt")
            if device == "mps":
                inputs = {k: v.to("mps") for k, v in inputs.items()}
            
            with torch.no_grad():
                outputs = model.generate(
                    **inputs,
                    max_new_tokens=500,
                    temperature=0.7,
                    do_sample=True,
                    pad_token_id=tokenizer.eos_token_id,
                    repetition_penalty=1.1
                )
            
            # è§£ç å›ç­”
            full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)
            
            # æå–AIç”Ÿæˆçš„éƒ¨åˆ†ï¼ˆå»æ‰promptï¼‰
            ai_response = full_response[len(prompt):].strip()
            
            print(ai_response)
            print("-" * 50)
            
        except Exception as e:
            print(f"âŒ ç”Ÿæˆå¤±è´¥: {e}")
        
        print()

def interactive_mode(model, tokenizer, device):
    """äº¤äº’æ¨¡å¼"""
    print("\nğŸ¯ è¿›å…¥äº¤äº’æ¨¡å¼ (è¾“å…¥ 'quit' é€€å‡º)")
    print("ç°åœ¨ä½ å¯ä»¥å‘ä½ çš„å¡”ç½—AIæé—®äº†ï¼")
    
    while True:
        print("\n" + "="*60)
        
        try:
            person = input("å’¨è¯¢è€…å§“å: ").strip()
            if person.lower() == 'quit':
                break
                
            question = input("é—®é¢˜: ").strip()
            if question.lower() == 'quit':
                break
                
            cards = input("æŠ½åˆ°çš„ç‰Œ (ç”¨ï¼›åˆ†éš”): ").strip()
            if cards.lower() == 'quit':
                break
                
            spread = input("ç‰Œé˜µç±»å‹ (å¯é€‰): ").strip() or "è‡ªç”±ç‰Œé˜µ"
            if spread.lower() == 'quit':
                break
            
            # æ„å»ºprompt
            prompt = f"""å¡”ç½—è§£è¯»ï¼š
å’¨è¯¢è€…ï¼š{person}
é—®é¢˜ï¼š{question}
ç‰Œé˜µï¼š{spread}
ç‰Œï¼š{cards}

è¯·æä¾›ä¸“ä¸šè§£è¯»ï¼š"""
            
            print(f"\nğŸ”® {person}çš„å¡”ç½—è§£è¯»:")
            print("="*60)
            
            # ç”Ÿæˆå›ç­”
            inputs = tokenizer(prompt, return_tensors="pt")
            if device == "mps":
                inputs = {k: v.to("mps") for k, v in inputs.items()}
            
            with torch.no_grad():
                outputs = model.generate(
                    **inputs,
                    max_new_tokens=800,
                    temperature=0.8,
                    do_sample=True,
                    pad_token_id=tokenizer.eos_token_id,
                    repetition_penalty=1.1
                )
            
            # è§£ç å›ç­”
            full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)
            ai_response = full_response[len(prompt):].strip()
            
            print(ai_response)
            print("="*60)
            
        except KeyboardInterrupt:
            print("\n\nğŸ‘‹ æ„Ÿè°¢ä½¿ç”¨ä½ çš„ä¸“å±å¡”ç½—AI!")
            break
        except Exception as e:
            print(f"âŒ ç”Ÿæˆå¤±è´¥: {e}")

def main():
    print("ğŸ”® å¡”ç½—AIæµ‹è¯•ç¨‹åº")
    print("="*50)
    
    # åŠ è½½æ¨¡å‹
    model, tokenizer, device = load_trained_model()
    if model is None:
        return
    
    # è¿è¡Œæµ‹è¯•
    test_tarot_reading(model, tokenizer, device)
    
    # è¯¢é—®æ˜¯å¦è¿›å…¥äº¤äº’æ¨¡å¼
    choice = input("æ˜¯å¦è¿›å…¥äº¤äº’æ¨¡å¼ï¼Ÿ(y/n): ").strip().lower()
    if choice in ['y', 'yes', 'æ˜¯']:
        interactive_mode(model, tokenizer, device)
    
    print("\nğŸ‰ æµ‹è¯•å®Œæˆï¼ä½ çš„å¡”ç½—AIå·²ç»å‡†å¤‡å°±ç»ªï¼")

if __name__ == "__main__":
    main() 
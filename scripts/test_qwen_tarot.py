#!/usr/bin/env python3
"""
æµ‹è¯•å¾®è°ƒåçš„Qwenå¡”ç½—æ¨¡å‹
"""

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel
import warnings
warnings.filterwarnings("ignore")

def load_model():
    """åŠ è½½è®­ç»ƒåçš„æ¨¡å‹"""
    model_path = "./models/qwen-tarot-24gb"
    base_model_name = "Qwen/Qwen1.5-7B-Chat"
    
    print("ğŸ”® åŠ è½½å¡”ç½—AIæ¨¡å‹...")
    
    try:
        # åŠ è½½åˆ†è¯å™¨
        tokenizer = AutoTokenizer.from_pretrained(base_model_name, trust_remote_code=True)
        
        # åŠ è½½åŸºç¡€æ¨¡å‹
        base_model = AutoModelForCausalLM.from_pretrained(
            base_model_name,
            torch_dtype=torch.float16,
            trust_remote_code=True,
            device_map="auto"
        )
        
        # åŠ è½½LoRAæƒé‡
        model = PeftModel.from_pretrained(base_model, model_path)
        
        # åˆå¹¶æƒé‡ä»¥æé«˜æ¨ç†é€Ÿåº¦
        model = model.merge_and_unload()
        
        print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼")
        return model, tokenizer
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        print("ğŸ’¡ è¯·ç¡®ä¿è®­ç»ƒå·²å®Œæˆä¸”æ¨¡å‹æ–‡ä»¶å­˜åœ¨")
        return None, None

def generate_reading(model, tokenizer, prompt, max_length=500):
    """ç”Ÿæˆå¡”ç½—è§£è¯»"""
    inputs = tokenizer(prompt, return_tensors="pt")
    
    # å¦‚æœä½¿ç”¨MPSï¼Œç§»åŠ¨åˆ°è®¾å¤‡
    if torch.backends.mps.is_available():
        inputs = {k: v.to("mps") for k, v in inputs.items()}
    
    with torch.no_grad():
        outputs = model.generate(
            inputs.input_ids,
            max_new_tokens=max_length,
            temperature=0.8,
            do_sample=True,
            top_p=0.9,
            repetition_penalty=1.1,
            pad_token_id=tokenizer.eos_token_id,
            eos_token_id=tokenizer.eos_token_id
        )
    
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    # ç§»é™¤åŸå§‹promptï¼Œåªè¿”å›ç”Ÿæˆçš„éƒ¨åˆ†
    response = response[len(prompt):].strip()
    
    return response

def main():
    model, tokenizer = load_model()
    
    if model is None:
        return
    
    # æµ‹è¯•æ¡ˆä¾‹
    test_cases = [
        {
            "title": "å·¥ä½œå‘å±•æµ‹è¯•",
            "prompt": """å¡”ç½—è§£è¯»ï¼š
å’¨è¯¢è€…ï¼šMel
é—®é¢˜ï¼šæœªæ¥å·¥ä½œå‘å±•æ–¹å‘
ç‰Œé˜µï¼šä¸‰å¼ ç‰Œ
ç‰Œï¼šæ„šè€…ï¼›æ˜Ÿå¸ä¸‰ï¼›å®å‰‘ç‹å

è¯·æä¾›ä¸“ä¸šè§£è¯»ï¼š"""
        },
        {
            "title": "æ„Ÿæƒ…å’¨è¯¢æµ‹è¯•", 
            "prompt": """å¡”ç½—è§£è¯»ï¼š
å’¨è¯¢è€…ï¼šSarah
é—®é¢˜ï¼šå½“å‰æ„Ÿæƒ…çŠ¶å†µå¦‚ä½•
ç‰Œé˜µï¼šå•å¼ ç‰Œ
ç‰Œï¼šæ‹äºº

è¯·æä¾›ä¸“ä¸šè§£è¯»ï¼š"""
        },
        {
            "title": "ä¸ªäººæˆé•¿æµ‹è¯•",
            "prompt": """å¡”ç½—è§£è¯»ï¼š
å’¨è¯¢è€…ï¼šKK
é—®é¢˜ï¼š2025å¹´ä¸ªäººå‘å±•é‡ç‚¹
ç‰Œé˜µï¼šè¿‡å»ç°åœ¨æœªæ¥
ç‰Œï¼šéšè€…ï¼›æ˜Ÿå¸çš‡åï¼›å¤ªé˜³

è¯·æä¾›ä¸“ä¸šè§£è¯»ï¼š"""
        }
    ]
    
    print("ğŸ­ å¼€å§‹æµ‹è¯•å¾®è°ƒåçš„å¡”ç½—AI...")
    print("="*60)
    
    for i, test in enumerate(test_cases, 1):
        print(f"\nã€æµ‹è¯• {i}ã€‘{test['title']}")
        print("-" * 40)
        print("ğŸ“ è¾“å…¥:")
        print(test['prompt'])
        print("\nğŸ”® AIè§£è¯»:")
        
        try:
            response = generate_reading(model, tokenizer, test['prompt'])
            print(response)
            
        except Exception as e:
            print(f"âŒ ç”Ÿæˆå¤±è´¥: {e}")
        
        print("\n" + "="*60)
        
        if i < len(test_cases):
            input("æŒ‰ Enter ç»§ç»­ä¸‹ä¸€ä¸ªæµ‹è¯•...")
    
    print("\nğŸŠ æµ‹è¯•å®Œæˆï¼")
    print("ğŸ’¡ å¦‚æœç»“æœä¸ç†æƒ³ï¼Œå¯ä»¥:")
    print("   1. å¢åŠ è®­ç»ƒæ•°æ®")
    print("   2. è°ƒæ•´è®­ç»ƒè½®æ•°")
    print("   3. ä¼˜åŒ–promptæ ¼å¼")

if __name__ == "__main__":
    main() 
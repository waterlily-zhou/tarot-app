#!/usr/bin/env python3
"""
RAGå¢å¼ºå¡”ç½—AIç³»ç»Ÿ - ç»“åˆæ£€ç´¢å’Œç”Ÿæˆ
"""

import os
import sys
import torch
import json
import numpy as np
import sqlite3
from pathlib import Path
from typing import List, Dict, Tuple
import re

# è®¾ç½®ç¯å¢ƒå˜é‡
os.environ["PYTORCH_ENABLE_MPS_FALLBACK"] = "1"
os.environ["TOKENIZERS_PARALLELISM"] = "false"

try:
    from transformers import AutoTokenizer, AutoModelForCausalLM
    from peft import PeftModel
    from sentence_transformers import SentenceTransformer
    print("âœ… ä¾èµ–åº“å¯¼å…¥æˆåŠŸ")
except ImportError as e:
    print(f"âŒ ç¼ºå°‘ä¾èµ–åº“: {e}")
    print("è¯·å®‰è£…: pip install sentence-transformers")
    sys.exit(1)

class TarotKnowledgeBase:
    """å¡”ç½—çŸ¥è¯†åº“ - ç”¨äºæ£€ç´¢ç›¸å…³è§£è¯»"""
    
    def __init__(self, db_path: str = "data/tarot_knowledge.db"):
        self.db_path = db_path
        self.embedding_model = None
        self.init_database()
        
    def init_database(self):
        """åˆå§‹åŒ–æ•°æ®åº“"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # åˆ›å»ºè¡¨æ ¼
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS readings (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                person TEXT,
                question TEXT,
                cards TEXT,
                spread TEXT,
                content TEXT,
                embedding BLOB,
                source_file TEXT
            )
        ''')
        
        conn.commit()
        conn.close()
        print(f"âœ… çŸ¥è¯†åº“æ•°æ®åº“åˆå§‹åŒ–å®Œæˆ: {self.db_path}")
    
    def load_embedding_model(self):
        """åŠ è½½åµŒå…¥æ¨¡å‹"""
        if self.embedding_model is None:
            print("ğŸ“¥ åŠ è½½ä¸­æ–‡åµŒå…¥æ¨¡å‹...")
            try:
                # ä½¿ç”¨ä¸­æ–‡åµŒå…¥æ¨¡å‹
                self.embedding_model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
                print("âœ… åµŒå…¥æ¨¡å‹åŠ è½½æˆåŠŸ")
            except Exception as e:
                print(f"âš ï¸ åµŒå…¥æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆ: {e}")
                # å¤‡ç”¨ï¼šä½¿ç”¨æ›´ç®€å•çš„æ¨¡å‹
                self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')
        
        return self.embedding_model
    
    def add_reading(self, person: str, question: str, cards: List[str], 
                   spread: str, content: str, source_file: str):
        """æ·»åŠ è§£è¯»åˆ°çŸ¥è¯†åº“"""
        # ç”ŸæˆåµŒå…¥å‘é‡
        embedding_model = self.load_embedding_model()
        
        # ç»„åˆæŸ¥è¯¢æ–‡æœ¬ç”¨äºåµŒå…¥
        query_text = f"å’¨è¯¢è€…:{person} é—®é¢˜:{question} ç‰Œ:{';'.join(cards)} ç‰Œé˜µ:{spread}"
        embedding = embedding_model.encode(query_text)
        
        # å­˜å‚¨åˆ°æ•°æ®åº“
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute('''
            INSERT INTO readings (person, question, cards, spread, content, embedding, source_file)
            VALUES (?, ?, ?, ?, ?, ?, ?)
        ''', (person, question, ';'.join(cards), spread, content, 
              embedding.tobytes(), source_file))
        
        conn.commit()
        conn.close()
    
    def search_similar_readings(self, person: str, question: str, 
                              cards: List[str], spread: str, top_k: int = 3) -> List[Dict]:
        """æœç´¢ç›¸ä¼¼çš„è§£è¯»"""
        embedding_model = self.load_embedding_model()
        
        # ç”ŸæˆæŸ¥è¯¢åµŒå…¥
        query_text = f"å’¨è¯¢è€…:{person} é—®é¢˜:{question} ç‰Œ:{';'.join(cards)} ç‰Œé˜µ:{spread}"
        query_embedding = embedding_model.encode(query_text)
        
        # ä»æ•°æ®åº“æ£€ç´¢æ‰€æœ‰è®°å½•
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute('SELECT * FROM readings')
        rows = cursor.fetchall()
        conn.close()
        
        if not rows:
            return []
        
        # è®¡ç®—ç›¸ä¼¼åº¦
        similarities = []
        for row in rows:
            stored_embedding = np.frombuffer(row[6], dtype=np.float32)
            similarity = np.dot(query_embedding, stored_embedding) / (
                np.linalg.norm(query_embedding) * np.linalg.norm(stored_embedding)
            )
            
            similarities.append({
                'id': row[0],
                'person': row[1],
                'question': row[2],
                'cards': row[3],
                'spread': row[4],
                'content': row[5],
                'source_file': row[7],
                'similarity': similarity
            })
        
        # æŒ‰ç›¸ä¼¼åº¦æ’åº
        similarities.sort(key=lambda x: x['similarity'], reverse=True)
        
        return similarities[:top_k]

class RAGTarotAI:
    """RAGå¢å¼ºçš„å¡”ç½—AI"""
    
    def __init__(self):
        self.knowledge_base = TarotKnowledgeBase()
        self.model = None
        self.tokenizer = None
        self.device = None
        
    def load_model(self):
        """åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹"""
        print("ğŸ¤– åŠ è½½RAGå¢å¼ºå¡”ç½—AI...")
        
        # æ£€æŸ¥è®¾å¤‡
        if torch.backends.mps.is_available():
            self.device = "mps"
            print("âœ… ä½¿ç”¨ Apple Silicon MPS")
        else:
            self.device = "cpu"
            print("âš ï¸ ä½¿ç”¨ CPU")
        
        model_path = "./models/qwen-tarot-24gb"
        base_model_name = "Qwen/Qwen1.5-1.8B-Chat"
        
        try:
            # ä¿®å¤LoRAé…ç½®
            adapter_config_path = Path(model_path) / "adapter_config.json"
            with open(adapter_config_path, 'r') as f:
                config = json.load(f)
            config["inference_mode"] = False
            with open(adapter_config_path, 'w') as f:
                json.dump(config, f, indent=2)
            
            # åŠ è½½æ¨¡å‹
            self.tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
            
            base_model = AutoModelForCausalLM.from_pretrained(
                base_model_name,
                torch_dtype=torch.float32,
                device_map="cpu",
                trust_remote_code=True,
                low_cpu_mem_usage=True
            )
            
            self.model = PeftModel.from_pretrained(base_model, model_path)
            
            if self.device == "mps":
                self.model = self.model.to("mps")
            
            print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
            return True
            
        except Exception as e:
            print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            return False
    
    def build_knowledge_base(self):
        """æ„å»ºçŸ¥è¯†åº“"""
        print("ğŸ“š æ„å»ºå¡”ç½—çŸ¥è¯†åº“...")
        
        # æ£€æŸ¥æ˜¯å¦å·²æœ‰æ•°æ®
        conn = sqlite3.connect(self.knowledge_base.db_path)
        cursor = conn.cursor()
        cursor.execute('SELECT COUNT(*) FROM readings')
        count = cursor.fetchone()[0]
        conn.close()
        
        if count > 0:
            print(f"âœ… çŸ¥è¯†åº“å·²æœ‰ {count} æ¡è®°å½•")
            return
        
        # ä»è®­ç»ƒæ•°æ®æ„å»ºçŸ¥è¯†åº“
        data_file = "data/finetune/tarot_readings.jsonl"
        if not Path(data_file).exists():
            print(f"âŒ è®­ç»ƒæ•°æ®ä¸å­˜åœ¨: {data_file}")
            return
        
        count = 0
        with open(data_file, 'r', encoding='utf-8') as f:
            for line in f:
                try:
                    sample = json.loads(line)
                    metadata = sample['metadata']
                    
                    # æå–ä¿¡æ¯
                    person = metadata.get('person', '')
                    cards = metadata.get('cards', [])
                    spread = metadata.get('spread', '')
                    content = sample['response']
                    source_file = metadata.get('source_file', '')
                    
                    # ä»æŒ‡ä»¤ä¸­æå–é—®é¢˜
                    instruction = sample['instruction']
                    question_match = re.search(r'é—®é¢˜ï¼š([^\n]+)', instruction)
                    question = question_match.group(1) if question_match else ''
                    
                    # æ·»åŠ åˆ°çŸ¥è¯†åº“
                    self.knowledge_base.add_reading(
                        person, question, cards, spread, content, source_file
                    )
                    
                    count += 1
                    if count % 10 == 0:
                        print(f"   å·²å¤„ç† {count} æ¡è®°å½•...")
                        
                except Exception as e:
                    print(f"âš ï¸ å¤„ç†è®°å½•æ—¶å‡ºé”™: {e}")
        
        print(f"âœ… çŸ¥è¯†åº“æ„å»ºå®Œæˆï¼Œå…± {count} æ¡è®°å½•")
    
    def generate_enhanced_reading(self, person: str, question: str, 
                                cards: List[str], spread: str = "è‡ªç”±ç‰Œé˜µ") -> str:
        """ç”ŸæˆRAGå¢å¼ºçš„è§£è¯»"""
        
        # 1. æ£€ç´¢ç›¸ä¼¼è§£è¯»
        print("ğŸ” æ£€ç´¢ç›¸ä¼¼è§£è¯»...")
        similar_readings = self.knowledge_base.search_similar_readings(
            person, question, cards, spread, top_k=3
        )
        
        # 2. æ„å»ºå¢å¼ºprompt
        enhanced_prompt = self._build_enhanced_prompt(
            person, question, cards, spread, similar_readings
        )
        
        # 3. ç”Ÿæˆè§£è¯»
        print("ğŸ¤– ç”Ÿæˆå¢å¼ºè§£è¯»...")
        generated = self._generate_with_model(enhanced_prompt)
        
        # 4. åå¤„ç†å’Œæ ¼å¼åŒ–
        final_reading = self._format_final_reading(
            person, question, cards, spread, generated, similar_readings
        )
        
        return final_reading
    
    def _build_enhanced_prompt(self, person: str, question: str, cards: List[str], 
                             spread: str, similar_readings: List[Dict]) -> str:
        """æ„å»ºå¢å¼ºprompt"""
        
        prompt = f"""å¡”ç½—è§£è¯»ï¼š
å’¨è¯¢è€…ï¼š{person}
é—®é¢˜ï¼š{question}
ç‰Œé˜µï¼š{spread}
ç‰Œï¼š{';'.join(cards)}

å‚è€ƒç›¸ä¼¼è§£è¯»ï¼š
"""
        
        # æ·»åŠ ç›¸ä¼¼è§£è¯»ä½œä¸ºå‚è€ƒ
        for i, reading in enumerate(similar_readings, 1):
            similarity_score = reading['similarity']
            if similarity_score > 0.3:  # åªä½¿ç”¨ç›¸ä¼¼åº¦è¾ƒé«˜çš„
                prompt += f"\nå‚è€ƒ{i} (ç›¸ä¼¼åº¦: {similarity_score:.2f}):\n"
                prompt += f"å’¨è¯¢è€…: {reading['person']}\n"
                prompt += f"é—®é¢˜: {reading['question']}\n"
                prompt += f"ç‰Œ: {reading['cards']}\n"
                
                # æˆªå–å‚è€ƒå†…å®¹çš„å…³é”®éƒ¨åˆ† (å‰500å­—ç¬¦)
                ref_content = reading['content'][:500]
                prompt += f"è§£è¯»æ‘˜è¦: {ref_content}...\n"
        
        prompt += "\nåŸºäºä»¥ä¸Šå‚è€ƒå’Œä½ çš„ä¸“ä¸šçŸ¥è¯†ï¼Œè¯·æä¾›ä¸“ä¸šè§£è¯»ï¼š"
        
        return prompt
    
    def _generate_with_model(self, prompt: str) -> str:
        """ä½¿ç”¨æ¨¡å‹ç”Ÿæˆè§£è¯»"""
        try:
            inputs = self.tokenizer(prompt, return_tensors="pt")
            if self.device == "mps":
                inputs = {k: v.to("mps") for k, v in inputs.items()}
            
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=600,
                    temperature=0.7,
                    do_sample=True,
                    pad_token_id=self.tokenizer.eos_token_id,
                    repetition_penalty=1.1,
                    top_p=0.9
                )
            
            full_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            generated = full_response[len(prompt):].strip()
            
            return generated
            
        except Exception as e:
            print(f"âŒ ç”Ÿæˆå¤±è´¥: {e}")
            return "ç”Ÿæˆè§£è¯»æ—¶é‡åˆ°æŠ€æœ¯é—®é¢˜ï¼Œè¯·é‡è¯•ã€‚"
    
    def _format_final_reading(self, person: str, question: str, cards: List[str], 
                            spread: str, generated: str, similar_readings: List[Dict]) -> str:
        """æ ¼å¼åŒ–æœ€ç»ˆè§£è¯»"""
        
        final_reading = f"""ğŸ”® {person}çš„å¡”ç½—è§£è¯»

ğŸ“‹ å’¨è¯¢ä¿¡æ¯ï¼š
   é—®é¢˜ï¼š{question}
   ç‰Œé˜µï¼š{spread}
   æŠ½åˆ°çš„ç‰Œï¼š{' | '.join(cards)}

ğŸ¯ AIè§£è¯»ï¼š
{generated}

ğŸ“š ç›¸å…³å‚è€ƒï¼š
"""
        
        # æ·»åŠ å‚è€ƒä¿¡æ¯
        for i, reading in enumerate(similar_readings[:2], 1):  # åªæ˜¾ç¤ºå‰2ä¸ªå‚è€ƒ
            if reading['similarity'] > 0.3:
                final_reading += f"   å‚è€ƒ{i}: {reading['person']}çš„{reading['question']} (ç›¸ä¼¼åº¦: {reading['similarity']:.1%})\n"
        
        return final_reading

def main():
    print("ğŸ”® RAGå¢å¼ºå¡”ç½—AIç³»ç»Ÿ")
    print("="*50)
    
    # åˆå§‹åŒ–ç³»ç»Ÿ
    rag_ai = RAGTarotAI()
    
    # åŠ è½½æ¨¡å‹
    if not rag_ai.load_model():
        return
    
    # æ„å»ºçŸ¥è¯†åº“
    rag_ai.build_knowledge_base()
    
    # æµ‹è¯•æ¡ˆä¾‹
    print("\nğŸ§ª æµ‹è¯•RAGå¢å¼ºè§£è¯»...")
    
    test_cases = [
        {
            "person": "Mel",
            "question": "äº‹ä¸šå‘å±•æ–¹å‘",
            "cards": ["æ„šäºº(æ­£ä½)", "åŠ›é‡(æ­£ä½)", "æ˜Ÿå¸å(æ­£ä½)"],
            "spread": "ä¸‰å¼ ç‰Œè§£è¯»"
        },
        {
            "person": "æµ‹è¯•è€…",
            "question": "æ„Ÿæƒ…è¿åŠ¿",
            "cards": ["æ‹äºº(æ­£ä½)", "åœ£æ¯äºŒ(æ­£ä½)", "åœ£æ¯å(æ­£ä½)"],
            "spread": "æ„Ÿæƒ…ç‰Œé˜µ"
        }
    ]
    
    for i, case in enumerate(test_cases, 1):
        print(f"\n{'='*60}")
        print(f"æµ‹è¯•æ¡ˆä¾‹ {i}")
        print('='*60)
        
        reading = rag_ai.generate_enhanced_reading(
            case["person"], case["question"], 
            case["cards"], case["spread"]
        )
        
        print(reading)
        
        if i < len(test_cases):
            input("\næŒ‰Enterç»§ç»­ä¸‹ä¸€ä¸ªæµ‹è¯•...")
    
    # è¿›å…¥äº¤äº’æ¨¡å¼
    print(f"\n{'='*60}")
    choice = input("æ˜¯å¦è¿›å…¥äº¤äº’æ¨¡å¼ï¼Ÿ(y/n): ").strip().lower()
    
    if choice in ['y', 'yes', 'æ˜¯']:
        print("\nğŸ¯ è¿›å…¥RAGå¢å¼ºäº¤äº’æ¨¡å¼ (è¾“å…¥ 'quit' é€€å‡º)")
        
        while True:
            try:
                print("\n" + "-"*40)
                person = input("å’¨è¯¢è€…å§“å: ").strip()
                if person.lower() == 'quit':
                    break
                
                question = input("é—®é¢˜: ").strip()
                if question.lower() == 'quit':
                    break
                
                cards_input = input("æŠ½åˆ°çš„ç‰Œ (ç”¨åˆ†å·;åˆ†éš”): ").strip()
                if cards_input.lower() == 'quit':
                    break
                
                cards = [card.strip() for card in cards_input.split(';') if card.strip()]
                
                spread = input("ç‰Œé˜µç±»å‹ (å¯é€‰): ").strip() or "è‡ªç”±ç‰Œé˜µ"
                if spread.lower() == 'quit':
                    break
                
                print("\nğŸ”® æ­£åœ¨ç”ŸæˆRAGå¢å¼ºè§£è¯»...")
                reading = rag_ai.generate_enhanced_reading(person, question, cards, spread)
                
                print(f"\n{reading}")
                
            except KeyboardInterrupt:
                print("\n\nğŸ‘‹ æ„Ÿè°¢ä½¿ç”¨RAGå¢å¼ºå¡”ç½—AI!")
                break
            except Exception as e:
                print(f"âŒ ç”Ÿæˆå¤±è´¥: {e}")
    
    print("\nğŸ‰ RAGå¢å¼ºå¡”ç½—AIç³»ç»Ÿæµ‹è¯•å®Œæˆ!")

if __name__ == "__main__":
    main() 
#!/usr/bin/env python3
"""
è°ƒè¯•ç‰ˆå¾®è°ƒè„šæœ¬ - å¢åŠ è¯¦ç»†è¿›åº¦è¾“å‡º
"""

import os
import sys
import torch
import time
import json
from pathlib import Path

print("ğŸš€ å¼€å§‹è°ƒè¯•ç‰ˆè®­ç»ƒ...")
print(f"â° å¼€å§‹æ—¶é—´: {time.strftime('%H:%M:%S')}")

# è®¾ç½®ç¯å¢ƒå˜é‡
os.environ["PYTORCH_ENABLE_MPS_FALLBACK"] = "1"
os.environ["TOKENIZERS_PARALLELISM"] = "false"

try:
    from transformers import (
        AutoTokenizer, AutoModelForCausalLM, 
        TrainingArguments, Trainer, DataCollatorForLanguageModeling
    )
    from peft import LoraConfig, get_peft_model, TaskType
    from datasets import Dataset
    print("âœ… æ‰€æœ‰ä¾èµ–åº“å¯¼å…¥æˆåŠŸ")
except ImportError as e:
    print(f"âŒ ä¾èµ–åº“å¯¼å…¥å¤±è´¥: {e}")
    sys.exit(1)

def debug_system():
    """è°ƒè¯•ç³»ç»Ÿä¿¡æ¯"""
    print("\nğŸ“Š ç³»ç»Ÿä¿¡æ¯:")
    
    # æ£€æŸ¥è®¾å¤‡
    if torch.backends.mps.is_available():
        print("  âœ… MPS å¯ç”¨")
        device = "mps"
    else:
        print("  âš ï¸ MPS ä¸å¯ç”¨ï¼Œä½¿ç”¨ CPU")
        device = "cpu"
    
    # æ£€æŸ¥å†…å­˜
    import psutil
    memory = psutil.virtual_memory()
    print(f"  ğŸ’¾ æ€»å†…å­˜: {memory.total / (1024**3):.1f} GB")
    print(f"  ğŸ’¾ å¯ç”¨å†…å­˜: {memory.available / (1024**3):.1f} GB")
    print(f"  ğŸ’¾ ä½¿ç”¨ç‡: {memory.percent}%")
    
    return device

def load_and_check_data():
    """åŠ è½½å¹¶æ£€æŸ¥æ•°æ®"""
    print("\nğŸ“„ åŠ è½½è®­ç»ƒæ•°æ®...")
    
    data_file = "data/finetune/tarot_readings.jsonl"
    if not Path(data_file).exists():
        print(f"âŒ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_file}")
        return None
    
    samples = []
    with open(data_file, 'r', encoding='utf-8') as f:
        for i, line in enumerate(f):
            if i >= 5:  # åªåŠ è½½å‰5ä¸ªæ ·æœ¬è¿›è¡Œæµ‹è¯•
                break
            samples.append(json.loads(line))
    
    print(f"âœ… åŠ è½½äº† {len(samples)} ä¸ªæ ·æœ¬ (æµ‹è¯•ç”¨)")
    for i, sample in enumerate(samples):
        title = sample.get('metadata', {}).get('title', 'unknown')
        print(f"  æ ·æœ¬ {i+1}: {title}")
    
    return samples

def test_model_loading():
    """æµ‹è¯•æ¨¡å‹åŠ è½½"""
    print("\nğŸ¤– æµ‹è¯•æ¨¡å‹åŠ è½½...")
    
    model_name = "Qwen/Qwen1.5-1.8B-Chat"
    print(f"ğŸ“¥ åŠ è½½åˆ†è¯å™¨: {model_name}")
    
    try:
        tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        print("âœ… åˆ†è¯å™¨åŠ è½½æˆåŠŸ")
    except Exception as e:
        print(f"âŒ åˆ†è¯å™¨åŠ è½½å¤±è´¥: {e}")
        return None, None
    
    print(f"ğŸ“¥ åŠ è½½æ¨¡å‹: {model_name}")
    start_time = time.time()
    
    try:
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.float32,
            device_map="cpu",
            trust_remote_code=True,
            low_cpu_mem_usage=True
        )
        load_time = time.time() - start_time
        print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼Œè€—æ—¶: {load_time:.1f}ç§’")
        print(f"ğŸ“Š æ¨¡å‹å‚æ•°é‡: {model.num_parameters():,}")
        
        # ç§»åŠ¨åˆ°MPS
        device = debug_system()
        if device == "mps":
            print("ğŸ”„ ç§»åŠ¨æ¨¡å‹åˆ°MPS...")
            start_time = time.time()
            model = model.to("mps")
            move_time = time.time() - start_time
            print(f"âœ… æ¨¡å‹ç§»åŠ¨åˆ°MPSæˆåŠŸï¼Œè€—æ—¶: {move_time:.1f}ç§’")
        
        return model, tokenizer
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return None, None

def test_data_processing(samples, tokenizer):
    """æµ‹è¯•æ•°æ®å¤„ç†"""
    print("\nğŸ”§ æµ‹è¯•æ•°æ®å¤„ç†...")
    
    dataset_dict = {
        'instruction': [s['instruction'] for s in samples],
        'response': [s['response'] for s in samples]
    }
    
    dataset = Dataset.from_dict(dataset_dict)
    print(f"âœ… åˆ›å»ºæ•°æ®é›†ï¼Œæ ·æœ¬æ•°: {len(dataset)}")
    
    # æµ‹è¯•é¢„å¤„ç†
    def preprocess_function(examples):
        inputs = []
        labels = []
        
        for instruction, response in zip(examples['instruction'], examples['response']):
            text = f"{instruction}\n\n{response}"
            
            tokenized = tokenizer(
                text,
                truncation=True,
                max_length=512,  # ä½¿ç”¨è¾ƒå°çš„é•¿åº¦è¿›è¡Œæµ‹è¯•
                padding=False,
                return_tensors=None
            )
            
            inputs.append(tokenized['input_ids'])
            labels.append(tokenized['input_ids'].copy())
        
        return {'input_ids': inputs, 'labels': labels}
    
    print("ğŸ”„ å¤„ç†æ•°æ®...")
    processed_dataset = dataset.map(
        preprocess_function,
        batched=True,
        remove_columns=dataset.column_names
    )
    print(f"âœ… æ•°æ®å¤„ç†å®Œæˆï¼Œå¤„ç†åæ ·æœ¬æ•°: {len(processed_dataset)}")
    
    # æ‰“å°ç¬¬ä¸€ä¸ªæ ·æœ¬çš„tokené•¿åº¦
    first_sample = processed_dataset[0]
    print(f"ğŸ“Š ç¬¬ä¸€ä¸ªæ ·æœ¬tokené•¿åº¦: {len(first_sample['input_ids'])}")
    
    return processed_dataset

def test_lora_setup(model):
    """æµ‹è¯•LoRAè®¾ç½®"""
    print("\nâš™ï¸ æµ‹è¯•LoRAè®¾ç½®...")
    
    peft_config = LoraConfig(
        task_type=TaskType.CAUSAL_LM,
        inference_mode=False,
        r=8,  # æ›´å°çš„rankç”¨äºæµ‹è¯•
        lora_alpha=16,
        lora_dropout=0.1,
        target_modules=["q_proj", "v_proj"],  # åªå¾®è°ƒéƒ¨åˆ†æ¨¡å—
        bias="none"
    )
    
    model = get_peft_model(model, peft_config)
    print("âœ… LoRAé…ç½®æˆåŠŸ")
    model.print_trainable_parameters()
    
    return model

def test_trainer_setup(model, tokenizer, dataset):
    """æµ‹è¯•Trainerè®¾ç½®"""
    print("\nğŸ‹ï¸ æµ‹è¯•Trainerè®¾ç½®...")
    
    training_args = TrainingArguments(
        output_dir="./models/debug-test",
        num_train_epochs=1,  # åªè®­ç»ƒ1ä¸ªepoch
        per_device_train_batch_size=1,
        gradient_accumulation_steps=2,
        warmup_steps=1,
        learning_rate=5e-5,
        fp16=False,
        logging_steps=1,  # æ¯æ­¥éƒ½è¾“å‡ºæ—¥å¿—
        save_strategy="no",  # ä¸ä¿å­˜checkpointï¼Œåªæµ‹è¯•
        eval_strategy="no",
        dataloader_num_workers=0,
        remove_unused_columns=False,
        push_to_hub=False,
        report_to=None,
        max_grad_norm=1.0,
        dataloader_pin_memory=False
    )
    
    data_collator = DataCollatorForLanguageModeling(
        tokenizer=tokenizer,
        mlm=False,
        pad_to_multiple_of=8
    )
    
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=dataset,
        data_collator=data_collator,
        tokenizer=tokenizer,
    )
    
    print("âœ… Trainerè®¾ç½®æˆåŠŸ")
    return trainer

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¯ å¼€å§‹è°ƒè¯•æµç¨‹...")
    
    # 1. æ£€æŸ¥ç³»ç»Ÿ
    device = debug_system()
    
    # 2. åŠ è½½æ•°æ®
    samples = load_and_check_data()
    if not samples:
        return
    
    # 3. æµ‹è¯•æ¨¡å‹åŠ è½½
    model, tokenizer = test_model_loading()
    if model is None:
        return
    
    # 4. æµ‹è¯•æ•°æ®å¤„ç†
    dataset = test_data_processing(samples, tokenizer)
    
    # 5. æµ‹è¯•LoRA
    model = test_lora_setup(model)
    
    # 6. æµ‹è¯•Trainer
    trainer = test_trainer_setup(model, tokenizer, dataset)
    
    # 7. å¼€å§‹è®­ç»ƒæµ‹è¯•
    print("\nğŸš€ å¼€å§‹è®­ç»ƒæµ‹è¯•...")
    print("æ³¨æ„ï¼šè¿™åªæ˜¯ä¸€ä¸ªçŸ­æ—¶é—´çš„è®­ç»ƒæµ‹è¯•")
    
    try:
        trainer.train()
        print("ğŸ‰ è®­ç»ƒæµ‹è¯•å®Œæˆï¼")
        
        # æ£€æŸ¥è¾“å‡ºç›®å½•
        output_dir = Path("./models/debug-test")
        if output_dir.exists():
            files = list(output_dir.glob("*"))
            print(f"ğŸ“ ç”Ÿæˆäº† {len(files)} ä¸ªæ–‡ä»¶")
            for f in files[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ªæ–‡ä»¶
                print(f"  - {f.name}")
        
    except Exception as e:
        print(f"âŒ è®­ç»ƒå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main() 
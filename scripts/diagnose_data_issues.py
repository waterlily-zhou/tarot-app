#!/usr/bin/env python3
"""
è¯Šæ–­è®­ç»ƒæ•°æ®ä¸­çš„é—®é¢˜æ ·æœ¬
"""

import json
from pathlib import Path
from transformers import AutoTokenizer

def diagnose_training_data():
    data_file = Path("data/finetune/tarot_readings.jsonl")
    
    if not data_file.exists():
        print("âŒ è®­ç»ƒæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨")
        return
    
    print("ğŸ” è¯Šæ–­è®­ç»ƒæ•°æ®...")
    
    # åŠ è½½åˆ†è¯å™¨æ¥æµ‹è¯•é•¿åº¦
    try:
        tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen1.5-7B-Chat", trust_remote_code=True)
    except:
        print("âš ï¸ æ— æ³•åŠ è½½åˆ†è¯å™¨ï¼Œä»…åˆ†æå­—ç¬¦é•¿åº¦")
        tokenizer = None
    
    samples = []
    with open(data_file, 'r', encoding='utf-8') as f:
        for i, line in enumerate(f):
            try:
                sample = json.loads(line)
                samples.append((i, sample))
            except json.JSONDecodeError as e:
                print(f"âŒ ç¬¬{i+1}è¡ŒJSONè§£æé”™è¯¯: {e}")
    
    print(f"ğŸ“Š æ€»æ ·æœ¬æ•°: {len(samples)}")
    
    issues = []
    
    for i, (line_num, sample) in enumerate(samples):
        try:
            instruction = sample.get('instruction', '')
            response = sample.get('response', '')
            
            # æ£€æŸ¥æ•°æ®ç±»å‹
            if not isinstance(instruction, str):
                issues.append(f"ç¬¬{line_num+1}è¡Œ: instructionä¸æ˜¯å­—ç¬¦ä¸²ï¼Œç±»å‹: {type(instruction)}")
                continue
                
            if not isinstance(response, str):
                issues.append(f"ç¬¬{line_num+1}è¡Œ: responseä¸æ˜¯å­—ç¬¦ä¸²ï¼Œç±»å‹: {type(response)}")
                continue
            
            # æ£€æŸ¥é•¿åº¦
            char_len = len(instruction) + len(response)
            
            if tokenizer:
                # æ¨¡æ‹Ÿå®Œæ•´çš„è®­ç»ƒæ ¼å¼
                full_text = f"<|im_start|>user\n{instruction}<|im_end|>\n<|im_start|>assistant\n{response}<|im_end|>"
                try:
                    tokens = tokenizer.encode(full_text)
                    token_len = len(tokens)
                    
                    print(f"æ ·æœ¬ {i+1:2d}: å­—ç¬¦={char_len:5d}, Token={token_len:5d} - {sample.get('metadata', {}).get('title', 'unknown')[:50]}")
                    
                    if token_len > 8192:  # è¶…é•¿æ ·æœ¬
                        issues.append(f"ç¬¬{line_num+1}è¡Œ: Tokené•¿åº¦è¿‡é•¿ ({token_len}), æ ‡é¢˜: {sample.get('metadata', {}).get('title', 'unknown')}")
                        
                except Exception as e:
                    issues.append(f"ç¬¬{line_num+1}è¡Œ: åˆ†è¯å¤±è´¥ - {e}")
            else:
                print(f"æ ·æœ¬ {i+1:2d}: å­—ç¬¦={char_len:5d} - {sample.get('metadata', {}).get('title', 'unknown')[:50]}")
                
                if char_len > 20000:  # è¶…é•¿æ ·æœ¬ï¼ˆå­—ç¬¦çº§åˆ«ï¼‰
                    issues.append(f"ç¬¬{line_num+1}è¡Œ: å­—ç¬¦é•¿åº¦è¿‡é•¿ ({char_len}), æ ‡é¢˜: {sample.get('metadata', {}).get('title', 'unknown')}")
            
            # æ£€æŸ¥ç©ºæ•°æ®
            if not instruction.strip() or not response.strip():
                issues.append(f"ç¬¬{line_num+1}è¡Œ: åŒ…å«ç©ºçš„instructionæˆ–response")
                
        except Exception as e:
            issues.append(f"ç¬¬{line_num+1}è¡Œ: å¤„ç†æ—¶å‡ºé”™ - {e}")
    
    if issues:
        print(f"\nâŒ å‘ç° {len(issues)} ä¸ªé—®é¢˜:")
        for issue in issues:
            print(f"   {issue}")
    else:
        print("âœ… æ•°æ®æ ¼å¼æ£€æŸ¥é€šè¿‡")
    
    return issues

if __name__ == "__main__":
    diagnose_training_data() 
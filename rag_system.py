#!/usr/bin/env python3
"""
å¡”ç½—AI RAG (æ£€ç´¢å¢žå¼ºç”Ÿæˆ) ç³»ç»Ÿ
ç®¡ç†å¡”ç½—çŸ¥è¯†åº“ã€ç”¨æˆ·ä¸Šä¸‹æ–‡ï¼Œå¹¶æä¾›è¯­ä¹‰æ£€ç´¢åŠŸèƒ½
"""

import json
import os
import chromadb
import hashlib
from pathlib import Path
from typing import Dict, List, Optional, Tuple
from sentence_transformers import SentenceTransformer
import numpy as np
from datetime import datetime
import re

class TarotRAGSystem:
    def __init__(self, data_dir: str = "data", embedding_model: str = "BAAI/bge-small-zh-v1.5"):
        self.data_dir = Path(data_dir)
        self.processed_dir = self.data_dir / "processed"
        self.db_dir = self.data_dir / "vector_db"
        self.db_dir.mkdir(exist_ok=True)
        
        print(f"ðŸ¤– åŠ è½½åµŒå…¥æ¨¡åž‹: {embedding_model}")
        self.embedding_model = SentenceTransformer(embedding_model)
        print(f"âœ… åµŒå…¥æ¨¡åž‹åŠ è½½å®Œæˆ")
        
        # åˆå§‹åŒ–ChromaDB
        self.chroma_client = chromadb.PersistentClient(path=str(self.db_dir))
        
        # åˆ›å»ºä¸åŒçš„å‘é‡é›†åˆ
        self.collections = {
            "course_notes": self.chroma_client.get_or_create_collection("tarot_course_notes"),
            "readings": self.chroma_client.get_or_create_collection("tarot_readings"),
            "birthcharts": self.chroma_client.get_or_create_collection("birthcharts"),
            "user_context": self.chroma_client.get_or_create_collection("user_context")
        }
        
        print(f"ðŸ—ƒï¸  å‘é‡æ•°æ®åº“åˆå§‹åŒ–å®Œæˆ")
        
    def get_embedding(self, text: str) -> List[float]:
        """èŽ·å–æ–‡æœ¬çš„å‘é‡åµŒå…¥"""
        return self.embedding_model.encode(text).tolist()
    
    def chunk_text(self, text: str, max_length: int = 500, overlap: int = 50) -> List[str]:
        """å°†é•¿æ–‡æœ¬åˆ†å‰²æˆå—"""
        # æŒ‰æ®µè½åˆ†å‰²
        paragraphs = text.split('\n\n')
        chunks = []
        current_chunk = ""
        
        for paragraph in paragraphs:
            paragraph = paragraph.strip()
            if not paragraph:
                continue
                
            # å¦‚æžœå½“å‰æ®µè½å¾ˆé•¿ï¼Œè¿›ä¸€æ­¥åˆ†å‰²
            if len(paragraph) > max_length:
                sentences = re.split(r'[ã€‚ï¼ï¼Ÿ]', paragraph)
                for sentence in sentences:
                    sentence = sentence.strip()
                    if not sentence:
                        continue
                    sentence += "ã€‚"
                    
                    if len(current_chunk) + len(sentence) > max_length:
                        if current_chunk:
                            chunks.append(current_chunk.strip())
                        current_chunk = sentence
                    else:
                        current_chunk += sentence
            else:
                if len(current_chunk) + len(paragraph) > max_length:
                    if current_chunk:
                        chunks.append(current_chunk.strip())
                    current_chunk = paragraph
                else:
                    current_chunk += "\n\n" + paragraph if current_chunk else paragraph
        
        if current_chunk:
            chunks.append(current_chunk.strip())
        
        return chunks
    
    def index_course_notes(self):
        """ç´¢å¼•è¯¾ç¨‹ç¬”è®°"""
        print("ðŸ“š ç´¢å¼•è¯¾ç¨‹ç¬”è®°...")
        
        notes_dir = self.processed_dir / "course_notes"
        if not notes_dir.exists():
            print("âŒ è¯¾ç¨‹ç¬”è®°ç›®å½•ä¸å­˜åœ¨")
            return
        
        indexed_count = 0
        for json_file in notes_dir.glob("*.json"):
            with open(json_file, 'r', encoding='utf-8') as f:
                note_data = json.load(f)
            
            card_name = note_data['card_name']
            content = note_data['content']
            keywords = note_data.get('keywords', [])
            
            # åˆ†å—å¤„ç†é•¿æ–‡æœ¬
            chunks = self.chunk_text(content)
            
            for i, chunk in enumerate(chunks):
                chunk_id = f"{note_data['id']}_chunk_{i}"
                
                # æ·»åŠ åˆ°å‘é‡æ•°æ®åº“
                self.collections["course_notes"].add(
                    documents=[chunk],
                    metadatas=[{
                        "card_name": card_name,
                        "keywords": ",".join(keywords),
                        "chunk_index": i,
                        "total_chunks": len(chunks),
                        "source_file": str(json_file),
                        "type": "course_note"
                    }],
                    ids=[chunk_id]
                )
            
            indexed_count += 1
            print(f"âœ… å·²ç´¢å¼•: {card_name} ({len(chunks)} ä¸ªæ–‡æœ¬å—)")
        
        print(f"ðŸ“š è¯¾ç¨‹ç¬”è®°ç´¢å¼•å®Œæˆ: {indexed_count} ä¸ªæ–‡ä»¶")
    
    def index_readings(self):
        """ç´¢å¼•è§£ç‰Œè®°å½•"""
        print("ðŸ”® ç´¢å¼•è§£ç‰Œè®°å½•...")
        
        readings_dir = self.processed_dir / "readings"
        if not readings_dir.exists():
            print("âŒ è§£ç‰Œè®°å½•ç›®å½•ä¸å­˜åœ¨")
            return
        
        indexed_count = 0
        for json_file in readings_dir.glob("*.json"):
            with open(json_file, 'r', encoding='utf-8') as f:
                reading_data = json.load(f)
            
            title = reading_data['title']
            interpretation = reading_data['interpretation']
            cards = reading_data.get('cards', [])
            date = reading_data.get('date', '')
            
            # åˆ›å»ºå¡ç‰Œåˆ—è¡¨å­—ç¬¦ä¸²
            card_list = ", ".join([card['name'] for card in cards])
            
            # åˆ†å—å¤„ç†è§£ç‰Œå†…å®¹
            chunks = self.chunk_text(interpretation)
            
            for i, chunk in enumerate(chunks):
                chunk_id = f"{reading_data['id']}_chunk_{i}"
                
                self.collections["readings"].add(
                    documents=[chunk],
                    metadatas=[{
                        "title": title,
                        "cards": card_list,
                        "date": date,
                        "chunk_index": i,
                        "total_chunks": len(chunks),
                        "source_file": str(json_file),
                        "type": "reading"
                    }],
                    ids=[chunk_id]
                )
            
            indexed_count += 1
            print(f"âœ… å·²ç´¢å¼•: {title} ({len(chunks)} ä¸ªæ–‡æœ¬å—)")
        
        print(f"ðŸ”® è§£ç‰Œè®°å½•ç´¢å¼•å®Œæˆ: {indexed_count} ä¸ªæ–‡ä»¶")
    
    def index_birthcharts(self):
        """ç´¢å¼•æ˜Ÿç›˜æ•°æ®"""
        print("â­ ç´¢å¼•æ˜Ÿç›˜æ•°æ®...")
        
        charts_dir = self.processed_dir / "birthcharts"
        if not charts_dir.exists():
            print("âŒ æ˜Ÿç›˜ç›®å½•ä¸å­˜åœ¨")
            return
        
        indexed_count = 0
        for json_file in charts_dir.glob("*.json"):
            with open(json_file, 'r', encoding='utf-8') as f:
                chart_data = json.load(f)
            
            person_name = chart_data['person_name']
            content = chart_data['content']
            sections = chart_data.get('sections', {})
            
            # ç´¢å¼•å®Œæ•´å†…å®¹
            chunks = self.chunk_text(content)
            
            for i, chunk in enumerate(chunks):
                chunk_id = f"{chart_data['id']}_chunk_{i}"
                
                self.collections["birthcharts"].add(
                    documents=[chunk],
                    metadatas=[{
                        "person_name": person_name,
                        "chunk_index": i,
                        "total_chunks": len(chunks),
                        "source_file": str(json_file),
                        "type": "birthchart"
                    }],
                    ids=[chunk_id]
                )
            
            # å•ç‹¬ç´¢å¼•å„ä¸ªç« èŠ‚
            for section_name, section_content in sections.items():
                if section_content.strip():
                    section_id = f"{chart_data['id']}_section_{hashlib.md5(section_name.encode()).hexdigest()[:8]}"
                    
                    self.collections["birthcharts"].add(
                        documents=[section_content],
                        metadatas=[{
                            "person_name": person_name,
                            "section_name": section_name,
                            "source_file": str(json_file),
                            "type": "birthchart_section"
                        }],
                        ids=[section_id]
                    )
            
            indexed_count += 1
            print(f"âœ… å·²ç´¢å¼•: {person_name} ({len(chunks)} ä¸ªæ–‡æœ¬å— + {len(sections)} ä¸ªç« èŠ‚)")
        
        print(f"â­ æ˜Ÿç›˜æ•°æ®ç´¢å¼•å®Œæˆ: {indexed_count} ä¸ªæ–‡ä»¶")
    
    def add_user_context(self, user_id: str, context_data: Dict):
        """æ·»åŠ ç”¨æˆ·ä¸Šä¸‹æ–‡ä¿¡æ¯"""
        context_id = f"user_{user_id}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        
        # å°†ä¸Šä¸‹æ–‡è½¬æ¢ä¸ºå¯æœç´¢çš„æ–‡æœ¬
        context_text = f"ç”¨æˆ·: {user_id}\n"
        if 'summary' in context_data:
            context_text += f"æ‘˜è¦: {context_data['summary']}\n"
        if 'cards' in context_data:
            context_text += f"ç›¸å…³å¡ç‰Œ: {', '.join(context_data['cards'])}\n"
        if 'themes' in context_data:
            context_text += f"ä¸»é¢˜: {', '.join(context_data['themes'])}\n"
        if 'notes' in context_data:
            context_text += f"å¤‡æ³¨: {context_data['notes']}\n"
        
        # å¤„ç†metadataï¼Œç¡®ä¿æ‰€æœ‰å€¼éƒ½æ˜¯æ ‡é‡ç±»åž‹
        metadata = {
            "user_id": user_id,
            "timestamp": datetime.now().isoformat(),
            "context_type": context_data.get('type', 'general')
        }
        
        # å°†åˆ—è¡¨è½¬æ¢ä¸ºå­—ç¬¦ä¸²
        for key, value in context_data.items():
            if isinstance(value, list):
                metadata[key] = ','.join(str(v) for v in value)
            elif isinstance(value, (str, int, float, bool)) or value is None:
                metadata[key] = value
            else:
                metadata[key] = str(value)
        
        self.collections["user_context"].add(
            documents=[context_text],
            metadatas=[metadata],
            ids=[context_id]
        )
        
        print(f"âœ… ç”¨æˆ·ä¸Šä¸‹æ–‡å·²æ·»åŠ : {user_id}")
    
    def search_knowledge_base(self, query: str, collection_names: List[str] = None, n_results: int = 5) -> Dict:
        """æœç´¢çŸ¥è¯†åº“"""
        if collection_names is None:
            collection_names = ["course_notes", "readings", "birthcharts"]
        
        results = {}
        
        for collection_name in collection_names:
            if collection_name in self.collections:
                try:
                    search_results = self.collections[collection_name].query(
                        query_texts=[query],
                        n_results=n_results
                    )
                    
                    # æ ¼å¼åŒ–ç»“æžœ
                    formatted_results = []
                    if search_results['documents'][0]:  # ç¡®ä¿æœ‰ç»“æžœ
                        for i in range(len(search_results['documents'][0])):
                            formatted_results.append({
                                "content": search_results['documents'][0][i],
                                "metadata": search_results['metadatas'][0][i],
                                "distance": search_results['distances'][0][i] if 'distances' in search_results else None
                            })
                    
                    results[collection_name] = formatted_results
                    
                except Exception as e:
                    print(f"âŒ æœç´¢ {collection_name} æ—¶å‡ºé”™: {e}")
                    results[collection_name] = []
        
        return results
    
    def get_user_context(self, user_id: str, limit: int = 10) -> List[Dict]:
        """èŽ·å–ç”¨æˆ·åŽ†å²ä¸Šä¸‹æ–‡"""
        try:
            search_results = self.collections["user_context"].query(
                query_texts=[f"ç”¨æˆ·: {user_id}"],
                n_results=limit,
                where={"user_id": user_id}
            )
            
            formatted_results = []
            if search_results['documents'][0]:
                for i in range(len(search_results['documents'][0])):
                    formatted_results.append({
                        "content": search_results['documents'][0][i],
                        "metadata": search_results['metadatas'][0][i]
                    })
            
            return formatted_results
            
        except Exception as e:
            print(f"âŒ èŽ·å–ç”¨æˆ·ä¸Šä¸‹æ–‡æ—¶å‡ºé”™: {e}")
            return []
    
    def generate_context_for_query(self, query: str, user_id: str = None, max_context_length: int = 2000) -> str:
        """ä¸ºæŸ¥è¯¢ç”Ÿæˆç›¸å…³ä¸Šä¸‹æ–‡"""
        context_parts = []
        
        # 1. æœç´¢çŸ¥è¯†åº“
        knowledge_results = self.search_knowledge_base(query, n_results=3)
        
        # æ·»åŠ è¯¾ç¨‹ç¬”è®°
        if knowledge_results.get("course_notes"):
            context_parts.append("=== ç›¸å…³è¯¾ç¨‹ç¬”è®° ===")
            for result in knowledge_results["course_notes"][:2]:
                card_name = result['metadata'].get('card_name', 'æœªçŸ¥')
                content = result['content'][:300] + "..." if len(result['content']) > 300 else result['content']
                context_parts.append(f"ã€{card_name}ã€‘{content}")
        
        # æ·»åŠ åŽ†å²è§£ç‰Œ
        if knowledge_results.get("readings"):
            context_parts.append("\n=== ç›¸å…³è§£ç‰Œè®°å½• ===")
            for result in knowledge_results["readings"][:2]:
                title = result['metadata'].get('title', 'æœªçŸ¥')
                content = result['content'][:300] + "..." if len(result['content']) > 300 else result['content']
                context_parts.append(f"ã€{title}ã€‘{content}")
        
        # 2. æ·»åŠ ç”¨æˆ·ä¸Šä¸‹æ–‡
        if user_id:
            user_context = self.get_user_context(user_id, limit=3)
            if user_context:
                context_parts.append("\n=== ç”¨æˆ·åŽ†å²ä¿¡æ¯ ===")
                for ctx in user_context:
                    content = ctx['content'][:200] + "..." if len(ctx['content']) > 200 else ctx['content']
                    context_parts.append(content)
        
        # ç»„åˆä¸Šä¸‹æ–‡ï¼Œç¡®ä¿ä¸è¶…è¿‡é•¿åº¦é™åˆ¶
        full_context = "\n".join(context_parts)
        if len(full_context) > max_context_length:
            full_context = full_context[:max_context_length] + "..."
        
        return full_context
    
    def get_database_stats(self) -> Dict:
        """èŽ·å–æ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯"""
        stats = {}
        
        for name, collection in self.collections.items():
            try:
                count = collection.count()
                stats[name] = count
            except Exception as e:
                stats[name] = f"é”™è¯¯: {e}"
        
        return stats
    
    def clear_database(self, collection_name: str = None):
        """æ¸…ç©ºæ•°æ®åº“"""
        if collection_name and collection_name in self.collections:
            try:
                self.chroma_client.delete_collection(collection_name)
            except Exception:
                pass  # é›†åˆå¯èƒ½ä¸å­˜åœ¨
            self.collections[collection_name] = self.chroma_client.get_or_create_collection(collection_name)
            print(f"âœ… å·²æ¸…ç©ºé›†åˆ: {collection_name}")
        else:
            for name in self.collections.keys():
                try:
                    self.chroma_client.delete_collection(name)
                except Exception:
                    pass  # é›†åˆå¯èƒ½ä¸å­˜åœ¨
                self.collections[name] = self.chroma_client.get_or_create_collection(name)
            print("âœ… å·²æ¸…ç©ºæ‰€æœ‰é›†åˆ")

def main():
    """ä¸»å‡½æ•° - å»ºç«‹çŸ¥è¯†åº“"""
    rag = TarotRAGSystem()
    
    print("ðŸš€ å¼€å§‹å»ºç«‹å¡”ç½—AIçŸ¥è¯†åº“...")
    print("=" * 50)
    
    # æ£€æŸ¥çŽ°æœ‰æ•°æ®åº“çŠ¶æ€
    print("ðŸ“Š å½“å‰æ•°æ®åº“çŠ¶æ€:")
    stats = rag.get_database_stats()
    for name, count in stats.items():
        print(f"  {name}: {count} æ¡è®°å½•")
    
    # è¯¢é—®æ˜¯å¦é‡æ–°ç´¢å¼•
    if any(isinstance(count, int) and count > 0 for count in stats.values()):
        choice = input("\nâš ï¸  æ£€æµ‹åˆ°å·²æœ‰æ•°æ®ï¼Œæ˜¯å¦é‡æ–°ç´¢å¼•ï¼Ÿ(y/n): ")
        if choice.lower() == 'y':
            rag.clear_database()
    
    # å¼€å§‹ç´¢å¼•
    print("\nðŸ”„ å¼€å§‹ç´¢å¼•æ•°æ®...")
    
    # ç´¢å¼•å„ç±»æ•°æ®
    rag.index_course_notes()
    rag.index_readings()
    rag.index_birthcharts()
    
    # æ·»åŠ ç¤ºä¾‹ç”¨æˆ·ä¸Šä¸‹æ–‡
    print("\nðŸ‘¤ æ·»åŠ ç¤ºä¾‹ç”¨æˆ·ä¸Šä¸‹æ–‡...")
    rag.add_user_context("mel", {
        "type": "profile",
        "summary": "å¯¹å¡”ç½—å’Œæ˜Ÿåº§å­¦ä¹ æœ‰æ·±åº¦å…´è¶£ï¼Œç‰¹åˆ«å…³æ³¨å¿ƒè½®èƒ½é‡å’Œå¤§çˆ±ä¸»é¢˜",
        "cards": ["çš‡åŽ", "æ˜Ÿæ˜Ÿ", "åœ£æ¯å›½çŽ‹"],
        "themes": ["å¿ƒè½®é€šé“", "å¤§çˆ±", "çµæ€§æˆé•¿"],
        "notes": "M4 MacBookç”¨æˆ·ï¼Œå¸Œæœ›æœ¬åœ°è¿è¡ŒAIæ¨¡åž‹"
    })
    
    # æ˜¾ç¤ºæœ€ç»ˆç»Ÿè®¡
    print("\nðŸ“Š ç´¢å¼•å®ŒæˆåŽçš„æ•°æ®åº“çŠ¶æ€:")
    final_stats = rag.get_database_stats()
    for name, count in final_stats.items():
        print(f"  {name}: {count} æ¡è®°å½•")
    
    print("\nâœ… å¡”ç½—AIçŸ¥è¯†åº“å»ºç«‹å®Œæˆï¼")
    print("ðŸ’¡ ä½ å¯ä»¥ä½¿ç”¨ä»¥ä¸‹æ–¹æ³•æµ‹è¯•RAGç³»ç»Ÿ:")
    print("  - rag.search_knowledge_base('çš‡åŽç‰Œçš„å«ä¹‰')")
    print("  - rag.generate_context_for_query('å¿ƒè½®èƒ½é‡', 'mel')")
    
    return rag

if __name__ == "__main__":
    rag_system = main()
    
    # ç®€å•çš„äº¤äº’æµ‹è¯•
    print("\nðŸ§ª RAGç³»ç»Ÿæµ‹è¯•")
    print("è¾“å…¥æŸ¥è¯¢æ¥æµ‹è¯•çŸ¥è¯†åº“æ£€ç´¢ (è¾“å…¥ 'quit' é€€å‡º):")
    
    while True:
        query = input("\nðŸ” æŸ¥è¯¢: ").strip()
        if query.lower() in ['quit', 'exit', 'é€€å‡º']:
            break
        
        if query:
            print("ðŸ“š æœç´¢ç»“æžœ:")
            results = rag_system.search_knowledge_base(query, n_results=2)
            
            for collection_name, items in results.items():
                if items:
                    print(f"\n--- {collection_name} ---")
                    for i, item in enumerate(items[:2]):
                        content = item['content'][:200] + "..." if len(item['content']) > 200 else item['content']
                        print(f"{i+1}. {content}")
            
            print("\nðŸŽ¯ ç”Ÿæˆçš„ä¸Šä¸‹æ–‡:")
            context = rag_system.generate_context_for_query(query, "mel")
            print(context[:500] + "..." if len(context) > 500 else context) 